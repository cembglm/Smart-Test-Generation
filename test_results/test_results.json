{
    "_id": {
      "$oid": "6786168a7c84385a0519cfbf"
    },
    "session_id": "20250114104721",
    "original_prompts": [
      {
        "_id": {
          "$oid": "673d7efca741904082541a61"
        },
        "test_name": "Functional Testing",
        "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a detailed functional test scenario focused on evaluating the system’s core functionalities against specified requirements. This scenario should assess whether the system performs its intended operations correctly, meeting both functional specifications and expected user behaviors. Clearly define the purpose of the test, including necessary preconditions, setup configurations, and initial data required. Identify the primary functions to be tested, including input conditions, expected outputs, and any error-handling mechanisms. Outline specific test steps that simulate realistic user actions to validate each function. Describe the expected outcomes for each step, ensuring that both positive and negative cases are covered. Emphasize that the scenario aligns with ISTQB functional testing standards, aiming for accuracy, completeness, and robustness in functional validation.",
        "test_scoring_elements_and_prompts": {
          "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as preconditions, test steps, expected outcomes, and error handling requirements.",
          "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for functional testing, including elements like input conditions, functional checks, and expected results.",
          "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, expected outcomes, and error-handling match the intended functional testing standards.",
          "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete examples for inputs, clear test step definitions, and detailed expected results. Specificity in test data and expected outcomes is essential for robust functional testing.",
          "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of functional testing as per ISTQB guidelines."
        },
        "test_instruction_elements_and_prompts": {
          "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of functional testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
          "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, input conditions, expected outputs, and error handling. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
          "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (functional) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant test steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
          "Detail Verification": "Review each scenario for specificity in areas such as input conditions, expected results, and error handling. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
          "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
        },
        "llm_output_judges_and_prompts": {
          "Relevance": "Assess whether the generated test scenario effectively evaluates the system's core functionalities against the specified requirements, focusing on whether it tests intended operations and expected user behaviors as outlined in the prompt. Provide a score (1-5) and a brief justification.",
          "Completeness": "Evaluate if the scenario includes all essential components for functional testing, such as clear objectives, necessary preconditions, detailed test steps simulating realistic user actions, input conditions, expected outputs, and error handling mechanisms. Provide a score (1-5) and specify any missing elements.",
          "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with functional testing standards and ISTQB guidelines. Ensure that the test steps are appropriate for the functionalities being tested and that expected outcomes are properly defined. Provide a True/False verdict and explain any inconsistencies.",
          "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for functional testing. Consider if the scenario is well-organized, uses precise technical language, and provides clear instructions. Provide a score (1-5) and suggest improvements if necessary."
        },
        "test_case_create_prompts": {
          "Core Functionalities Test Case": "Create a test case that verifies the core functionalities of the system as defined in the requirements. Include preconditions, detailed steps, input data, and expected results to ensure the system performs its intended operations correctly. Ensure the case aligns with ISTQB Functional Testing guidelines. Specify each step's expected output clearly and concisely.",
          "Boundary Value Analysis Test Case": "Develop a test case focusing on Boundary Value Analysis (BVA) to evaluate system behavior at the edges of input ranges. Include preconditions, input data at lower and upper boundaries, and the expected system response for each scenario. Define clear test steps for validating boundary conditions, ensuring compliance with functional testing standards.",
          "Error Recovery Functional Test Case": "Design a test case to assess the system's error recovery mechanisms during normal operations. Include scenarios with invalid inputs, interruptions, or fault conditions. Specify detailed steps to simulate these scenarios and describe the expected system behavior for each step, including recovery or error handling outputs.",
          "User Input Validation Test Case": "Create a test case to validate the system's ability to handle various user inputs. Include valid, invalid, and edge-case inputs, with steps to simulate input actions and monitor system responses. Specify preconditions, detailed test steps, and expected outcomes to ensure the system handles inputs correctly and provides appropriate feedback for invalid inputs."
        },
        "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, generate a comprehensive set of functional test cases based on the provided test scenario. Ensure that the test cases collectively cover all actions, steps, and functional areas outlined in the scenario. Each test case should include detailed descriptions of preconditions, input conditions, step-by-step actions, and expected outcomes. Cover positive, negative, and boundary cases to ensure complete validation of the system’s functionalities. Include specific checks for error handling and edge cases where applicable. Organize the test cases to ensure clarity and traceability, aligning with ISTQB standards. The goal is to produce the maximum number of test cases necessary to comprehensively validate the scenario while avoiding redundancy.",
        "customised_prompt_status": false
      },
      {
        "_id": {
          "$oid": "6732fb8abb7182490b5d6551"
        },
        "test_name": "Edge Cases and Boundary Testing",
        "test_prompt": "Acting as a senior ISTQB-certified test analyst, design detailed test scenarios focused on evaluating the system's response to edge cases and boundary values. The scenario should explore how the system behaves at the extremes of input ranges, identifying potential vulnerabilities and ensuring reliability. Clearly define the purpose of the test, necessary preconditions, and environment configurations to create accurate testing conditions. Specify a range of test cases, including boundary values (just below, at, and just above limits) and edge cases that challenge system assumptions. Describe the expected outcomes for each case, detailing system responses, error handling, or corrective measures as needed. Outline the test steps, ensuring they capture all critical values for each parameter. Emphasize that the scenario aligns with ISTQB standards for boundary testing, aiming to identify weaknesses and ensure robustness against extreme inputs.",
        "test_scoring_elements_and_prompts": {
          "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as boundary value examples, edge cases, expected outcomes, and specific error handling requirements.",
          "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for edge cases and boundary testing, including elements like precise boundary values, error handling expectations, and critical input limits.",
          "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, boundary values, and expected outcomes match the intended category and testing standards for boundary and edge case evaluation.",
          "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that provide concrete boundary values, clear definitions of acceptable limits, and detailed error handling for edge cases. Specificity in test data and expected results is essential for robust testing.",
          "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of boundary and edge case testing as per ISTQB guidelines."
        },
        "test_instruction_elements_and_prompts": {
          "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of boundary and edge case testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
          "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, boundary values, edge cases, and expected outcomes. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
          "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (boundary and edge cases) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
          "Detail Verification": "Review each scenario for specificity in areas such as boundary values, edge case examples, and error expectations. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
          "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
        },
        "llm_output_judges_and_prompts": {
          "Relevance": "Assess whether the generated test scenario effectively targets edge cases and boundary values, exploring system behavior at the extremes of input ranges as specified in the prompt. Provide a score (1-5) and a brief justification.",
          "Completeness": "Evaluate if the scenario includes all essential components for edge case and boundary testing, such as specific boundary values (just below, at, and just above limits), detailed preconditions, test steps, and expected outcomes for each case. Provide a score (1-5) and specify any missing elements.",
          "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with boundary testing standards and ISTQB guidelines. Check if the test steps correspond correctly to the specified boundary values. Provide a True/False verdict and explain any inconsistencies.",
          "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for boundary testing. Consider if the scenario clearly communicates the test conditions and expectations. Provide a score (1-5) and suggest improvements if necessary."
        },
        "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, create a detailed set of test cases to evaluate system behavior under edge case scenarios and at boundary input values. Include preconditions, specific test data for both lower and upper limits, and steps to assess how the system handles inputs outside the expected range. Ensure scenarios include both positive and negative test cases to validate robustness and reliability. Align test cases with ISTQB boundary testing guidelines for comprehensive validation of system behavior.",
        "test_case_create_prompts": {
          "Boundary Value Analysis Test Case": "Design a test case focusing on testing inputs at the boundaries of acceptable ranges. Specify lower and upper limits, just below and above these limits, and expected outcomes for each condition. Include clear steps for validating the system's response to each input.",
          "Extreme Input Test Case": "Create a test case to evaluate system performance and stability when handling extreme or unrealistic input values. Define test data that pushes the system beyond its designed operational range, specifying the expected error messages or graceful degradation responses.",
          "Null and Empty Input Test Case": "Develop a test case to assess system behavior when null or empty inputs are provided. Include detailed steps to simulate such inputs and expected outcomes, such as validation error messages or fallback defaults.",
          "Invalid Format Test Case": "Create a test case to evaluate the system's validation mechanisms when inputs are provided in incorrect formats. Include examples such as malformed dates, special characters in numeric fields, or overly long strings, and specify expected error handling outcomes."
        },
        "customised_prompt_status": false
      },
      {
        "_id": {
          "$oid": "6732fb8abb7182490b5d6555"
        },
        "test_name": "Security Testing",
        "test_prompt": "As an ISTQB-certified test analyst, design a comprehensive security test scenario for the TaskMaster system, focusing on evaluating its resilience against various security threats. The scenario should identify vulnerabilities, ensure data protection, and validate access controls within the task management application.\n\nPreconditions: A user with administrative privileges is logged in to the TaskMaster system.\nEnvironment Configurations: The system is configured with standard security settings.\nUser Roles:\n- Admin: Can manage tasks, projects, and users.\n- User: Can view and manage their own tasks and projects.\n\nKey areas to be tested:\n- Authentication: User login functionality.\n- Authorization: Access control for task management.\n- Data Encryption: Protection of sensitive data in the system.\n- Input Validation: Preventing malicious input from being executed.\n- Session Management: Handling user sessions securely.\n- Error Handling: System response to errors and exceptions.\n\nCommon security tests:\n- SQL injection: Testing for vulnerabilities in task retrieval and update.\n- XSS: Testing for cross-site scripting vulnerabilities in task descriptions.\n- CSRF: Testing for cross-site request forgery attacks on project management.\n- Brute-force attacks: Testing the system's resistance to repeated login attempts.\n- Privilege escalation: Testing the system's ability to prevent unauthorized access to sensitive data.\n\nExpected outcomes:\n- The system successfully authenticates users and authorizes access to tasks.\n- Sensitive data is protected from unauthorized access.\n- Malicious input is prevented from being executed.\n- User sessions are handled securely.\n- System responses are logged and reported.\n\nThis scenario aligns with ISTQB security testing standards, focusing on thorough vulnerability detection, data integrity, and system robustness against unauthorized access.",
        "test_scoring_elements_and_prompts": {
          "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as preconditions, security vulnerabilities to test, expected outcomes, and error handling mechanisms.",
          "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for security testing, including elements like specific security threats, test steps, and expected defensive responses.",
          "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, security vulnerabilities, and expected outcomes match the intended security testing standards.",
          "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete security threat examples, detailed test steps, and clear expected outcomes for each threat. Specificity in vulnerability descriptions and system responses is essential for robust security testing.",
          "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of security testing as per ISTQB guidelines."
        },
        "test_instruction_elements_and_prompts": {
          "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of security testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
          "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, security vulnerabilities, expected outcomes, and error handling. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
          "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (security) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant test steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
          "Detail Verification": "Review each scenario for specificity in areas such as vulnerability descriptions, expected results, and error handling requirements. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
          "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
        },
        "llm_output_judges_and_prompts": {
          "Relevance": "Assess whether the generated test scenario effectively evaluates the system's resilience against security threats, focusing on identifying vulnerabilities and validating access controls as specified in the prompt. Provide a score (1-5) and a brief justification.",
          "Completeness": "Evaluate if the scenario includes all essential components for security testing, such as specific security vulnerabilities to test (e.g., SQL injection, XSS, CSRF), necessary preconditions, user roles, test steps, expected system responses, and error handling mechanisms. Provide a score (1-5) and specify any missing elements.",
          "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with security testing standards and ISTQB guidelines. Ensure that the security threats and test steps correspond correctly to the objectives. Provide a True/False verdict and explain any inconsistencies.",
          "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for security testing. Consider if the scenario is detailed, well-structured, and uses appropriate security terminology. Provide a score (1-5) and suggest improvements if necessary."
        },
        "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive set of security test cases to evaluate the system's resilience against threats. Include scenarios for testing authentication, input validation, data encryption, and unauthorized access prevention. Each test case should include detailed preconditions, steps to simulate security vulnerabilities, and expected outcomes. Align all test cases with ISTQB security testing guidelines to ensure thorough validation of system security measures.",
        "test_case_create_prompts": {
          "Authentication Test Case": "Create a test case to verify the robustness of the system's authentication mechanism, including password strength requirements and protection against brute-force attacks.",
          "Input Validation Security Test Case": "Develop a test case to test for vulnerabilities such as SQL injection, XSS, or CSRF attacks by providing malicious input. Specify expected outcomes like rejection of input and secure logging.",
          "Data Encryption Test Case": "Design a test case to assess the effectiveness of encryption for sensitive data during transmission and storage. Specify tools and methods to verify encryption implementation.",
          "Access Control Test Case": "Create a test case to ensure access control policies are enforced correctly, preventing unauthorized users from accessing restricted features or data."
        },
        "customised_prompt_status": true
      },
      {
        "_id": {
          "$oid": "6732fb8abb7182490b5d6556"
        },
        "test_name": "User Interface (GUI) Testing",
        "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive GUI test scenario to evaluate the usability, responsiveness, and visual consistency of the user interface for a specified system. This scenario should focus on ensuring that the interface meets design specifications and provides a seamless user experience. Clearly outline the purpose of the test, specifying necessary preconditions, screen resolutions, and device configurations required. Identify key UI elements to be tested, such as buttons, menus, text fields, alignment, colors, font sizes, and responsiveness across different devices. Describe test steps that include checking the interface's layout, navigation flow, interaction feedback, and responsiveness to user inputs. Define expected outcomes for each interaction, including alignment, visual feedback, error messages, and responsiveness criteria. Emphasize that the scenario aligns with ISTQB GUI testing standards, focusing on visual accuracy, and user experience quality.",
        "test_scoring_elements_and_prompts": {
          "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as preconditions, UI elements, expected outcomes, and visual consistency checks.",
          "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for GUI testing, including elements like specific UI components, visual feedback expectations, and responsiveness requirements.",
          "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, UI elements, and expected outcomes match the intended GUI testing standards.",
          "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete examples for UI elements, detailed test steps, and clear expected results. Specificity in layout checks, responsiveness, and user feedback is essential for comprehensive GUI testing.",
          "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of GUI testing as per ISTQB guidelines."
        },
        "test_instruction_elements_and_prompts": {
          "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of GUI testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
          "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, UI components, expected outcomes, and visual consistency checks. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
          "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (GUI) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant UI elements, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
          "Detail Verification": "Review each scenario for specificity in areas such as UI elements, layout expectations, and visual feedback. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
          "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
        },
        "llm_output_judges_and_prompts": {
          "Relevance": "Assess whether the generated test scenario effectively evaluates the usability, responsiveness, and visual consistency of the user interface, focusing on UI elements and user experience as specified in the prompt. Provide a score (1-5) and a brief justification.",
          "Completeness": "Evaluate if the scenario includes all essential components for GUI testing, such as specific UI elements to be tested (buttons, menus, text fields), necessary preconditions (screen resolutions, device configurations), test steps, expected outcomes (alignment, visual feedback, responsiveness), and visual consistency checks. Provide a score (1-5) and specify any missing elements.",
          "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with GUI testing standards and ISTQB guidelines. Check if the test steps accurately reflect the UI components and expected user interactions. Provide a True/False verdict and explain any inconsistencies.",
          "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for GUI testing. Consider if the scenario effectively communicates the testing of UI elements and user experience aspects. Provide a score (1-5) and suggest improvements if necessary."
        },
        "test_case_main_prompt": "As a senior ISTQB-certified test analyst, create a comprehensive set of GUI test cases to validate usability, visual consistency, and responsiveness. Include preconditions, test steps, and expected outcomes for all UI elements. Ensure test cases cover navigation, layout alignment, input handling, and feedback mechanisms. Align with ISTQB GUI testing standards for a seamless and user-friendly experience.",
        "test_case_create_prompts": {
          "Visual Consistency Test Case": "Create a test case to ensure UI elements are visually consistent with design specifications across resolutions. Specify alignment, color schemes, and font size checks.",
          "Navigation Flow Test Case": "Develop a test case to verify the intuitiveness of the navigation flow, ensuring users can seamlessly move between pages and access features.",
          "Input Field Validation Test Case": "Design a test case to test input field behavior, including placeholder text, error messages, and constraints on input formats.",
          "Responsiveness Test Case": "Create a test case to evaluate how the UI adapts to different screen sizes and orientations. Specify steps to verify element repositioning and integration."
        },
        "customised_prompt_status": false
      },
      {
        "_id": {
          "$oid": "6732fb8abb7182490b5d6550"
        },
        "test_name": "Compatibility Testing",
        "test_prompt": "Acting as a senior ISTQB-certified test analyst, design comprehensive test scenarios to evaluate the compatibility of a specified system across different environments. This scenario should focus on assessing the system’s ability to function as expected on various platforms, devices, operating systems, browsers, and configurations. Clearly outline the purpose of the test, including the preconditions and environmental setups required for accurate testing across multiple platforms. Specify a range of environments, such as specific operating system versions, browser types, device specifications, screen resolutions, and hardware configurations, ensuring broad coverage. Describe the expected outcomes for each environment, identifying potential compatibility issues, or visual rendering differences. Detail test steps that simulate user behavior in each environment to validate consistent performance. Emphasize that the scenario aligns with ISTQB compatibility testing standards, prioritizing cross-environment consistency, reliability, and user experience.",
        "test_scoring_elements_and_prompts": {
          "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective, structured approach, and including essential elements such as environment configurations, expected outcomes, and compatibility criteria.",
          "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for compatibility testing, including specific platform details, and cross-environment checks.",
          "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, environment configurations, and expected outcomes match the intended category and testing standards for compatibility evaluation.",
          "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete configurations (such as operating system versions, browser types), detailed expected results, and identified compatibility metrics.",
          "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of compatibility testing as per ISTQB guidelines."
        },
        "test_instruction_elements_and_prompts": {
          "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of compatibility testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
          "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, environment configurations, expected outcomes, and compatibility checks. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
          "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (compatibility) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant configurations, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
          "Detail Verification": "Review each scenario for specificity in areas such as platform versions, environment configurations, and expected responses. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
          "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
        },
        "llm_output_judges_and_prompts": {
          "Relevance": "Assess whether the generated test scenario effectively addresses compatibility testing across different environments as specified in the prompt. Consider if it includes testing on various platforms, devices, operating systems, browsers, and configurations. Provide a score (1-5) and a brief justification.",
          "Completeness": "Evaluate if the scenario includes all essential components for compatibility testing, such as detailed environment configurations, preconditions, test steps simulating user behavior on different platforms, and expected outcomes for each environment. Provide a score (1-5) and specify any missing elements.",
          "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with compatibility testing standards and ISTQB guidelines. Check if the test steps and expected outcomes match the specified environments and configurations. Provide a True/False verdict and explain any inconsistencies.",
          "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for compatibility testing. Consider if the scenario is clearly organized, uses technical language appropriately, and provides precise details. Provide a score (1-5) and suggest improvements if necessary."
        },
        "test_case_main_prompt": "As a senior ISTQB-certified test analyst, design a comprehensive set of compatibility test cases to evaluate the across different platforms, devices, operating systems, and browsers. Specify preconditions, test steps, and expected outcomes for each scenario. Ensure the cases assess cross-platform consistency, and visual rendering. Align with ISTQB compatibility testing standards to ensure the system provides a seamless user experience in diverse environments.",
        "test_case_create_prompts": {
          "Cross-Browser Test Case": "Create a test case to ensure consistency across multiple web browsers and their versions. Specify preconditions, browser configurations, detailed steps for validation, and expected outputs for compatibility verification.",
          "Device Compatibility Test Case": "Develop a test case to evaluate system behavior on a variety of devices, such as smartphones, tablets, and desktops. Include steps to verify responsive design and integrition across device specifications.",
          "Operating System Compatibility Test Case": "Design a test case to assess system performance on different operating systems, including specific versions. Include steps to test installation, execution, and interoperability between OS-specific features.",
          "Screen Resolution Test Case": "Create a test case to validate the system's responsiveness and layout consistency across various screen resolutions. Include steps for testing visual elements and alignment across resolutions."
        },
        "customised_prompt_status": false
      },
      {
        "_id": {
          "$oid": "6732fb8abb7182490b5d6552"
        },
        "test_name": "Input Data Variety Testing",
        "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive test scenario focused on assessing the system’s ability to handle a diverse range of input data. The scenario should evaluate how the system processes various data types, formats, edge cases, and outliers to ensure robust input handling. Clearly define the purpose of the test, the preconditions required, and the environment configurations necessary for accurate testing. Specify a range of input data examples, including valid, invalid, boundary, and null values. Describe the expected system responses, detailing any validation errors, warnings, or specific handling methods required for each type of input. Outline test steps that simulate user behavior with different input scenarios to validate the system’s resilience. Emphasize that the scenario aligns with ISTQB testing standards, ensuring a thorough assessment of data handling robustness and consistency.",
        "test_scoring_elements_and_prompts": {
          "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective, structured approach, and including all necessary elements such as input data examples, expected outcomes, and error handling.",
          "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for input variety testing, including elements like specific data types, validation requirements, and error handling processes.",
          "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, data requirements, and expected outcomes match the intended category and testing standards for input variety evaluation.",
          "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that include concrete examples of various input data types, specific configurations for the test environment, and clear definitions of expected outcomes for each input category. Specificity in data validation requirements and error expectations is essential.",
          "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of input data variety testing as per ISTQB guidelines."
        },
        "test_instruction_elements_and_prompts": {
          "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of input data variety testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
          "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, data examples (valid, invalid, boundary, null), and expected outcomes. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
          "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (input data variety) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
          "Detail Verification": "Review each scenario for specificity in areas such as input data examples, error expectations, and response definitions. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
          "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
        },
        "llm_output_judges_and_prompts": {
          "Relevance": "Assess whether the generated test scenario effectively evaluates the system's ability to handle a diverse range of input data, including valid, invalid, boundary, and null values, as specified in the prompt. Provide a score (1-5) and a brief justification.",
          "Completeness": "Evaluate if the scenario includes all essential components for input data variety testing, such as a range of input data examples, necessary preconditions, detailed test steps, expected system responses, validation errors, and error handling for each type of input. Provide a score (1-5) and specify any missing elements.",
          "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with input data variety testing standards and ISTQB guidelines. Check if the test steps accurately reflect the diverse input data and expected outcomes. Provide a True/False verdict and explain any inconsistencies.",
          "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for input data variety testing. Consider if the scenario effectively communicates the testing of different data types and their expected handling. Provide a score (1-5) and suggest improvements if necessary."
        },
        "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, generate a comprehensive set of test cases to validate the system's ability to handle diverse input data types. Include scenarios for valid, invalid, null, and edge-case inputs. Specify detailed steps, expected outcomes, and validation criteria for each input type. Align all test cases with ISTQB testing guidelines to ensure the system’s robustness in input handling.",
        "test_case_create_prompts": {
          "Valid Input Test Case": "Create a test case to validate the system’s ability to process valid input data accurately. Include examples for various data types such as text, numbers, and dates.",
          "Invalid Input Test Case": "Develop a test case to test system behavior with invalid inputs, ensuring appropriate error messages or rejections are triggered.",
          "Boundary Input Test Case": "Design a test case to assess the system’s handling of input values at the boundaries of acceptable ranges, including just below and above these limits.",
          "Special Character Input Test Case": "Create a test case to evaluate how the system handles inputs with special characters, emojis, or unexpected symbols. Specify expected sanitization or rejection behaviors."
        },
        "customised_prompt_status": false
      },
      {
        "_id": {
          "$oid": "6732fb8abb7182490b5d6554"
        },
        "test_name": "Performance and Load Testing",
        "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive performance and load testing scenario for a specified system. The scenario should aim to assess the system’s performance and load handling under realistic conditions, simulating user behaviors to evaluate how the system operates under various levels of stress. Clearly establish the purpose of the test, the preconditions required, and the environment configurations necessary to create accurate, real-world conditions. Outline the user load, including distribution and behavior patterns that reflect peak periods, random delays, and sudden surges. Identify key performance metrics such as response time, throughput, resource utilization, and error rates, detailing acceptable thresholds based on industry standards. Specify the data requirements to represent real-world usage, including edge cases, and present a clear, sequential process for test execution from setup to conclusion. Emphasize expected outcomes, define conditions for success and failure, and include guidelines for comprehensive logging and analysis. Ensure that the scenario aligns with ISTQB performance testing standards, incorporates best practices, and prioritizes precision, consistency, and robustness.",
        "test_scoring_elements_and_prompts": {
          "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, depth, and clarity. A high-quality scenario should align well with the specified test purpose, providing a clear objective and structured approach. It should include all necessary elements such as preconditions, environment configuration, user load details, performance metrics, and expected outcomes.",
          "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for performance and load testing, including specified elements like response time, error rates, data requirements, and system behavior under stress.",
          "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described testing steps, data requirements, and performance metrics match the intended category and testing standards for performance and load evaluation.",
          "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete numbers for user loads, specific configurations for the test environment, and clear definitions of success and failure thresholds. Specificity in metrics and detailed steps for test execution are essential.",
          "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of performance and load testing as per ISTQB guidelines."
        },
        "test_instruction_elements_and_prompts": {
          "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of performance and load testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
          "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, user load distribution, key performance metrics, and expected outcomes. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
          "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (performance and load) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched metrics or off-target objectives, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
          "Detail Verification": "Review each scenario for specificity in areas such as user load numbers, test environment configurations, and threshold definitions. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
          "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
        },
        "llm_output_judges_and_prompts": {
          "Relevance": "Assess whether the generated test scenario effectively evaluates the system's performance and load handling under various levels of stress, simulating realistic user behaviors and conditions as specified in the prompt. Provide a score (1-5) and a brief justification.",
          "Completeness": "Evaluate if the scenario includes all essential components for performance and load testing, such as user load details (distribution, behavior patterns), key performance metrics (response time, throughput, resource utilization, error rates), necessary preconditions, environment configurations, and expected outcomes. Provide a score (1-5) and specify any missing elements.",
          "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with performance and load testing standards and ISTQB guidelines. Check if the test steps accurately reflect the intended load conditions and performance metrics. Provide a True/False verdict and explain any inconsistencies.",
          "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for performance and load testing. Consider if the scenario effectively communicates the testing process and performance expectations. Provide a score (1-5) and suggest improvements if necessary."
        },
        "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive set of performance and load test cases to evaluate the system's ability to handle various levels of stress. Ensure that the test cases cover response time, throughput, scalability, and stability under different load conditions. Include preconditions, detailed execution steps, specific input data, and measurable expected outcomes for each scenario. Align all test cases with ISTQB performance testing guidelines to validate both typical and edge performance conditions. Emphasize precision, accuracy, and reproducibility in test documentation.",
        "test_case_create_prompts": {
          "Core Performance Test Case": "Develop a test case to verify the system's ability to handle normal operational loads. Include typical user behavior as input, preconditions for environment setup, and measurable metrics such as response time and system throughput. Ensure test steps simulate realistic user actions and specify precise success criteria.",
          "Stress Test Case": "Create a test case to evaluate how the system behaves under extreme stress, such as peak user loads or resource exhaustion. Include detailed steps for incrementally increasing load, monitoring system behavior, and identifying failure points. Specify thresholds for acceptable and critical performance levels.",
          "Scalability Test Case": "Design a test case to assess the system's ability to scale with increasing workload. Include steps to simulate user growth, resource allocation adjustments, and infrastructure elasticity. Define metrics like maximum concurrent users or transactions per second.",
          "Resource Utilization Test Case": "Create a test case focusing on system resource usage under varying load conditions. Specify steps to measure CPU, memory, disk I/O, and network bandwidth. Include scenarios for detecting bottlenecks and provide acceptable utilization thresholds."
        },
        "customised_prompt_status": false
      },
      {
        "_id": {
          "$oid": "6732fb8abb7182490b5d6553"
        },
        "test_name": "Integration Testing",
        "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive integration test scenario aimed at evaluating how different modules or components of a specified system interact with each other. This scenario should focus on ensuring that integrated components communicate correctly and work together. Clearly outline the purpose of the test, specifying necessary preconditions, setup configurations, and initial data required. Identify key interactions between components, including input and output data flow, dependencies, and any shared resources. Outline test steps that simulate realistic data exchange and process flows between components, capturing both normal and abnormal interaction patterns. Describe expected outcomes for each interaction, including response times, data integrity, and error handling requirements. Emphasize that the scenario aligns with ISTQB integration testing standards, focusing on accuracy, consistency, and robustness in component interaction validation.",
        "test_scoring_elements_and_prompts": {
          "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as preconditions, data flow between components, expected outcomes, and error handling mechanisms.",
          "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for integration testing, including specific component interactions, data exchange, and expected results.",
          "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, component interactions, and expected outcomes match the intended integration testing standards.",
          "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that include concrete component interactions, detailed data flows, and clear expected outcomes for each integration point. Specificity in data handling and error scenarios is essential for thorough integration testing.",
          "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of integration testing as per ISTQB guidelines."
        },
        "test_instruction_elements_and_prompts": {
          "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of integration testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
          "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, data flow between components, expected outcomes, and error handling. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
          "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (integration) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant test steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
          "Detail Verification": "Review each scenario for specificity in areas such as component interactions, expected results, and error handling mechanisms. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
          "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
        },
        "llm_output_judges_and_prompts": {
          "Relevance": "Assess whether the generated test scenario effectively evaluates the interactions between different modules or components of the system, focusing on data flow and communication between components as specified in the prompt. Provide a score (1-5) and a brief justification.",
          "Completeness": "Evaluate if the scenario includes all essential components for integration testing, such as detailed component interactions, necessary preconditions, data exchange processes, dependencies, shared resources, and expected outcomes for each interaction. Provide a score (1-5) and specify any missing elements.",
          "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with integration testing standards and ISTQB guidelines. Ensure that the interactions between components are correctly represented and that expected outcomes are properly defined. Provide a True/False verdict and explain any inconsistencies.",
          "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for integration testing. Consider if the scenario clearly outlines component interactions and is structured logically. Provide a score (1-5) and suggest improvements if necessary."
        },
        "test_case_main_prompt": "As a senior ISTQB-certified test analyst, generate a set of integration test cases to evaluate interactions between system components. Include detailed preconditions, interaction sequences, and data flow verification steps. Focus on validating module communication, data consistency, and error handling. Ensure the test cases align with ISTQB integration testing standards.",
        "test_case_create_prompts": {
          "Interface Communication Test Case": "Develop a test case to validate communication between integrated components, ensuring proper data exchange and synchronization. Include scenarios for both normal and interrupted communication.",
          "Data Consistency Test Case": "Create a test case to verify that data remains consistent when shared or transferred between modules. Specify test steps for simulating updates, deletions, and retrievals.",
          "Shared Resource Access Test Case": "Design a test case to assess how integrated components handle access to shared resources, avoiding conflicts or deadlocks. Include test steps for concurrent access scenarios.",
          "Error Propagation Test Case": "Create a test case to evaluate the system’s ability to handle and isolate errors in one component without affecting others. Include scenarios to test error logging and recovery mechanisms."
        },
        "customised_prompt_status": false
      }
    ],
    "process_title": "security_testing",
    "document_type": "Technical Design Document",
    "generated_prompt": "As an ISTQB-certified test analyst, design a comprehensive security test scenario for the TaskMaster system, focusing on evaluating its resilience against various security threats. The scenario should identify vulnerabilities, ensure data protection, and validate access controls within the task management application.\n\nPreconditions: A user with administrative privileges is logged in to the TaskMaster system.\nEnvironment Configurations: The system is configured with standard security settings.\nUser Roles:\n- Admin: Can manage tasks, projects, and users.\n- User: Can view and manage their own tasks and projects.\n\nKey areas to be tested:\n- Authentication: User login functionality.\n- Authorization: Access control for task management.\n- Data Encryption: Protection of sensitive data in the system.\n- Input Validation: Preventing malicious input from being executed.\n- Session Management: Handling user sessions securely.\n- Error Handling: System response to errors and exceptions.\n\nCommon security tests:\n- SQL injection: Testing for vulnerabilities in task retrieval and update.\n- XSS: Testing for cross-site scripting vulnerabilities in task descriptions.\n- CSRF: Testing for cross-site request forgery attacks on project management.\n- Brute-force attacks: Testing the system's resistance to repeated login attempts.\n- Privilege escalation: Testing the system's ability to prevent unauthorized access to sensitive data.\n\nExpected outcomes:\n- The system successfully authenticates users and authorizes access to tasks.\n- Sensitive data is protected from unauthorized access.\n- Malicious input is prevented from being executed.\n- User sessions are handled securely.\n- System responses are logged and reported.\n\nThis scenario aligns with ISTQB security testing standards, focusing on thorough vulnerability detection, data integrity, and system robustness against unauthorized access.\n\nInstruction and consistency situation: \nConfirm that each scenario includes essential elements such as objective, preconditions, security vulnerabilities, expected outcomes, and error handling. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.\n\nInstruction and consistency situation: \nAssess the scenario's alignment with the intended test category (security) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant test steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.\n\nScoring situation: \n Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for security testing, including elements like specific security threats, test steps, and expected defensive responses.\n\nScoring situation: \n Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of security testing as per ISTQB guidelines.\n\n\n    JSON Output Structure:\n\n    {\n        \"TestScenarios\": [\n            {\n                \"ScenarioID\": \"security_testing_Test_Scenario_1\",\n                \"Title\": \"<Scenario Title>\",\n                \"Description\": \"<Detailed scenario description at least 3 sentences. This is the most important part of the generate test scenario!>\",\n                \"Objective\": \"<Objective or goal of the scenario>\",\n                \"Category\": \"Security Testing\",\n                \"Comments\": \"<Any inconsistency or additional notes>\"\n            }\n        ]\n    }\n\n    Guidelines for JSON Structure:\n    - Ensure the ScenarioID is dynamic and matches the required format.\n\n    Example JSON output:\n    {\n        \"ScenarioID\": \"Process_A_ModelX_Test_Scenario_1\",\n        \"Title\": \"Verify Login Functionality\",\n        \"Description\": \"Test the login functionality to ensure that users can successfully log in with valid credentials and are rejected with incorrect credentials. Additionally, verify that the system displays appropriate error messages for failed login attempts to guide users in correcting their input. Furthermore, ensure that the login session is maintained correctly, allowing users to access their accounts seamlessly after a successful login.\",\n        \"Objective\": \"Validate user authentication mechanism.\",\n        \"Category\": \"Functional Tests\",\n        \"Comments\": \"\"\n    }\n\n    This document is classified as a Technical Design Document. When generating test scenarios, ensure that the structure, format, and content align with the nature of this document type.\n\n    Document Content: \n    # Technical Design Document: TaskMaster\n\n## 1. Introduction\nTaskMaster is a simple task management system that allows users to manage tasks and projects. The application provides an interface where users can list tasks, add new tasks, and mark tasks as completed.\n\n## 2. Architectural Design\nThe application is built around two main classes: `Task` and `Project`.\n\n### 2.1 Task Class\n- **Attributes:**\n  - `title` (str): The title of the task.\n  - `description` (str): A brief description of the task.\n  - `completed` (bool): A flag indicating whether the task is completed.\n- **Methods:**\n  - `mark_completed()`: Marks the task as completed.\n  - `__str__()`: Returns a string representation of the task status.\n\n### 2.2 Project Class\n- **Attributes:**\n  - `name` (str): The name of the project.\n  - `tasks` (list): A list of tasks associated with the project.\n- **Methods:**\n  - `add_task(task)`: Adds a new task to the project.\n  - `remove_task(task_title)`: Removes a task from the project by its title.\n  - `list_tasks()`: Lists all tasks in the project.\n  - `__str__()`: Returns a string representation of the project's status.\n\n## 3. Flow Diagram\n1. The user creates a `Project` instance.\n2. The user creates one or more `Task` instances under the project.\n3. The user can mark tasks as completed and view the list of tasks with their statuses.\n\n## 4. Modularity and Extensibility\nThe application is designed with modularity in mind, allowing for easy extension of classes and methods for future enhancements.\n\n    ",
    "model_output": {
      "TestScenarios": [
        {
          "ScenarioID": "security_testing_Test_Scenario_1",
          "Title": "Validate User Authentication",
          "Description": "Test the login functionality to ensure that users can successfully log in with valid credentials and are rejected with incorrect credentials. Additionally, verify that the system displays appropriate error messages for failed login attempts to guide users in correcting their input.",
          "Objective": "Validate user authentication mechanism.",
          "Category": "Security Testing",
          "Comments": ""
        },
        {
          "ScenarioID": "security_testing_Test_Scenario_2",
          "Title": "Test Authorization for Task Management",
          "Description": "Ensure that users with the correct privileges can access and manage tasks, while unauthorized users are denied access.",
          "Objective": "Validate authorization for task management.",
          "Category": "Security Testing",
          "Comments": ""
        },
        {
          "ScenarioID": "security_testing_Test_Scenario_3",
          "Title": "Verify Data Encryption",
          "Description": "Test the application's data encryption to ensure that sensitive information is protected from unauthorized access.",
          "Objective": "Validate data encryption mechanism.",
          "Category": "Security Testing",
          "Comments": ""
        },
        {
          "ScenarioID": "security_testing_Test_Scenario_4",
          "Title": "Test Input Validation",
          "Description": "Verify that the application prevents malicious input from being executed, ensuring the security of user data.",
          "Objective": "Validate input validation mechanism.",
          "Category": "Security Testing",
          "Comments": ""
        },
        {
          "ScenarioID": "security_testing_Test_Scenario_5",
          "Title": "Assess Session Management",
          "Description": "Test the application's session management to ensure that user sessions are handled securely, preventing unauthorized access.",
          "Objective": "Validate session management mechanism.",
          "Category": "Security Testing",
          "Comments": ""
        },
        {
          "ScenarioID": "security_testing_Test_Scenario_6",
          "Title": "Evaluate Error Handling",
          "Description": "Test the application's error handling to ensure that system responses are logged and reported, providing valuable information for debugging and security improvements.",
          "Objective": "Validate error handling mechanism.",
          "Category": "Security Testing",
          "Comments": ""
        },
        {
          "ScenarioID": "security_testing_Test_Scenario_7",
          "Title": "Perform SQL Injection Test",
          "Description": "Test the application for vulnerabilities in task retrieval and update, ensuring that no malicious input can be executed.",
          "Objective": "Validate SQL injection prevention mechanism.",
          "Category": "Security Testing",
          "Comments": ""
        },
        {
          "ScenarioID": "security_testing_Test_Scenario_8",
          "Title": "Test Cross-Site Scripting (XSS)",
          "Description": "Verify that the application prevents cross-site scripting vulnerabilities in task descriptions, ensuring user safety.",
          "Objective": "Validate XSS prevention mechanism.",
          "Category": "Security Testing",
          "Comments": ""
        },
        {
          "ScenarioID": "security_testing_Test_Scenario_9",
          "Title": "Assess Cross-Site Request Forgery (CSRF) Protection",
          "Description": "Test the application's resistance to cross-site request forgery attacks on project management, ensuring that user data is protected.",
          "Objective": "Validate CSRF prevention mechanism.",
          "Category": "Security Testing",
          "Comments": ""
        },
        {
          "ScenarioID": "security_testing_Test_Scenario_10",
          "Title": "Test Brute-Force Attack Resistance",
          "Description": "Evaluate the system's resistance to repeated login attempts, ensuring that only authorized users can access the application.",
          "Objective": "Validate brute-force attack prevention mechanism.",
          "Category": "Security Testing",
          "Comments": ""
        },
        {
          "ScenarioID": "security_testing_Test_Scenario_11",
          "Title": "Test Privilege Escalation Prevention",
          "Description": "Verify that the application prevents unauthorized access to sensitive data, ensuring user safety and security.",
          "Objective": "Validate privilege escalation prevention mechanism.",
          "Category": "Security Testing",
          "Comments": ""
        }
        ]
        }
        }
        {
            "_id": {
              "$oid": "678614997c84385a0519cfbe"
            },
            "session_id": "20250114103904",
            "original_prompts": [
              {
                "_id": {
                  "$oid": "673d7efca741904082541a61"
                },
                "test_name": "Functional Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a detailed functional test scenario focused on evaluating the system’s core functionalities against specified requirements. This scenario should assess whether the system performs its intended operations correctly, meeting both functional specifications and expected user behaviors. Clearly define the purpose of the test, including necessary preconditions, setup configurations, and initial data required. Identify the primary functions to be tested, including input conditions, expected outputs, and any error-handling mechanisms. Outline specific test steps that simulate realistic user actions to validate each function. Describe the expected outcomes for each step, ensuring that both positive and negative cases are covered. Emphasize that the scenario aligns with ISTQB functional testing standards, aiming for accuracy, completeness, and robustness in functional validation.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as preconditions, test steps, expected outcomes, and error handling requirements.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for functional testing, including elements like input conditions, functional checks, and expected results.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, expected outcomes, and error-handling match the intended functional testing standards.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete examples for inputs, clear test step definitions, and detailed expected results. Specificity in test data and expected outcomes is essential for robust functional testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of functional testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of functional testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, input conditions, expected outputs, and error handling. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (functional) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant test steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as input conditions, expected results, and error handling. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the system's core functionalities against the specified requirements, focusing on whether it tests intended operations and expected user behaviors as outlined in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for functional testing, such as clear objectives, necessary preconditions, detailed test steps simulating realistic user actions, input conditions, expected outputs, and error handling mechanisms. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with functional testing standards and ISTQB guidelines. Ensure that the test steps are appropriate for the functionalities being tested and that expected outcomes are properly defined. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for functional testing. Consider if the scenario is well-organized, uses precise technical language, and provides clear instructions. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_create_prompts": {
                  "Core Functionalities Test Case": "Create a test case that verifies the core functionalities of the system as defined in the requirements. Include preconditions, detailed steps, input data, and expected results to ensure the system performs its intended operations correctly. Ensure the case aligns with ISTQB Functional Testing guidelines. Specify each step's expected output clearly and concisely.",
                  "Boundary Value Analysis Test Case": "Develop a test case focusing on Boundary Value Analysis (BVA) to evaluate system behavior at the edges of input ranges. Include preconditions, input data at lower and upper boundaries, and the expected system response for each scenario. Define clear test steps for validating boundary conditions, ensuring compliance with functional testing standards.",
                  "Error Recovery Functional Test Case": "Design a test case to assess the system's error recovery mechanisms during normal operations. Include scenarios with invalid inputs, interruptions, or fault conditions. Specify detailed steps to simulate these scenarios and describe the expected system behavior for each step, including recovery or error handling outputs.",
                  "User Input Validation Test Case": "Create a test case to validate the system's ability to handle various user inputs. Include valid, invalid, and edge-case inputs, with steps to simulate input actions and monitor system responses. Specify preconditions, detailed test steps, and expected outcomes to ensure the system handles inputs correctly and provides appropriate feedback for invalid inputs."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, generate a comprehensive set of functional test cases based on the provided test scenario. Ensure that the test cases collectively cover all actions, steps, and functional areas outlined in the scenario. Each test case should include detailed descriptions of preconditions, input conditions, step-by-step actions, and expected outcomes. Cover positive, negative, and boundary cases to ensure complete validation of the system’s functionalities. Include specific checks for error handling and edge cases where applicable. Organize the test cases to ensure clarity and traceability, aligning with ISTQB standards. The goal is to produce the maximum number of test cases necessary to comprehensively validate the scenario while avoiding redundancy.",
                "customised_prompt_status": false
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6551"
                },
                "test_name": "Edge Cases and Boundary Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design detailed test scenarios focused on evaluating the system's response to edge cases and boundary values. The scenario should explore how the system behaves at the extremes of input ranges, identifying potential vulnerabilities and ensuring reliability. Clearly define the purpose of the test, necessary preconditions, and environment configurations to create accurate testing conditions. Specify a range of test cases, including boundary values (just below, at, and just above limits) and edge cases that challenge system assumptions. Describe the expected outcomes for each case, detailing system responses, error handling, or corrective measures as needed. Outline the test steps, ensuring they capture all critical values for each parameter. Emphasize that the scenario aligns with ISTQB standards for boundary testing, aiming to identify weaknesses and ensure robustness against extreme inputs.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as boundary value examples, edge cases, expected outcomes, and specific error handling requirements.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for edge cases and boundary testing, including elements like precise boundary values, error handling expectations, and critical input limits.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, boundary values, and expected outcomes match the intended category and testing standards for boundary and edge case evaluation.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that provide concrete boundary values, clear definitions of acceptable limits, and detailed error handling for edge cases. Specificity in test data and expected results is essential for robust testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of boundary and edge case testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of boundary and edge case testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, boundary values, edge cases, and expected outcomes. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (boundary and edge cases) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as boundary values, edge case examples, and error expectations. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively targets edge cases and boundary values, exploring system behavior at the extremes of input ranges as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for edge case and boundary testing, such as specific boundary values (just below, at, and just above limits), detailed preconditions, test steps, and expected outcomes for each case. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with boundary testing standards and ISTQB guidelines. Check if the test steps correspond correctly to the specified boundary values. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for boundary testing. Consider if the scenario clearly communicates the test conditions and expectations. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, create a detailed set of test cases to evaluate system behavior under edge case scenarios and at boundary input values. Include preconditions, specific test data for both lower and upper limits, and steps to assess how the system handles inputs outside the expected range. Ensure scenarios include both positive and negative test cases to validate robustness and reliability. Align test cases with ISTQB boundary testing guidelines for comprehensive validation of system behavior.",
                "test_case_create_prompts": {
                  "Boundary Value Analysis Test Case": "Design a test case focusing on testing inputs at the boundaries of acceptable ranges. Specify lower and upper limits, just below and above these limits, and expected outcomes for each condition. Include clear steps for validating the system's response to each input.",
                  "Extreme Input Test Case": "Create a test case to evaluate system performance and stability when handling extreme or unrealistic input values. Define test data that pushes the system beyond its designed operational range, specifying the expected error messages or graceful degradation responses.",
                  "Null and Empty Input Test Case": "Develop a test case to assess system behavior when null or empty inputs are provided. Include detailed steps to simulate such inputs and expected outcomes, such as validation error messages or fallback defaults.",
                  "Invalid Format Test Case": "Create a test case to evaluate the system's validation mechanisms when inputs are provided in incorrect formats. Include examples such as malformed dates, special characters in numeric fields, or overly long strings, and specify expected error handling outcomes."
                },
                "customised_prompt_status": false
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6555"
                },
                "test_name": "Security Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive security test scenario aimed at evaluating the system’s resilience against various security threats. The scenario should focus on identifying vulnerabilities, ensuring data protection, and validating access controls within the specified system. Clearly define the purpose of the test, including the preconditions, environment configurations, and user roles required to simulate realistic security conditions. Identify key areas to be tested, such as authentication, authorization, data encryption, input validation, session management, and error handling. Outline test steps that cover common security tests such as SQL injection, XSS, CSRF, brute-force attacks, and privilege escalation attempts. Describe the expected outcomes for each test, including the system’s response to attempted breaches, error messages, and any logging requirements. Emphasize that the scenario aligns with ISTQB security testing standards, focusing on thorough vulnerability detection, data integrity, and system robustness against unauthorized access.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as preconditions, security vulnerabilities to test, expected outcomes, and error handling mechanisms.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for security testing, including elements like specific security threats, test steps, and expected defensive responses.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, security vulnerabilities, and expected outcomes match the intended security testing standards.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete security threat examples, detailed test steps, and clear expected outcomes for each threat. Specificity in vulnerability descriptions and system responses is essential for robust security testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of security testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of security testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, security vulnerabilities, expected outcomes, and error handling. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (security) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant test steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as vulnerability descriptions, expected results, and error handling requirements. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the system's resilience against security threats, focusing on identifying vulnerabilities and validating access controls as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for security testing, such as specific security vulnerabilities to test (e.g., SQL injection, XSS, CSRF), necessary preconditions, user roles, test steps, expected system responses, and error handling mechanisms. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with security testing standards and ISTQB guidelines. Ensure that the security threats and test steps correspond correctly to the objectives. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for security testing. Consider if the scenario is detailed, well-structured, and uses appropriate security terminology. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive set of security test cases to evaluate the system's resilience against threats. Include scenarios for testing authentication, input validation, data encryption, and unauthorized access prevention. Each test case should include detailed preconditions, steps to simulate security vulnerabilities, and expected outcomes. Align all test cases with ISTQB security testing guidelines to ensure thorough validation of system security measures.",
                "test_case_create_prompts": {
                  "Authentication Test Case": "Create a test case to verify the robustness of the system's authentication mechanism, including password strength requirements and protection against brute-force attacks.",
                  "Input Validation Security Test Case": "Develop a test case to test for vulnerabilities such as SQL injection, XSS, or CSRF attacks by providing malicious input. Specify expected outcomes like rejection of input and secure logging.",
                  "Data Encryption Test Case": "Design a test case to assess the effectiveness of encryption for sensitive data during transmission and storage. Specify tools and methods to verify encryption implementation.",
                  "Access Control Test Case": "Create a test case to ensure access control policies are enforced correctly, preventing unauthorized users from accessing restricted features or data."
                },
                "customised_prompt_status": false
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6556"
                },
                "test_name": "User Interface (GUI) Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive GUI test scenario to evaluate the usability, responsiveness, and visual consistency of the user interface for a specified system. This scenario should focus on ensuring that the interface meets design specifications and provides a seamless user experience. Clearly outline the purpose of the test, specifying necessary preconditions, screen resolutions, and device configurations required. Identify key UI elements to be tested, such as buttons, menus, text fields, alignment, colors, font sizes, and responsiveness across different devices. Describe test steps that include checking the interface's layout, navigation flow, interaction feedback, and responsiveness to user inputs. Define expected outcomes for each interaction, including alignment, visual feedback, error messages, and responsiveness criteria. Emphasize that the scenario aligns with ISTQB GUI testing standards, focusing on visual accuracy, and user experience quality.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as preconditions, UI elements, expected outcomes, and visual consistency checks.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for GUI testing, including elements like specific UI components, visual feedback expectations, and responsiveness requirements.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, UI elements, and expected outcomes match the intended GUI testing standards.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete examples for UI elements, detailed test steps, and clear expected results. Specificity in layout checks, responsiveness, and user feedback is essential for comprehensive GUI testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of GUI testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of GUI testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, UI components, expected outcomes, and visual consistency checks. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (GUI) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant UI elements, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as UI elements, layout expectations, and visual feedback. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the usability, responsiveness, and visual consistency of the user interface, focusing on UI elements and user experience as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for GUI testing, such as specific UI elements to be tested (buttons, menus, text fields), necessary preconditions (screen resolutions, device configurations), test steps, expected outcomes (alignment, visual feedback, responsiveness), and visual consistency checks. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with GUI testing standards and ISTQB guidelines. Check if the test steps accurately reflect the UI components and expected user interactions. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for GUI testing. Consider if the scenario effectively communicates the testing of UI elements and user experience aspects. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "As a senior ISTQB-certified test analyst, create a comprehensive set of GUI test cases to validate usability, visual consistency, and responsiveness. Include preconditions, test steps, and expected outcomes for all UI elements. Ensure test cases cover navigation, layout alignment, input handling, and feedback mechanisms. Align with ISTQB GUI testing standards for a seamless and user-friendly experience.",
                "test_case_create_prompts": {
                  "Visual Consistency Test Case": "Create a test case to ensure UI elements are visually consistent with design specifications across resolutions. Specify alignment, color schemes, and font size checks.",
                  "Navigation Flow Test Case": "Develop a test case to verify the intuitiveness of the navigation flow, ensuring users can seamlessly move between pages and access features.",
                  "Input Field Validation Test Case": "Design a test case to test input field behavior, including placeholder text, error messages, and constraints on input formats.",
                  "Responsiveness Test Case": "Create a test case to evaluate how the UI adapts to different screen sizes and orientations. Specify steps to verify element repositioning and integration."
                },
                "customised_prompt_status": false
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6550"
                },
                "test_name": "Compatibility Testing",
                "test_prompt": "Compatibility Testing of User Interface Tasks\n-----------------------------------------------\n\nAs an ISTQB-certified test analyst, this test scenario focuses on evaluating the compatibility of user interface tasks in a Requirements Document, specifically tailored to the 'User Interface' module.\nThe purpose of this test is to ensure that the system functions correctly and consistently across different platforms, devices, operating systems, browsers, and configurations, as outlined in the document content:\n\n1. Users should be able to create, edit, and delete tasks.\n2. Users should organize tasks under specific projects or categories.\n3. Users can mark tasks as completed, pending, or in-progress.\n4. Users can set due dates and receive reminders for tasks.\n5. Users can filter and view tasks based on status (completed, pending, in-progress).\n6. The application should allow users to prioritize tasks.\n7. Users should be able to search for tasks based on keywords or project names.\n8. Users can add notes or attachments to individual tasks.\n\nPreconditions:\n- Access to the system with a valid username and password\n- A project or category with at least one task assigned\n- The user interface is enabled in the system settings\n\nEnvironmental Setups:\n- Specific operating system versions (e.g., Windows 10, macOS High Sierra)\n- Browser types (e.g., Google Chrome, Mozilla Firefox)\n- Device specifications (e.g., laptop, desktop, tablet)\n- Screen resolutions (e.g., 1080p, 4K)\n- Hardware configurations (e.g., Intel Core i5, NVIDIA GeForce GTX 1050)\n\nTest Steps:\n1. Log in to the system with a valid username and password.\n2. Navigate to the project or category with at least one task assigned.\n3. Verify that the user can create, edit, and delete tasks correctly.\n4. Validate that users can organize tasks under specific projects or categories.\n5. Ensure that users can mark tasks as completed, pending, or in-progress accurately.\n6. Test due dates and reminders for tasks.\n7. Filter and view tasks based on status.\n8. Prioritize tasks.\n9. Search for tasks based on keywords or project names.\n10. Add notes or attachments to individual tasks.\n\nExpected Outcomes:\n- The system functions correctly across different platforms, devices, operating systems, browsers, and configurations.\n- Consistent user experience is maintained across environments.\n- No compatibility issues are reported.\n\nTest Scope:\n- This test scenario covers the 'User Interface' module as specified in the Requirements Document.\n- It focuses on evaluating the system's compatibility with various platforms, devices, operating systems, browsers, and configurations.\n\nISTQB Alignment:\n- This test scenario aligns with ISTQB compatibility testing standards, prioritizing cross-environment consistency, reliability, and user experience.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective, structured approach, and including essential elements such as environment configurations, expected outcomes, and compatibility criteria.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for compatibility testing, including specific platform details, and cross-environment checks.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, environment configurations, and expected outcomes match the intended category and testing standards for compatibility evaluation.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete configurations (such as operating system versions, browser types), detailed expected results, and identified compatibility metrics.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of compatibility testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of compatibility testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, environment configurations, expected outcomes, and compatibility checks. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (compatibility) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant configurations, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as platform versions, environment configurations, and expected responses. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively addresses compatibility testing across different environments as specified in the prompt. Consider if it includes testing on various platforms, devices, operating systems, browsers, and configurations. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for compatibility testing, such as detailed environment configurations, preconditions, test steps simulating user behavior on different platforms, and expected outcomes for each environment. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with compatibility testing standards and ISTQB guidelines. Check if the test steps and expected outcomes match the specified environments and configurations. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for compatibility testing. Consider if the scenario is clearly organized, uses technical language appropriately, and provides precise details. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "As a senior ISTQB-certified test analyst, design a comprehensive set of compatibility test cases to evaluate the across different platforms, devices, operating systems, and browsers. Specify preconditions, test steps, and expected outcomes for each scenario. Ensure the cases assess cross-platform consistency, and visual rendering. Align with ISTQB compatibility testing standards to ensure the system provides a seamless user experience in diverse environments.",
                "test_case_create_prompts": {
                  "Cross-Browser Test Case": "Create a test case to ensure consistency across multiple web browsers and their versions. Specify preconditions, browser configurations, detailed steps for validation, and expected outputs for compatibility verification.",
                  "Device Compatibility Test Case": "Develop a test case to evaluate system behavior on a variety of devices, such as smartphones, tablets, and desktops. Include steps to verify responsive design and integrition across device specifications.",
                  "Operating System Compatibility Test Case": "Design a test case to assess system performance on different operating systems, including specific versions. Include steps to test installation, execution, and interoperability between OS-specific features.",
                  "Screen Resolution Test Case": "Create a test case to validate the system's responsiveness and layout consistency across various screen resolutions. Include steps for testing visual elements and alignment across resolutions."
                },
                "customised_prompt_status": true
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6552"
                },
                "test_name": "Input Data Variety Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive test scenario focused on assessing the system’s ability to handle a diverse range of input data. The scenario should evaluate how the system processes various data types, formats, edge cases, and outliers to ensure robust input handling. Clearly define the purpose of the test, the preconditions required, and the environment configurations necessary for accurate testing. Specify a range of input data examples, including valid, invalid, boundary, and null values. Describe the expected system responses, detailing any validation errors, warnings, or specific handling methods required for each type of input. Outline test steps that simulate user behavior with different input scenarios to validate the system’s resilience. Emphasize that the scenario aligns with ISTQB testing standards, ensuring a thorough assessment of data handling robustness and consistency.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective, structured approach, and including all necessary elements such as input data examples, expected outcomes, and error handling.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for input variety testing, including elements like specific data types, validation requirements, and error handling processes.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, data requirements, and expected outcomes match the intended category and testing standards for input variety evaluation.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that include concrete examples of various input data types, specific configurations for the test environment, and clear definitions of expected outcomes for each input category. Specificity in data validation requirements and error expectations is essential.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of input data variety testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of input data variety testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, data examples (valid, invalid, boundary, null), and expected outcomes. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (input data variety) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as input data examples, error expectations, and response definitions. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the system's ability to handle a diverse range of input data, including valid, invalid, boundary, and null values, as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for input data variety testing, such as a range of input data examples, necessary preconditions, detailed test steps, expected system responses, validation errors, and error handling for each type of input. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with input data variety testing standards and ISTQB guidelines. Check if the test steps accurately reflect the diverse input data and expected outcomes. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for input data variety testing. Consider if the scenario effectively communicates the testing of different data types and their expected handling. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, generate a comprehensive set of test cases to validate the system's ability to handle diverse input data types. Include scenarios for valid, invalid, null, and edge-case inputs. Specify detailed steps, expected outcomes, and validation criteria for each input type. Align all test cases with ISTQB testing guidelines to ensure the system’s robustness in input handling.",
                "test_case_create_prompts": {
                  "Valid Input Test Case": "Create a test case to validate the system’s ability to process valid input data accurately. Include examples for various data types such as text, numbers, and dates.",
                  "Invalid Input Test Case": "Develop a test case to test system behavior with invalid inputs, ensuring appropriate error messages or rejections are triggered.",
                  "Boundary Input Test Case": "Design a test case to assess the system’s handling of input values at the boundaries of acceptable ranges, including just below and above these limits.",
                  "Special Character Input Test Case": "Create a test case to evaluate how the system handles inputs with special characters, emojis, or unexpected symbols. Specify expected sanitization or rejection behaviors."
                },
                "customised_prompt_status": false
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6554"
                },
                "test_name": "Performance and Load Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive performance and load testing scenario for a specified system. The scenario should aim to assess the system’s performance and load handling under realistic conditions, simulating user behaviors to evaluate how the system operates under various levels of stress. Clearly establish the purpose of the test, the preconditions required, and the environment configurations necessary to create accurate, real-world conditions. Outline the user load, including distribution and behavior patterns that reflect peak periods, random delays, and sudden surges. Identify key performance metrics such as response time, throughput, resource utilization, and error rates, detailing acceptable thresholds based on industry standards. Specify the data requirements to represent real-world usage, including edge cases, and present a clear, sequential process for test execution from setup to conclusion. Emphasize expected outcomes, define conditions for success and failure, and include guidelines for comprehensive logging and analysis. Ensure that the scenario aligns with ISTQB performance testing standards, incorporates best practices, and prioritizes precision, consistency, and robustness.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, depth, and clarity. A high-quality scenario should align well with the specified test purpose, providing a clear objective and structured approach. It should include all necessary elements such as preconditions, environment configuration, user load details, performance metrics, and expected outcomes.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for performance and load testing, including specified elements like response time, error rates, data requirements, and system behavior under stress.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described testing steps, data requirements, and performance metrics match the intended category and testing standards for performance and load evaluation.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete numbers for user loads, specific configurations for the test environment, and clear definitions of success and failure thresholds. Specificity in metrics and detailed steps for test execution are essential.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of performance and load testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of performance and load testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, user load distribution, key performance metrics, and expected outcomes. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (performance and load) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched metrics or off-target objectives, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as user load numbers, test environment configurations, and threshold definitions. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the system's performance and load handling under various levels of stress, simulating realistic user behaviors and conditions as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for performance and load testing, such as user load details (distribution, behavior patterns), key performance metrics (response time, throughput, resource utilization, error rates), necessary preconditions, environment configurations, and expected outcomes. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with performance and load testing standards and ISTQB guidelines. Check if the test steps accurately reflect the intended load conditions and performance metrics. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for performance and load testing. Consider if the scenario effectively communicates the testing process and performance expectations. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive set of performance and load test cases to evaluate the system's ability to handle various levels of stress. Ensure that the test cases cover response time, throughput, scalability, and stability under different load conditions. Include preconditions, detailed execution steps, specific input data, and measurable expected outcomes for each scenario. Align all test cases with ISTQB performance testing guidelines to validate both typical and edge performance conditions. Emphasize precision, accuracy, and reproducibility in test documentation.",
                "test_case_create_prompts": {
                  "Core Performance Test Case": "Develop a test case to verify the system's ability to handle normal operational loads. Include typical user behavior as input, preconditions for environment setup, and measurable metrics such as response time and system throughput. Ensure test steps simulate realistic user actions and specify precise success criteria.",
                  "Stress Test Case": "Create a test case to evaluate how the system behaves under extreme stress, such as peak user loads or resource exhaustion. Include detailed steps for incrementally increasing load, monitoring system behavior, and identifying failure points. Specify thresholds for acceptable and critical performance levels.",
                  "Scalability Test Case": "Design a test case to assess the system's ability to scale with increasing workload. Include steps to simulate user growth, resource allocation adjustments, and infrastructure elasticity. Define metrics like maximum concurrent users or transactions per second.",
                  "Resource Utilization Test Case": "Create a test case focusing on system resource usage under varying load conditions. Specify steps to measure CPU, memory, disk I/O, and network bandwidth. Include scenarios for detecting bottlenecks and provide acceptable utilization thresholds."
                },
                "customised_prompt_status": false
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6553"
                },
                "test_name": "Integration Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive integration test scenario aimed at evaluating how different modules or components of a specified system interact with each other. This scenario should focus on ensuring that integrated components communicate correctly and work together. Clearly outline the purpose of the test, specifying necessary preconditions, setup configurations, and initial data required. Identify key interactions between components, including input and output data flow, dependencies, and any shared resources. Outline test steps that simulate realistic data exchange and process flows between components, capturing both normal and abnormal interaction patterns. Describe expected outcomes for each interaction, including response times, data integrity, and error handling requirements. Emphasize that the scenario aligns with ISTQB integration testing standards, focusing on accuracy, consistency, and robustness in component interaction validation.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as preconditions, data flow between components, expected outcomes, and error handling mechanisms.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for integration testing, including specific component interactions, data exchange, and expected results.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, component interactions, and expected outcomes match the intended integration testing standards.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that include concrete component interactions, detailed data flows, and clear expected outcomes for each integration point. Specificity in data handling and error scenarios is essential for thorough integration testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of integration testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of integration testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, data flow between components, expected outcomes, and error handling. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (integration) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant test steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as component interactions, expected results, and error handling mechanisms. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the interactions between different modules or components of the system, focusing on data flow and communication between components as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for integration testing, such as detailed component interactions, necessary preconditions, data exchange processes, dependencies, shared resources, and expected outcomes for each interaction. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with integration testing standards and ISTQB guidelines. Ensure that the interactions between components are correctly represented and that expected outcomes are properly defined. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for integration testing. Consider if the scenario clearly outlines component interactions and is structured logically. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "As a senior ISTQB-certified test analyst, generate a set of integration test cases to evaluate interactions between system components. Include detailed preconditions, interaction sequences, and data flow verification steps. Focus on validating module communication, data consistency, and error handling. Ensure the test cases align with ISTQB integration testing standards.",
                "test_case_create_prompts": {
                  "Interface Communication Test Case": "Develop a test case to validate communication between integrated components, ensuring proper data exchange and synchronization. Include scenarios for both normal and interrupted communication.",
                  "Data Consistency Test Case": "Create a test case to verify that data remains consistent when shared or transferred between modules. Specify test steps for simulating updates, deletions, and retrievals.",
                  "Shared Resource Access Test Case": "Design a test case to assess how integrated components handle access to shared resources, avoiding conflicts or deadlocks. Include test steps for concurrent access scenarios.",
                  "Error Propagation Test Case": "Create a test case to evaluate the system’s ability to handle and isolate errors in one component without affecting others. Include scenarios to test error logging and recovery mechanisms."
                },
                "customised_prompt_status": false
              }
            ],
            "process_title": "compatibility_testing",
            "document_type": "Requirements Document",
            "generated_prompt": "Compatibility Testing of User Interface Tasks\n-----------------------------------------------\n\nAs an ISTQB-certified test analyst, this test scenario focuses on evaluating the compatibility of user interface tasks in a Requirements Document, specifically tailored to the 'User Interface' module.\nThe purpose of this test is to ensure that the system functions correctly and consistently across different platforms, devices, operating systems, browsers, and configurations, as outlined in the document content:\n\n1. Users should be able to create, edit, and delete tasks.\n2. Users should organize tasks under specific projects or categories.\n3. Users can mark tasks as completed, pending, or in-progress.\n4. Users can set due dates and receive reminders for tasks.\n5. Users can filter and view tasks based on status (completed, pending, in-progress).\n6. The application should allow users to prioritize tasks.\n7. Users should be able to search for tasks based on keywords or project names.\n8. Users can add notes or attachments to individual tasks.\n\nPreconditions:\n- Access to the system with a valid username and password\n- A project or category with at least one task assigned\n- The user interface is enabled in the system settings\n\nEnvironmental Setups:\n- Specific operating system versions (e.g., Windows 10, macOS High Sierra)\n- Browser types (e.g., Google Chrome, Mozilla Firefox)\n- Device specifications (e.g., laptop, desktop, tablet)\n- Screen resolutions (e.g., 1080p, 4K)\n- Hardware configurations (e.g., Intel Core i5, NVIDIA GeForce GTX 1050)\n\nTest Steps:\n1. Log in to the system with a valid username and password.\n2. Navigate to the project or category with at least one task assigned.\n3. Verify that the user can create, edit, and delete tasks correctly.\n4. Validate that users can organize tasks under specific projects or categories.\n5. Ensure that users can mark tasks as completed, pending, or in-progress accurately.\n6. Test due dates and reminders for tasks.\n7. Filter and view tasks based on status.\n8. Prioritize tasks.\n9. Search for tasks based on keywords or project names.\n10. Add notes or attachments to individual tasks.\n\nExpected Outcomes:\n- The system functions correctly across different platforms, devices, operating systems, browsers, and configurations.\n- Consistent user experience is maintained across environments.\n- No compatibility issues are reported.\n\nTest Scope:\n- This test scenario covers the 'User Interface' module as specified in the Requirements Document.\n- It focuses on evaluating the system's compatibility with various platforms, devices, operating systems, browsers, and configurations.\n\nISTQB Alignment:\n- This test scenario aligns with ISTQB compatibility testing standards, prioritizing cross-environment consistency, reliability, and user experience.\n\nInstruction and consistency situation: \nAssess the scenario's alignment with the intended test category (compatibility) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant configurations, should be highlighted as 'Inconsistent' in the output with an explanation for review.\n\nScoring situation: \n Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete configurations (such as operating system versions, browser types), detailed expected results, and identified compatibility metrics.\n\n\n    JSON Output Structure:\n\n    {\n        \"TestScenarios\": [\n            {\n                \"ScenarioID\": \"compatibility_testing_Test_Scenario_1\",\n                \"Title\": \"<Scenario Title>\",\n                \"Description\": \"<Detailed scenario description at least 3 sentences. This is the most important part of the generate test scenario!>\",\n                \"Objective\": \"<Objective or goal of the scenario>\",\n                \"Category\": \"Compatibility Testing\",\n                \"Comments\": \"<Any inconsistency or additional notes>\"\n            }\n        ]\n    }\n\n    Guidelines for JSON Structure:\n    - Ensure the ScenarioID is dynamic and matches the required format.\n\n    Example JSON output:\n    {\n        \"ScenarioID\": \"Process_A_ModelX_Test_Scenario_1\",\n        \"Title\": \"Verify Login Functionality\",\n        \"Description\": \"Test the login functionality to ensure that users can successfully log in with valid credentials and are rejected with incorrect credentials. Additionally, verify that the system displays appropriate error messages for failed login attempts to guide users in correcting their input. Furthermore, ensure that the login session is maintained correctly, allowing users to access their accounts seamlessly after a successful login.\",\n        \"Objective\": \"Validate user authentication mechanism.\",\n        \"Category\": \"Functional Tests\",\n        \"Comments\": \"\"\n    }\n\n    This document is classified as a Requirements Document. When generating test scenarios, ensure that the structure, format, and content align with the nature of this document type.\n\n    Document Content: \n    1. Users should be able to create, edit, and delete tasks.\r\n2. Users should organize tasks under specific projects or categories.\r\n3. Users can mark tasks as completed, pending, or in progress.\r\n4. Users can set due dates and receive reminders for tasks.\r\n5. Users can filter and view tasks based on status (completed, pending, in-progress).\r\n6. The application should allow users to prioritize tasks.\r\n7. Users should be able to search for tasks based on keywords or project names.\r\n8. Users can add notes or attachments to individual tasks.\r\n\n    ",
            "model_output": {
              "TestScenarios": [
                {
                  "ScenarioID": "compatibility_testing_Test_Scenario_1",
                  "Title": "Compatibility Testing of User Interface Tasks",
                  "Description": "This test scenario evaluates the compatibility of user interface tasks in a Requirements Document, specifically tailored to the 'User Interface' module.",
                  "Objective": "Ensure that the system functions correctly and consistently across different platforms, devices, operating systems, browsers, and configurations.",
                  "Category": "Compatibility Testing",
                  "Comments": ""
                }
              ],
            }
          }
          {
            "_id": {
              "$oid": "67860eb2b27b720c109a8fb1"
            },
            "session_id": "20250114101352",
            "original_prompts": [
              {
                "_id": {
                  "$oid": "673d7efca741904082541a61"
                },
                "test_name": "Functional Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a detailed functional test scenario focused on evaluating the system’s core functionalities against specified requirements. This scenario should assess whether the system performs its intended operations correctly, meeting both functional specifications and expected user behaviors. Clearly define the purpose of the test, including necessary preconditions, setup configurations, and initial data required. Identify the primary functions to be tested, including input conditions, expected outputs, and any error-handling mechanisms. Outline specific test steps that simulate realistic user actions to validate each function. Describe the expected outcomes for each step, ensuring that both positive and negative cases are covered. Emphasize that the scenario aligns with ISTQB functional testing standards, aiming for accuracy, completeness, and robustness in functional validation.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as preconditions, test steps, expected outcomes, and error handling requirements.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for functional testing, including elements like input conditions, functional checks, and expected results.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, expected outcomes, and error-handling match the intended functional testing standards.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete examples for inputs, clear test step definitions, and detailed expected results. Specificity in test data and expected outcomes is essential for robust functional testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of functional testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of functional testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, input conditions, expected outputs, and error handling. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (functional) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant test steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as input conditions, expected results, and error handling. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the system's core functionalities against the specified requirements, focusing on whether it tests intended operations and expected user behaviors as outlined in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for functional testing, such as clear objectives, necessary preconditions, detailed test steps simulating realistic user actions, input conditions, expected outputs, and error handling mechanisms. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with functional testing standards and ISTQB guidelines. Ensure that the test steps are appropriate for the functionalities being tested and that expected outcomes are properly defined. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for functional testing. Consider if the scenario is well-organized, uses precise technical language, and provides clear instructions. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_create_prompts": {
                  "Core Functionalities Test Case": "Create a test case that verifies the core functionalities of the system as defined in the requirements. Include preconditions, detailed steps, input data, and expected results to ensure the system performs its intended operations correctly. Ensure the case aligns with ISTQB Functional Testing guidelines. Specify each step's expected output clearly and concisely.",
                  "Boundary Value Analysis Test Case": "Develop a test case focusing on Boundary Value Analysis (BVA) to evaluate system behavior at the edges of input ranges. Include preconditions, input data at lower and upper boundaries, and the expected system response for each scenario. Define clear test steps for validating boundary conditions, ensuring compliance with functional testing standards.",
                  "Error Recovery Functional Test Case": "Design a test case to assess the system's error recovery mechanisms during normal operations. Include scenarios with invalid inputs, interruptions, or fault conditions. Specify detailed steps to simulate these scenarios and describe the expected system behavior for each step, including recovery or error handling outputs.",
                  "User Input Validation Test Case": "Create a test case to validate the system's ability to handle various user inputs. Include valid, invalid, and edge-case inputs, with steps to simulate input actions and monitor system responses. Specify preconditions, detailed test steps, and expected outcomes to ensure the system handles inputs correctly and provides appropriate feedback for invalid inputs."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, generate a comprehensive set of functional test cases based on the provided test scenario. Ensure that the test cases collectively cover all actions, steps, and functional areas outlined in the scenario. Each test case should include detailed descriptions of preconditions, input conditions, step-by-step actions, and expected outcomes. Cover positive, negative, and boundary cases to ensure complete validation of the system’s functionalities. Include specific checks for error handling and edge cases where applicable. Organize the test cases to ensure clarity and traceability, aligning with ISTQB standards. The goal is to produce the maximum number of test cases necessary to comprehensively validate the scenario while avoiding redundancy.",
                "customised_prompt_status": false
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6551"
                },
                "test_name": "Edge Cases and Boundary Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design detailed test scenarios focused on evaluating the system's response to edge cases and boundary values. The scenario should explore how the system behaves at the extremes of input ranges, identifying potential vulnerabilities and ensuring reliability. Clearly define the purpose of the test, necessary preconditions, and environment configurations to create accurate testing conditions. Specify a range of test cases, including boundary values (just below, at, and just above limits) and edge cases that challenge system assumptions. Describe the expected outcomes for each case, detailing system responses, error handling, or corrective measures as needed. Outline the test steps, ensuring they capture all critical values for each parameter. Emphasize that the scenario aligns with ISTQB standards for boundary testing, aiming to identify weaknesses and ensure robustness against extreme inputs.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as boundary value examples, edge cases, expected outcomes, and specific error handling requirements.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for edge cases and boundary testing, including elements like precise boundary values, error handling expectations, and critical input limits.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, boundary values, and expected outcomes match the intended category and testing standards for boundary and edge case evaluation.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that provide concrete boundary values, clear definitions of acceptable limits, and detailed error handling for edge cases. Specificity in test data and expected results is essential for robust testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of boundary and edge case testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of boundary and edge case testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, boundary values, edge cases, and expected outcomes. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (boundary and edge cases) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as boundary values, edge case examples, and error expectations. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively targets edge cases and boundary values, exploring system behavior at the extremes of input ranges as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for edge case and boundary testing, such as specific boundary values (just below, at, and just above limits), detailed preconditions, test steps, and expected outcomes for each case. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with boundary testing standards and ISTQB guidelines. Check if the test steps correspond correctly to the specified boundary values. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for boundary testing. Consider if the scenario clearly communicates the test conditions and expectations. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, create a detailed set of test cases to evaluate system behavior under edge case scenarios and at boundary input values. Include preconditions, specific test data for both lower and upper limits, and steps to assess how the system handles inputs outside the expected range. Ensure scenarios include both positive and negative test cases to validate robustness and reliability. Align test cases with ISTQB boundary testing guidelines for comprehensive validation of system behavior.",
                "test_case_create_prompts": {
                  "Boundary Value Analysis Test Case": "Design a test case focusing on testing inputs at the boundaries of acceptable ranges. Specify lower and upper limits, just below and above these limits, and expected outcomes for each condition. Include clear steps for validating the system's response to each input.",
                  "Extreme Input Test Case": "Create a test case to evaluate system performance and stability when handling extreme or unrealistic input values. Define test data that pushes the system beyond its designed operational range, specifying the expected error messages or graceful degradation responses.",
                  "Null and Empty Input Test Case": "Develop a test case to assess system behavior when null or empty inputs are provided. Include detailed steps to simulate such inputs and expected outcomes, such as validation error messages or fallback defaults.",
                  "Invalid Format Test Case": "Create a test case to evaluate the system's validation mechanisms when inputs are provided in incorrect formats. Include examples such as malformed dates, special characters in numeric fields, or overly long strings, and specify expected error handling outcomes."
                },
                "customised_prompt_status": false
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6555"
                },
                "test_name": "Security Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive security test scenario aimed at evaluating the system’s resilience against various security threats. The scenario should focus on identifying vulnerabilities, ensuring data protection, and validating access controls within the specified system. Clearly define the purpose of the test, including the preconditions, environment configurations, and user roles required to simulate realistic security conditions. Identify key areas to be tested, such as authentication, authorization, data encryption, input validation, session management, and error handling. Outline test steps that cover common security tests such as SQL injection, XSS, CSRF, brute-force attacks, and privilege escalation attempts. Describe the expected outcomes for each test, including the system’s response to attempted breaches, error messages, and any logging requirements. Emphasize that the scenario aligns with ISTQB security testing standards, focusing on thorough vulnerability detection, data integrity, and system robustness against unauthorized access.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as preconditions, security vulnerabilities to test, expected outcomes, and error handling mechanisms.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for security testing, including elements like specific security threats, test steps, and expected defensive responses.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, security vulnerabilities, and expected outcomes match the intended security testing standards.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete security threat examples, detailed test steps, and clear expected outcomes for each threat. Specificity in vulnerability descriptions and system responses is essential for robust security testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of security testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of security testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, security vulnerabilities, expected outcomes, and error handling. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (security) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant test steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as vulnerability descriptions, expected results, and error handling requirements. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the system's resilience against security threats, focusing on identifying vulnerabilities and validating access controls as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for security testing, such as specific security vulnerabilities to test (e.g., SQL injection, XSS, CSRF), necessary preconditions, user roles, test steps, expected system responses, and error handling mechanisms. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with security testing standards and ISTQB guidelines. Ensure that the security threats and test steps correspond correctly to the objectives. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for security testing. Consider if the scenario is detailed, well-structured, and uses appropriate security terminology. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive set of security test cases to evaluate the system's resilience against threats. Include scenarios for testing authentication, input validation, data encryption, and unauthorized access prevention. Each test case should include detailed preconditions, steps to simulate security vulnerabilities, and expected outcomes. Align all test cases with ISTQB security testing guidelines to ensure thorough validation of system security measures.",
                "test_case_create_prompts": {
                  "Authentication Test Case": "Create a test case to verify the robustness of the system's authentication mechanism, including password strength requirements and protection against brute-force attacks.",
                  "Input Validation Security Test Case": "Develop a test case to test for vulnerabilities such as SQL injection, XSS, or CSRF attacks by providing malicious input. Specify expected outcomes like rejection of input and secure logging.",
                  "Data Encryption Test Case": "Design a test case to assess the effectiveness of encryption for sensitive data during transmission and storage. Specify tools and methods to verify encryption implementation.",
                  "Access Control Test Case": "Create a test case to ensure access control policies are enforced correctly, preventing unauthorized users from accessing restricted features or data."
                },
                "customised_prompt_status": false
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6556"
                },
                "test_name": "User Interface (GUI) Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive GUI test scenario to evaluate the usability, responsiveness, and visual consistency of the user interface for a specified system. This scenario should focus on ensuring that the interface meets design specifications and provides a seamless user experience. Clearly outline the purpose of the test, specifying necessary preconditions, screen resolutions, and device configurations required. Identify key UI elements to be tested, such as buttons, menus, text fields, alignment, colors, font sizes, and responsiveness across different devices. Describe test steps that include checking the interface's layout, navigation flow, interaction feedback, and responsiveness to user inputs. Define expected outcomes for each interaction, including alignment, visual feedback, error messages, and responsiveness criteria. Emphasize that the scenario aligns with ISTQB GUI testing standards, focusing on visual accuracy, and user experience quality.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as preconditions, UI elements, expected outcomes, and visual consistency checks.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for GUI testing, including elements like specific UI components, visual feedback expectations, and responsiveness requirements.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, UI elements, and expected outcomes match the intended GUI testing standards.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete examples for UI elements, detailed test steps, and clear expected results. Specificity in layout checks, responsiveness, and user feedback is essential for comprehensive GUI testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of GUI testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of GUI testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, UI components, expected outcomes, and visual consistency checks. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (GUI) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant UI elements, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as UI elements, layout expectations, and visual feedback. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the usability, responsiveness, and visual consistency of the user interface, focusing on UI elements and user experience as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for GUI testing, such as specific UI elements to be tested (buttons, menus, text fields), necessary preconditions (screen resolutions, device configurations), test steps, expected outcomes (alignment, visual feedback, responsiveness), and visual consistency checks. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with GUI testing standards and ISTQB guidelines. Check if the test steps accurately reflect the UI components and expected user interactions. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for GUI testing. Consider if the scenario effectively communicates the testing of UI elements and user experience aspects. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "As a senior ISTQB-certified test analyst, create a comprehensive set of GUI test cases to validate usability, visual consistency, and responsiveness. Include preconditions, test steps, and expected outcomes for all UI elements. Ensure test cases cover navigation, layout alignment, input handling, and feedback mechanisms. Align with ISTQB GUI testing standards for a seamless and user-friendly experience.",
                "test_case_create_prompts": {
                  "Visual Consistency Test Case": "Create a test case to ensure UI elements are visually consistent with design specifications across resolutions. Specify alignment, color schemes, and font size checks.",
                  "Navigation Flow Test Case": "Develop a test case to verify the intuitiveness of the navigation flow, ensuring users can seamlessly move between pages and access features.",
                  "Input Field Validation Test Case": "Design a test case to test input field behavior, including placeholder text, error messages, and constraints on input formats.",
                  "Responsiveness Test Case": "Create a test case to evaluate how the UI adapts to different screen sizes and orientations. Specify steps to verify element repositioning and integration."
                },
                "customised_prompt_status": false
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6550"
                },
                "test_name": "Compatibility Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design comprehensive test scenarios to evaluate the compatibility of a specified system across different environments. This scenario should focus on assessing the system’s ability to function as expected on various platforms, devices, operating systems, browsers, and configurations. Clearly outline the purpose of the test, including the preconditions and environmental setups required for accurate testing across multiple platforms. Specify a range of environments, such as specific operating system versions, browser types, device specifications, screen resolutions, and hardware configurations, ensuring broad coverage. Describe the expected outcomes for each environment, identifying potential compatibility issues, or visual rendering differences. Detail test steps that simulate user behavior in each environment to validate consistent performance. Emphasize that the scenario aligns with ISTQB compatibility testing standards, prioritizing cross-environment consistency, reliability, and user experience.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective, structured approach, and including essential elements such as environment configurations, expected outcomes, and compatibility criteria.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for compatibility testing, including specific platform details, and cross-environment checks.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, environment configurations, and expected outcomes match the intended category and testing standards for compatibility evaluation.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete configurations (such as operating system versions, browser types), detailed expected results, and identified compatibility metrics.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of compatibility testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of compatibility testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, environment configurations, expected outcomes, and compatibility checks. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (compatibility) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant configurations, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as platform versions, environment configurations, and expected responses. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively addresses compatibility testing across different environments as specified in the prompt. Consider if it includes testing on various platforms, devices, operating systems, browsers, and configurations. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for compatibility testing, such as detailed environment configurations, preconditions, test steps simulating user behavior on different platforms, and expected outcomes for each environment. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with compatibility testing standards and ISTQB guidelines. Check if the test steps and expected outcomes match the specified environments and configurations. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for compatibility testing. Consider if the scenario is clearly organized, uses technical language appropriately, and provides precise details. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "As a senior ISTQB-certified test analyst, design a comprehensive set of compatibility test cases to evaluate the across different platforms, devices, operating systems, and browsers. Specify preconditions, test steps, and expected outcomes for each scenario. Ensure the cases assess cross-platform consistency, and visual rendering. Align with ISTQB compatibility testing standards to ensure the system provides a seamless user experience in diverse environments.",
                "test_case_create_prompts": {
                  "Cross-Browser Test Case": "Create a test case to ensure consistency across multiple web browsers and their versions. Specify preconditions, browser configurations, detailed steps for validation, and expected outputs for compatibility verification.",
                  "Device Compatibility Test Case": "Develop a test case to evaluate system behavior on a variety of devices, such as smartphones, tablets, and desktops. Include steps to verify responsive design and integrition across device specifications.",
                  "Operating System Compatibility Test Case": "Design a test case to assess system performance on different operating systems, including specific versions. Include steps to test installation, execution, and interoperability between OS-specific features.",
                  "Screen Resolution Test Case": "Create a test case to validate the system's responsiveness and layout consistency across various screen resolutions. Include steps for testing visual elements and alignment across resolutions."
                },
                "customised_prompt_status": false
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6552"
                },
                "test_name": "Input Data Variety Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive test scenario focused on assessing the system’s ability to handle a diverse range of input data. The scenario should evaluate how the system processes various data types, formats, edge cases, and outliers to ensure robust input handling. Clearly define the purpose of the test, the preconditions required, and the environment configurations necessary for accurate testing. Specify a range of input data examples, including valid, invalid, boundary, and null values. Describe the expected system responses, detailing any validation errors, warnings, or specific handling methods required for each type of input. Outline test steps that simulate user behavior with different input scenarios to validate the system’s resilience. Emphasize that the scenario aligns with ISTQB testing standards, ensuring a thorough assessment of data handling robustness and consistency.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective, structured approach, and including all necessary elements such as input data examples, expected outcomes, and error handling.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for input variety testing, including elements like specific data types, validation requirements, and error handling processes.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, data requirements, and expected outcomes match the intended category and testing standards for input variety evaluation.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that include concrete examples of various input data types, specific configurations for the test environment, and clear definitions of expected outcomes for each input category. Specificity in data validation requirements and error expectations is essential.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of input data variety testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of input data variety testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, data examples (valid, invalid, boundary, null), and expected outcomes. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (input data variety) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as input data examples, error expectations, and response definitions. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the system's ability to handle a diverse range of input data, including valid, invalid, boundary, and null values, as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for input data variety testing, such as a range of input data examples, necessary preconditions, detailed test steps, expected system responses, validation errors, and error handling for each type of input. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with input data variety testing standards and ISTQB guidelines. Check if the test steps accurately reflect the diverse input data and expected outcomes. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for input data variety testing. Consider if the scenario effectively communicates the testing of different data types and their expected handling. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, generate a comprehensive set of test cases to validate the system's ability to handle diverse input data types. Include scenarios for valid, invalid, null, and edge-case inputs. Specify detailed steps, expected outcomes, and validation criteria for each input type. Align all test cases with ISTQB testing guidelines to ensure the system’s robustness in input handling.",
                "test_case_create_prompts": {
                  "Valid Input Test Case": "Create a test case to validate the system’s ability to process valid input data accurately. Include examples for various data types such as text, numbers, and dates.",
                  "Invalid Input Test Case": "Develop a test case to test system behavior with invalid inputs, ensuring appropriate error messages or rejections are triggered.",
                  "Boundary Input Test Case": "Design a test case to assess the system’s handling of input values at the boundaries of acceptable ranges, including just below and above these limits.",
                  "Special Character Input Test Case": "Create a test case to evaluate how the system handles inputs with special characters, emojis, or unexpected symbols. Specify expected sanitization or rejection behaviors."
                },
                "customised_prompt_status": false
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6554"
                },
                "test_name": "Performance and Load Testing",
                "test_prompt": "As a senior ISTQB-certified test analyst, design a comprehensive performance and load testing scenario for the provided source code. The scenario should aim to assess the system's performance and load handling under realistic conditions, simulating user behaviors to evaluate how the system operates under various levels of stress. Clearly establish the purpose of the test, the preconditions required, and the environment configurations necessary to create accurate, real-world conditions. Outline the user load, including distribution and behavior patterns that reflect peak periods, random delays, and sudden surges. Identify key performance metrics such as response time, throughput, resource utilization, and error rates, detailing acceptable thresholds based on industry standards. Specify the data requirements to represent real-world usage, including edge cases, and present a clear, sequential process for test execution from setup to conclusion. Emphasize expected outcomes, define conditions for success and failure, and include guidelines for comprehensive logging and analysis. Ensure that the scenario aligns with ISTQB performance testing standards, incorporates best practices, and prioritizes precision, consistency, and robustness.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, depth, and clarity. A high-quality scenario should align well with the specified test purpose, providing a clear objective and structured approach. It should include all necessary elements such as preconditions, environment configuration, user load details, performance metrics, and expected outcomes.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for performance and load testing, including specified elements like response time, error rates, data requirements, and system behavior under stress.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described testing steps, data requirements, and performance metrics match the intended category and testing standards for performance and load evaluation.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete numbers for user loads, specific configurations for the test environment, and clear definitions of success and failure thresholds. Specificity in metrics and detailed steps for test execution are essential.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of performance and load testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of performance and load testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, user load distribution, key performance metrics, and expected outcomes. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (performance and load) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched metrics or off-target objectives, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as user load numbers, test environment configurations, and threshold definitions. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the system's performance and load handling under various levels of stress, simulating realistic user behaviors and conditions as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for performance and load testing, such as user load details (distribution, behavior patterns), key performance metrics (response time, throughput, resource utilization, error rates), necessary preconditions, environment configurations, and expected outcomes. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with performance and load testing standards and ISTQB guidelines. Check if the test steps accurately reflect the intended load conditions and performance metrics. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for performance and load testing. Consider if the scenario effectively communicates the testing process and performance expectations. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive set of performance and load test cases to evaluate the system's ability to handle various levels of stress. Ensure that the test cases cover response time, throughput, scalability, and stability under different load conditions. Include preconditions, detailed execution steps, specific input data, and measurable expected outcomes for each scenario. Align all test cases with ISTQB performance testing guidelines to validate both typical and edge performance conditions. Emphasize precision, accuracy, and reproducibility in test documentation.",
                "test_case_create_prompts": {
                  "Core Performance Test Case": "Develop a test case to verify the system's ability to handle normal operational loads. Include typical user behavior as input, preconditions for environment setup, and measurable metrics such as response time and system throughput. Ensure test steps simulate realistic user actions and specify precise success criteria.",
                  "Stress Test Case": "Create a test case to evaluate how the system behaves under extreme stress, such as peak user loads or resource exhaustion. Include detailed steps for incrementally increasing load, monitoring system behavior, and identifying failure points. Specify thresholds for acceptable and critical performance levels.",
                  "Scalability Test Case": "Design a test case to assess the system's ability to scale with increasing workload. Include steps to simulate user growth, resource allocation adjustments, and infrastructure elasticity. Define metrics like maximum concurrent users or transactions per second.",
                  "Resource Utilization Test Case": "Create a test case focusing on system resource usage under varying load conditions. Specify steps to measure CPU, memory, disk I/O, and network bandwidth. Include scenarios for detecting bottlenecks and provide acceptable utilization thresholds."
                },
                "customised_prompt_status": true
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6553"
                },
                "test_name": "Integration Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive integration test scenario aimed at evaluating how different modules or components of a specified system interact with each other. This scenario should focus on ensuring that integrated components communicate correctly and work together. Clearly outline the purpose of the test, specifying necessary preconditions, setup configurations, and initial data required. Identify key interactions between components, including input and output data flow, dependencies, and any shared resources. Outline test steps that simulate realistic data exchange and process flows between components, capturing both normal and abnormal interaction patterns. Describe expected outcomes for each interaction, including response times, data integrity, and error handling requirements. Emphasize that the scenario aligns with ISTQB integration testing standards, focusing on accuracy, consistency, and robustness in component interaction validation.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as preconditions, data flow between components, expected outcomes, and error handling mechanisms.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for integration testing, including specific component interactions, data exchange, and expected results.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, component interactions, and expected outcomes match the intended integration testing standards.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that include concrete component interactions, detailed data flows, and clear expected outcomes for each integration point. Specificity in data handling and error scenarios is essential for thorough integration testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of integration testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of integration testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, data flow between components, expected outcomes, and error handling. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (integration) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant test steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as component interactions, expected results, and error handling mechanisms. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the interactions between different modules or components of the system, focusing on data flow and communication between components as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for integration testing, such as detailed component interactions, necessary preconditions, data exchange processes, dependencies, shared resources, and expected outcomes for each interaction. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with integration testing standards and ISTQB guidelines. Ensure that the interactions between components are correctly represented and that expected outcomes are properly defined. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for integration testing. Consider if the scenario clearly outlines component interactions and is structured logically. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "As a senior ISTQB-certified test analyst, generate a set of integration test cases to evaluate interactions between system components. Include detailed preconditions, interaction sequences, and data flow verification steps. Focus on validating module communication, data consistency, and error handling. Ensure the test cases align with ISTQB integration testing standards.",
                "test_case_create_prompts": {
                  "Interface Communication Test Case": "Develop a test case to validate communication between integrated components, ensuring proper data exchange and synchronization. Include scenarios for both normal and interrupted communication.",
                  "Data Consistency Test Case": "Create a test case to verify that data remains consistent when shared or transferred between modules. Specify test steps for simulating updates, deletions, and retrievals.",
                  "Shared Resource Access Test Case": "Design a test case to assess how integrated components handle access to shared resources, avoiding conflicts or deadlocks. Include test steps for concurrent access scenarios.",
                  "Error Propagation Test Case": "Create a test case to evaluate the system’s ability to handle and isolate errors in one component without affecting others. Include scenarios to test error logging and recovery mechanisms."
                },
                "customised_prompt_status": false
              }
            ],
            "process_title": "performance_and_loading_test",
            "document_type": "Source Code",
            "generated_prompt": "As a senior ISTQB-certified test analyst, design a comprehensive performance and load testing scenario for the provided source code. The scenario should aim to assess the system's performance and load handling under realistic conditions, simulating user behaviors to evaluate how the system operates under various levels of stress. Clearly establish the purpose of the test, the preconditions required, and the environment configurations necessary to create accurate, real-world conditions. Outline the user load, including distribution and behavior patterns that reflect peak periods, random delays, and sudden surges. Identify key performance metrics such as response time, throughput, resource utilization, and error rates, detailing acceptable thresholds based on industry standards. Specify the data requirements to represent real-world usage, including edge cases, and present a clear, sequential process for test execution from setup to conclusion. Emphasize expected outcomes, define conditions for success and failure, and include guidelines for comprehensive logging and analysis. Ensure that the scenario aligns with ISTQB performance testing standards, incorporates best practices, and prioritizes precision, consistency, and robustness.\n\nInstruction and consistency situation: \nEnsure that each generated scenario directly corresponds to the specified context and purpose of performance and load testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.\n\nScoring situation: \n Assign a score (1-5) based on the scenario’s relevance, depth, and clarity. A high-quality scenario should align well with the specified test purpose, providing a clear objective and structured approach. It should include all necessary elements such as preconditions, environment configuration, user load details, performance metrics, and expected outcomes.\n\n\n    JSON Output Structure:\n\n    {\n        \"TestScenarios\": [\n            {\n                \"ScenarioID\": \"performance_and_loading_test_Test_Scenario_1\",\n                \"Title\": \"<Scenario Title>\",\n                \"Description\": \"<Detailed scenario description at least 3 sentences. This is the most important part of the generate test scenario!>\",\n                \"Objective\": \"<Objective or goal of the scenario>\",\n                \"Category\": \"Performance and Load Testing\",\n                \"Comments\": \"<Any inconsistency or additional notes>\"\n            }\n        ]\n    }\n\n    Guidelines for JSON Structure:\n    - Ensure the ScenarioID is dynamic and matches the required format.\n\n    Example JSON output:\n    {\n        \"ScenarioID\": \"Process_A_ModelX_Test_Scenario_1\",\n        \"Title\": \"Verify Login Functionality\",\n        \"Description\": \"Test the login functionality to ensure that users can successfully log in with valid credentials and are rejected with incorrect credentials. Additionally, verify that the system displays appropriate error messages for failed login attempts to guide users in correcting their input. Furthermore, ensure that the login session is maintained correctly, allowing users to access their accounts seamlessly after a successful login.\",\n        \"Objective\": \"Validate user authentication mechanism.\",\n        \"Category\": \"Functional Tests\",\n        \"Comments\": \"\"\n    }\n\n    This document is classified as a Source Code. When generating test scenarios, ensure that the structure, format, and content align with the nature of this document type.\n\n    Document Content: \n    # taskmaster.py\n\nclass Task:\n    def __init__(self, title, description, completed=False):\n        self.title = title\n        self.description = description\n        self.completed = completed\n\n    def mark_completed(self):\n        \"\"\"Mark the task as completed.\"\"\"\n        self.completed = True\n\n    def __str__(self):\n        \"\"\"Return a string representation of the task.\"\"\"\n        status = \"Completed\" if self.completed else \"Pending\"\n        return f\"Task: {self.title}, Status: {status}\"\n\n\nclass Project:\n    def __init__(self, name):\n        self.name = name\n        self.tasks = []\n\n    def add_task(self, task):\n        \"\"\"Add a task to the project.\"\"\"\n        if isinstance(task, Task):\n            self.tasks.append(task)\n\n    def remove_task(self, task_title):\n        \"\"\"Remove a task from the project by its title.\"\"\"\n        self.tasks = [task for task in self.tasks if task.title != task_title]\n\n    def list_tasks(self):\n        \"\"\"List all tasks in the project.\"\"\"\n        return [str(task) for task in self.tasks]\n\n    def __str__(self):\n        \"\"\"Return a string representation of the project.\"\"\"\n        return f\"Project: {self.name}, Total Tasks: {len(self.tasks)}\"\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a new project\n    project = Project(\"Home Renovation\")\n\n    # Add tasks to the project\n    project.add_task(Task(\"Paint the walls\", \"Paint all the walls in the living room\"))\n    project.add_task(Task(\"Install new lights\", \"Install ceiling lights in all rooms\"))\n    project.add_task(Task(\"Buy furniture\", \"Purchase furniture for the living room\"))\n\n    # Mark a task as completed\n    project.tasks[0].mark_completed()\n\n    # List all tasks\n    print(project.list_tasks())\n\n    ",
            "model_output": {
              "TestScenarios": [
                {
                  "ScenarioID": "performance_and_loading_test_Test_Scenario_1",
                  "Title": "Performance and Load Testing of Taskmaster Application",
                  "Description": "This test scenario evaluates the performance and load handling of the Taskmaster application under realistic conditions, simulating user behaviors to assess how the system operates under various levels of stress.",
                  "Objective": "To validate the performance and load handling capabilities of the Taskmaster application.",
                  "Category": "Performance and Load Testing",
                  "Comments": ""
                },
                {
                  "ScenarioID": "performance_and_loading_test_Test_Scenario_2",
                  "Title": "Load Testing of Project Management Features",
                  "Description": "This test scenario focuses on loading testing of project management features, such as adding, removing, and listing tasks. It simulates a high volume of concurrent users to assess the system's ability to handle multiple requests simultaneously.",
                  "Objective": "To evaluate the performance and scalability of the project management features under heavy loads.",
                  "Category": "Performance and Load Testing",
                  "Comments": ""
                }
              ],
            }
          }
          {
            "_id": {
              "$oid": "677e65e3b85151d48e577cd7"
            },
            "session_id": "20250108144746",
            "original_prompts": [
              {
                "_id": {
                  "$oid": "673d7efca741904082541a61"
                },
                "test_name": "Functional Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a detailed functional test scenario focused on evaluating the system’s core functionalities against specified requirements. This scenario should assess whether the system performs its intended operations correctly, meeting both functional specifications and expected user behaviors. Clearly define the purpose of the test, including necessary preconditions, setup configurations, and initial data required. Identify the primary functions to be tested, including input conditions, expected outputs, and any error-handling mechanisms. Outline specific test steps that simulate realistic user actions to validate each function. Describe the expected outcomes for each step, ensuring that both positive and negative cases are covered. Emphasize that the scenario aligns with ISTQB functional testing standards, aiming for accuracy, completeness, and robustness in functional validation.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as preconditions, test steps, expected outcomes, and error handling requirements.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for functional testing, including elements like input conditions, functional checks, and expected results.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, expected outcomes, and error-handling match the intended functional testing standards.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete examples for inputs, clear test step definitions, and detailed expected results. Specificity in test data and expected outcomes is essential for robust functional testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of functional testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of functional testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, input conditions, expected outputs, and error handling. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (functional) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant test steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as input conditions, expected results, and error handling. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the system's core functionalities against the specified requirements, focusing on whether it tests intended operations and expected user behaviors as outlined in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for functional testing, such as clear objectives, necessary preconditions, detailed test steps simulating realistic user actions, input conditions, expected outputs, and error handling mechanisms. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with functional testing standards and ISTQB guidelines. Ensure that the test steps are appropriate for the functionalities being tested and that expected outcomes are properly defined. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for functional testing. Consider if the scenario is well-organized, uses precise technical language, and provides clear instructions. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_create_prompts": {
                  "Core Functionalities Test Case": "Create a test case that verifies the core functionalities of the system as defined in the requirements. Include preconditions, detailed steps, input data, and expected results to ensure the system performs its intended operations correctly. Ensure the case aligns with ISTQB Functional Testing guidelines. Specify each step's expected output clearly and concisely.",
                  "Boundary Value Analysis Test Case": "Develop a test case focusing on Boundary Value Analysis (BVA) to evaluate system behavior at the edges of input ranges. Include preconditions, input data at lower and upper boundaries, and the expected system response for each scenario. Define clear test steps for validating boundary conditions, ensuring compliance with functional testing standards.",
                  "Error Recovery Functional Test Case": "Design a test case to assess the system's error recovery mechanisms during normal operations. Include scenarios with invalid inputs, interruptions, or fault conditions. Specify detailed steps to simulate these scenarios and describe the expected system behavior for each step, including recovery or error handling outputs.",
                  "User Input Validation Test Case": "Create a test case to validate the system's ability to handle various user inputs. Include valid, invalid, and edge-case inputs, with steps to simulate input actions and monitor system responses. Specify preconditions, detailed test steps, and expected outcomes to ensure the system handles inputs correctly and provides appropriate feedback for invalid inputs."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, generate a comprehensive set of functional test cases based on the provided test scenario. Ensure that the test cases collectively cover all actions, steps, and functional areas outlined in the scenario. Each test case should include detailed descriptions of preconditions, input conditions, step-by-step actions, and expected outcomes. Cover positive, negative, and boundary cases to ensure complete validation of the system’s functionalities. Include specific checks for error handling and edge cases where applicable. Organize the test cases to ensure clarity and traceability, aligning with ISTQB standards. The goal is to produce the maximum number of test cases necessary to comprehensively validate the scenario while avoiding redundancy.",
                "customised_prompt_status": false
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6551"
                },
                "test_name": "Edge Cases and Boundary Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design detailed test scenarios focused on evaluating the system's response to edge cases and boundary values. The scenario should explore how the system behaves at the extremes of input ranges, identifying potential vulnerabilities and ensuring reliability. Clearly define the purpose of the test, necessary preconditions, and environment configurations to create accurate testing conditions. Specify a range of test cases, including boundary values (just below, at, and just above limits) and edge cases that challenge system assumptions. Describe the expected outcomes for each case, detailing system responses, error handling, or corrective measures as needed. Outline the test steps, ensuring they capture all critical values for each parameter. Emphasize that the scenario aligns with ISTQB standards for boundary testing, aiming to identify weaknesses and ensure robustness against extreme inputs.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as boundary value examples, edge cases, expected outcomes, and specific error handling requirements.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for edge cases and boundary testing, including elements like precise boundary values, error handling expectations, and critical input limits.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, boundary values, and expected outcomes match the intended category and testing standards for boundary and edge case evaluation.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that provide concrete boundary values, clear definitions of acceptable limits, and detailed error handling for edge cases. Specificity in test data and expected results is essential for robust testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of boundary and edge case testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of boundary and edge case testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, boundary values, edge cases, and expected outcomes. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (boundary and edge cases) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as boundary values, edge case examples, and error expectations. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively targets edge cases and boundary values, exploring system behavior at the extremes of input ranges as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for edge case and boundary testing, such as specific boundary values (just below, at, and just above limits), detailed preconditions, test steps, and expected outcomes for each case. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with boundary testing standards and ISTQB guidelines. Check if the test steps correspond correctly to the specified boundary values. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for boundary testing. Consider if the scenario clearly communicates the test conditions and expectations. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, create a detailed set of test cases to evaluate system behavior under edge case scenarios and at boundary input values. Include preconditions, specific test data for both lower and upper limits, and steps to assess how the system handles inputs outside the expected range. Ensure scenarios include both positive and negative test cases to validate robustness and reliability. Align test cases with ISTQB boundary testing guidelines for comprehensive validation of system behavior.",
                "test_case_create_prompts": {
                  "Boundary Value Analysis Test Case": "Design a test case focusing on testing inputs at the boundaries of acceptable ranges. Specify lower and upper limits, just below and above these limits, and expected outcomes for each condition. Include clear steps for validating the system's response to each input.",
                  "Extreme Input Test Case": "Create a test case to evaluate system performance and stability when handling extreme or unrealistic input values. Define test data that pushes the system beyond its designed operational range, specifying the expected error messages or graceful degradation responses.",
                  "Null and Empty Input Test Case": "Develop a test case to assess system behavior when null or empty inputs are provided. Include detailed steps to simulate such inputs and expected outcomes, such as validation error messages or fallback defaults.",
                  "Invalid Format Test Case": "Create a test case to evaluate the system's validation mechanisms when inputs are provided in incorrect formats. Include examples such as malformed dates, special characters in numeric fields, or overly long strings, and specify expected error handling outcomes."
                },
                "customised_prompt_status": false
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6555"
                },
                "test_name": "Security Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive security test scenario aimed at evaluating the system’s resilience against various security threats. The scenario should focus on identifying vulnerabilities, ensuring data protection, and validating access controls within the specified system. Clearly define the purpose of the test, including the preconditions, environment configurations, and user roles required to simulate realistic security conditions. Identify key areas to be tested, such as authentication, authorization, data encryption, input validation, session management, and error handling. Outline test steps that cover common security tests such as SQL injection, XSS, CSRF, brute-force attacks, and privilege escalation attempts. Describe the expected outcomes for each test, including the system’s response to attempted breaches, error messages, and any logging requirements. Emphasize that the scenario aligns with ISTQB security testing standards, focusing on thorough vulnerability detection, data integrity, and system robustness against unauthorized access.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as preconditions, security vulnerabilities to test, expected outcomes, and error handling mechanisms.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for security testing, including elements like specific security threats, test steps, and expected defensive responses.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, security vulnerabilities, and expected outcomes match the intended security testing standards.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete security threat examples, detailed test steps, and clear expected outcomes for each threat. Specificity in vulnerability descriptions and system responses is essential for robust security testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of security testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of security testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, security vulnerabilities, expected outcomes, and error handling. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (security) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant test steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as vulnerability descriptions, expected results, and error handling requirements. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the system's resilience against security threats, focusing on identifying vulnerabilities and validating access controls as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for security testing, such as specific security vulnerabilities to test (e.g., SQL injection, XSS, CSRF), necessary preconditions, user roles, test steps, expected system responses, and error handling mechanisms. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with security testing standards and ISTQB guidelines. Ensure that the security threats and test steps correspond correctly to the objectives. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for security testing. Consider if the scenario is detailed, well-structured, and uses appropriate security terminology. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive set of security test cases to evaluate the system's resilience against threats. Include scenarios for testing authentication, input validation, data encryption, and unauthorized access prevention. Each test case should include detailed preconditions, steps to simulate security vulnerabilities, and expected outcomes. Align all test cases with ISTQB security testing guidelines to ensure thorough validation of system security measures.",
                "test_case_create_prompts": {
                  "Authentication Test Case": "Create a test case to verify the robustness of the system's authentication mechanism, including password strength requirements and protection against brute-force attacks.",
                  "Input Validation Security Test Case": "Develop a test case to test for vulnerabilities such as SQL injection, XSS, or CSRF attacks by providing malicious input. Specify expected outcomes like rejection of input and secure logging.",
                  "Data Encryption Test Case": "Design a test case to assess the effectiveness of encryption for sensitive data during transmission and storage. Specify tools and methods to verify encryption implementation.",
                  "Access Control Test Case": "Create a test case to ensure access control policies are enforced correctly, preventing unauthorized users from accessing restricted features or data."
                },
                "customised_prompt_status": false
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6556"
                },
                "test_name": "User Interface (GUI) Testing",
                "test_prompt": "As an ISTQB-certified test analyst, create a comprehensive GUI test scenario to evaluate the usability, responsiveness, and visual consistency of the user interface for the provided source code. Focus on ensuring that the interface meets design specifications and provides a seamless user experience for the TaskMaster project.\n\nPurpose: Evaluate the GUI of the TaskMaster project to ensure it meets design specifications and provides a seamless user experience.\n\nPreconditions:\n- Screen resolution: 1920x1080 pixels or higher\n- Device configuration: Windows 10 or macOS Catalina\n- Internet connection available\n\nTest Scope:\n- Test all UI elements, including buttons, menus, text fields, alignment, colors, font sizes, and responsiveness across different devices.\n- Check the interface's layout, navigation flow, interaction feedback, and responsiveness to user inputs.\n- Verify expected outcomes for each interaction, including alignment, visual feedback, error messages, and responsiveness criteria.\n\nTest Steps:\n1. Open the TaskMaster project and navigate to the task list view.\n2. Verify that all tasks are displayed correctly, including title, description, and status.\n3. Test button interactions, ensuring correct functionality and response time.\n4. Validate menu options, confirming correct navigation and display.\n5. Confirm text field input accuracy, including completion of form fields.\n6. Assess alignment, colors, font sizes, and responsiveness across different devices.\n7. Monitor interface layout and adjust as necessary to ensure optimal user experience.\n8. Test interaction feedback, verifying accurate error messages and visual cues.\n9. Evaluate responsiveness to user inputs, ensuring fast and accurate response times.\n10. Verify compliance with design specifications and ISTQB GUI testing standards.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as preconditions, UI elements, expected outcomes, and visual consistency checks.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for GUI testing, including elements like specific UI components, visual feedback expectations, and responsiveness requirements.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, UI elements, and expected outcomes match the intended GUI testing standards.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete examples for UI elements, detailed test steps, and clear expected results. Specificity in layout checks, responsiveness, and user feedback is essential for comprehensive GUI testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of GUI testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of GUI testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, UI components, expected outcomes, and visual consistency checks. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (GUI) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant UI elements, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as UI elements, layout expectations, and visual feedback. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the usability, responsiveness, and visual consistency of the user interface, focusing on UI elements and user experience as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for GUI testing, such as specific UI elements to be tested (buttons, menus, text fields), necessary preconditions (screen resolutions, device configurations), test steps, expected outcomes (alignment, visual feedback, responsiveness), and visual consistency checks. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with GUI testing standards and ISTQB guidelines. Check if the test steps accurately reflect the UI components and expected user interactions. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for GUI testing. Consider if the scenario effectively communicates the testing of UI elements and user experience aspects. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "As a senior ISTQB-certified test analyst, create a comprehensive set of GUI test cases to validate usability, visual consistency, and responsiveness. Include preconditions, test steps, and expected outcomes for all UI elements. Ensure test cases cover navigation, layout alignment, input handling, and feedback mechanisms. Align with ISTQB GUI testing standards for a seamless and user-friendly experience.",
                "test_case_create_prompts": {
                  "Visual Consistency Test Case": "Create a test case to ensure UI elements are visually consistent with design specifications across resolutions. Specify alignment, color schemes, and font size checks.",
                  "Navigation Flow Test Case": "Develop a test case to verify the intuitiveness of the navigation flow, ensuring users can seamlessly move between pages and access features.",
                  "Input Field Validation Test Case": "Design a test case to test input field behavior, including placeholder text, error messages, and constraints on input formats.",
                  "Responsiveness Test Case": "Create a test case to evaluate how the UI adapts to different screen sizes and orientations. Specify steps to verify element repositioning and integration."
                },
                "customised_prompt_status": true,
                "combined_prompt": "As a senior ISTQB-certified test analyst, create a comprehensive set of GUI test cases to validate usability, visual consistency, and responsiveness. Include preconditions, test steps, and expected outcomes for all UI elements. Ensure test cases cover navigation, layout alignment, input handling, and feedback mechanisms. Align with ISTQB GUI testing standards for a seamless and user-friendly experience.\n\nScenario Details:\nScenarioID: User Interface (GUI) Testing_TaskMaster_Test_Scenario_6\nTitle: Responsiveness to User Inputs\nDescription: Test the responsiveness of the GUI to user inputs, such as button clicks and text field entries. Ensure that all interactions are fast and accurate, with appropriate visual feedback provided.\nObjective: Validate the responsiveness and accuracy of user interactions within the GUI.\nCategory: User Interface (GUI) Testing\nComments: \n\nCombined Test Case Prompts:\nTest Case Type: Visual Consistency Test Case\nCreate a test case to ensure UI elements are visually consistent with design specifications across resolutions. Specify alignment, color schemes, and font size checks.\n\nTest Case Type: Responsiveness Test Case\nCreate a test case to evaluate how the UI adapts to different screen sizes and orientations. Specify steps to verify element repositioning and integration.\n\n\n    JSON Output Structure:\n\n    {\n        \"TestScenarios\": [\n            {\n                \"ScenarioID\": \"<Dynamic Scenario ID>\",\n                \"TestCaseID\": \"<Dynamic Test Case ID>\",\n                \"Title\": \"<Scenario Title>\",\n                \"Description\": \"<Detailed scenario description at least 3 sentences. This is the most important part of the test scenario!>\",\n                \"Objective\": \"<Objective or goal of the scenario>\",\n                \"Category\": \"<Category of the test case>\",\n                \"Comments\": \"<Any inconsistency or additional notes>\"\n            }\n        ]\n    }\n\n    Guidelines for JSON Structure:\n    - Ensure the ScenarioID is dynamic and matches the required format.\n    - TestCaseID must be unique within the scope of its ScenarioID.\n\n    Example JSON output:\n    {\n        \"ScenarioID\": \"Scenario_1\",\n        \"TestCaseID\": \"TestCase_1\",\n        \"Title\": \"Verify Login Functionality\",\n        \"Description\": \"Test the login functionality to ensure users can log in with valid credentials and receive appropriate error messages for invalid inputs. Additionally, ensure session management operates correctly post-login.\",\n        \"Objective\": \"Validate user authentication mechanism.\",\n        \"Category\": \"Functional Tests\",\n        \"Comments\": \"Ensure edge cases for invalid inputs are covered.\"\n    }\n    \n\n",
                "scenario_id": "User Interface (GUI) Testing_TaskMaster_Test_Scenario_6",
                "test_case": {
                  "TestScenarios": [
                    {
                      "ScenarioID": "User Interface (GUI) Testing_TaskMaster_Test_Scenario_6",
                      "TestCaseID": "User Interface (GUI) Testing_TaskMaster_Test_Case_6.1",
                      "Title": "Visual Consistency Test Case",
                      "Description": "Test the visual consistency of UI elements across resolutions. Verify alignment, color schemes, and font size consistency.",
                      "Objective": "Validate visual consistency of UI elements.",
                      "Category": "User Interface (GUI) Testing",
                      "Comments": "N/A"
                    },
                    {
                      "ScenarioID": "User Interface (GUI) Testing_TaskMaster_Test_Scenario_6",
                      "TestCaseID": "User Interface (GUI) Testing_TaskMaster_Test_Case_6.2",
                      "Title": "Responsiveness Test Case",
                      "Description": "Test the responsiveness of UI elements to user inputs. Verify element repositioning and integration across different screen sizes and orientations.",
                      "Objective": "Validate responsiveness of UI elements.",
                      "Category": "User Interface (GUI) Testing",
                      "Comments": "N/A"
                    },
                    {
                      "ScenarioID": "User Interface (GUI) Testing_TaskMaster_Test_Scenario_6",
                      "TestCaseID": "User Interface (GUI) Testing_TaskMaster_Test_Case_6.3",
                      "Title": "Navigation Test Case",
                      "Description": "Test the navigation functionality of the GUI. Verify smooth transitions between different pages and sections.",
                      "Objective": "Validate navigation functionality.",
                      "Category": "User Interface (GUI) Testing",
                      "Comments": "N/A"
                    },
                    {
                      "ScenarioID": "User Interface (GUI) Testing_TaskMaster_Test_Scenario_6",
                      "TestCaseID": "User Interface (GUI) Testing_TaskMaster_Test_Case_6.4",
                      "Title": "Layout Alignment Test Case",
                      "Description": "Test the layout alignment of UI elements. Verify elements are positioned correctly within their designated areas.",
                      "Objective": "Validate layout alignment.",
                      "Category": "User Interface (GUI) Testing",
                      "Comments": "N/A"
                    },
                    {
                      "ScenarioID": "User Interface (GUI) Testing_TaskMaster_Test_Scenario_6",
                      "TestCaseID": "User Interface (GUI) Testing_TaskMaster_Test_Case_6.5",
                      "Title": "Input Handling Test Case",
                      "Description": "Test the handling of user inputs. Verify proper validation of input formats and appropriate responses.",
                      "Objective": "Validate input handling.",
                      "Category": "User Interface (GUI) Testing",
                      "Comments": "N/A"
                    },
                    {
                      "ScenarioID": "User Interface (GUI) Testing_TaskMaster_Test_Scenario_6",
                      "TestCaseID": "User Interface (GUI) Testing_TaskMaster_Test_Case_6.6",
                      "Title": "Feedback Mechanism Test Case",
                      "Description": "Test the feedback mechanism provided to users. Verify clear and informative messages are displayed based on user actions.",
                      "Objective": "Validate feedback mechanism.",
                      "Category": "User Interface (GUI) Testing",
                      "Comments": "N/A"
                    }
                  ]
                }
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6550"
                },
                "test_name": "Compatibility Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design comprehensive test scenarios to evaluate the compatibility of a specified system across different environments. This scenario should focus on assessing the system’s ability to function as expected on various platforms, devices, operating systems, browsers, and configurations. Clearly outline the purpose of the test, including the preconditions and environmental setups required for accurate testing across multiple platforms. Specify a range of environments, such as specific operating system versions, browser types, device specifications, screen resolutions, and hardware configurations, ensuring broad coverage. Describe the expected outcomes for each environment, identifying potential compatibility issues, or visual rendering differences. Detail test steps that simulate user behavior in each environment to validate consistent performance. Emphasize that the scenario aligns with ISTQB compatibility testing standards, prioritizing cross-environment consistency, reliability, and user experience.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective, structured approach, and including essential elements such as environment configurations, expected outcomes, and compatibility criteria.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for compatibility testing, including specific platform details, and cross-environment checks.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, environment configurations, and expected outcomes match the intended category and testing standards for compatibility evaluation.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete configurations (such as operating system versions, browser types), detailed expected results, and identified compatibility metrics.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of compatibility testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of compatibility testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, environment configurations, expected outcomes, and compatibility checks. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (compatibility) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant configurations, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as platform versions, environment configurations, and expected responses. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively addresses compatibility testing across different environments as specified in the prompt. Consider if it includes testing on various platforms, devices, operating systems, browsers, and configurations. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for compatibility testing, such as detailed environment configurations, preconditions, test steps simulating user behavior on different platforms, and expected outcomes for each environment. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with compatibility testing standards and ISTQB guidelines. Check if the test steps and expected outcomes match the specified environments and configurations. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for compatibility testing. Consider if the scenario is clearly organized, uses technical language appropriately, and provides precise details. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "As a senior ISTQB-certified test analyst, design a comprehensive set of compatibility test cases to evaluate the across different platforms, devices, operating systems, and browsers. Specify preconditions, test steps, and expected outcomes for each scenario. Ensure the cases assess cross-platform consistency, and visual rendering. Align with ISTQB compatibility testing standards to ensure the system provides a seamless user experience in diverse environments.",
                "test_case_create_prompts": {
                  "Cross-Browser Test Case": "Create a test case to ensure consistency across multiple web browsers and their versions. Specify preconditions, browser configurations, detailed steps for validation, and expected outputs for compatibility verification.",
                  "Device Compatibility Test Case": "Develop a test case to evaluate system behavior on a variety of devices, such as smartphones, tablets, and desktops. Include steps to verify responsive design and integrition across device specifications.",
                  "Operating System Compatibility Test Case": "Design a test case to assess system performance on different operating systems, including specific versions. Include steps to test installation, execution, and interoperability between OS-specific features.",
                  "Screen Resolution Test Case": "Create a test case to validate the system's responsiveness and layout consistency across various screen resolutions. Include steps for testing visual elements and alignment across resolutions."
                },
                "customised_prompt_status": false
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6552"
                },
                "test_name": "Input Data Variety Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive test scenario focused on assessing the system’s ability to handle a diverse range of input data. The scenario should evaluate how the system processes various data types, formats, edge cases, and outliers to ensure robust input handling. Clearly define the purpose of the test, the preconditions required, and the environment configurations necessary for accurate testing. Specify a range of input data examples, including valid, invalid, boundary, and null values. Describe the expected system responses, detailing any validation errors, warnings, or specific handling methods required for each type of input. Outline test steps that simulate user behavior with different input scenarios to validate the system’s resilience. Emphasize that the scenario aligns with ISTQB testing standards, ensuring a thorough assessment of data handling robustness and consistency.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective, structured approach, and including all necessary elements such as input data examples, expected outcomes, and error handling.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for input variety testing, including elements like specific data types, validation requirements, and error handling processes.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, data requirements, and expected outcomes match the intended category and testing standards for input variety evaluation.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that include concrete examples of various input data types, specific configurations for the test environment, and clear definitions of expected outcomes for each input category. Specificity in data validation requirements and error expectations is essential.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of input data variety testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of input data variety testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, data examples (valid, invalid, boundary, null), and expected outcomes. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (input data variety) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as input data examples, error expectations, and response definitions. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the system's ability to handle a diverse range of input data, including valid, invalid, boundary, and null values, as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for input data variety testing, such as a range of input data examples, necessary preconditions, detailed test steps, expected system responses, validation errors, and error handling for each type of input. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with input data variety testing standards and ISTQB guidelines. Check if the test steps accurately reflect the diverse input data and expected outcomes. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for input data variety testing. Consider if the scenario effectively communicates the testing of different data types and their expected handling. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, generate a comprehensive set of test cases to validate the system's ability to handle diverse input data types. Include scenarios for valid, invalid, null, and edge-case inputs. Specify detailed steps, expected outcomes, and validation criteria for each input type. Align all test cases with ISTQB testing guidelines to ensure the system’s robustness in input handling.",
                "test_case_create_prompts": {
                  "Valid Input Test Case": "Create a test case to validate the system’s ability to process valid input data accurately. Include examples for various data types such as text, numbers, and dates.",
                  "Invalid Input Test Case": "Develop a test case to test system behavior with invalid inputs, ensuring appropriate error messages or rejections are triggered.",
                  "Boundary Input Test Case": "Design a test case to assess the system’s handling of input values at the boundaries of acceptable ranges, including just below and above these limits.",
                  "Special Character Input Test Case": "Create a test case to evaluate how the system handles inputs with special characters, emojis, or unexpected symbols. Specify expected sanitization or rejection behaviors."
                },
                "customised_prompt_status": false
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6554"
                },
                "test_name": "Performance and Load Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive performance and load testing scenario for a specified system. The scenario should aim to assess the system’s performance and load handling under realistic conditions, simulating user behaviors to evaluate how the system operates under various levels of stress. Clearly establish the purpose of the test, the preconditions required, and the environment configurations necessary to create accurate, real-world conditions. Outline the user load, including distribution and behavior patterns that reflect peak periods, random delays, and sudden surges. Identify key performance metrics such as response time, throughput, resource utilization, and error rates, detailing acceptable thresholds based on industry standards. Specify the data requirements to represent real-world usage, including edge cases, and present a clear, sequential process for test execution from setup to conclusion. Emphasize expected outcomes, define conditions for success and failure, and include guidelines for comprehensive logging and analysis. Ensure that the scenario aligns with ISTQB performance testing standards, incorporates best practices, and prioritizes precision, consistency, and robustness.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, depth, and clarity. A high-quality scenario should align well with the specified test purpose, providing a clear objective and structured approach. It should include all necessary elements such as preconditions, environment configuration, user load details, performance metrics, and expected outcomes.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for performance and load testing, including specified elements like response time, error rates, data requirements, and system behavior under stress.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described testing steps, data requirements, and performance metrics match the intended category and testing standards for performance and load evaluation.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete numbers for user loads, specific configurations for the test environment, and clear definitions of success and failure thresholds. Specificity in metrics and detailed steps for test execution are essential.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of performance and load testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of performance and load testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, user load distribution, key performance metrics, and expected outcomes. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (performance and load) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched metrics or off-target objectives, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as user load numbers, test environment configurations, and threshold definitions. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the system's performance and load handling under various levels of stress, simulating realistic user behaviors and conditions as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for performance and load testing, such as user load details (distribution, behavior patterns), key performance metrics (response time, throughput, resource utilization, error rates), necessary preconditions, environment configurations, and expected outcomes. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with performance and load testing standards and ISTQB guidelines. Check if the test steps accurately reflect the intended load conditions and performance metrics. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for performance and load testing. Consider if the scenario effectively communicates the testing process and performance expectations. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive set of performance and load test cases to evaluate the system's ability to handle various levels of stress. Ensure that the test cases cover response time, throughput, scalability, and stability under different load conditions. Include preconditions, detailed execution steps, specific input data, and measurable expected outcomes for each scenario. Align all test cases with ISTQB performance testing guidelines to validate both typical and edge performance conditions. Emphasize precision, accuracy, and reproducibility in test documentation.",
                "test_case_create_prompts": {
                  "Core Performance Test Case": "Develop a test case to verify the system's ability to handle normal operational loads. Include typical user behavior as input, preconditions for environment setup, and measurable metrics such as response time and system throughput. Ensure test steps simulate realistic user actions and specify precise success criteria.",
                  "Stress Test Case": "Create a test case to evaluate how the system behaves under extreme stress, such as peak user loads or resource exhaustion. Include detailed steps for incrementally increasing load, monitoring system behavior, and identifying failure points. Specify thresholds for acceptable and critical performance levels.",
                  "Scalability Test Case": "Design a test case to assess the system's ability to scale with increasing workload. Include steps to simulate user growth, resource allocation adjustments, and infrastructure elasticity. Define metrics like maximum concurrent users or transactions per second.",
                  "Resource Utilization Test Case": "Create a test case focusing on system resource usage under varying load conditions. Specify steps to measure CPU, memory, disk I/O, and network bandwidth. Include scenarios for detecting bottlenecks and provide acceptable utilization thresholds."
                },
                "customised_prompt_status": false
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6553"
                },
                "test_name": "Integration Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive integration test scenario aimed at evaluating how different modules or components of a specified system interact with each other. This scenario should focus on ensuring that integrated components communicate correctly and work together. Clearly outline the purpose of the test, specifying necessary preconditions, setup configurations, and initial data required. Identify key interactions between components, including input and output data flow, dependencies, and any shared resources. Outline test steps that simulate realistic data exchange and process flows between components, capturing both normal and abnormal interaction patterns. Describe expected outcomes for each interaction, including response times, data integrity, and error handling requirements. Emphasize that the scenario aligns with ISTQB integration testing standards, focusing on accuracy, consistency, and robustness in component interaction validation.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as preconditions, data flow between components, expected outcomes, and error handling mechanisms.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for integration testing, including specific component interactions, data exchange, and expected results.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, component interactions, and expected outcomes match the intended integration testing standards.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that include concrete component interactions, detailed data flows, and clear expected outcomes for each integration point. Specificity in data handling and error scenarios is essential for thorough integration testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of integration testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of integration testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, data flow between components, expected outcomes, and error handling. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (integration) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant test steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as component interactions, expected results, and error handling mechanisms. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the interactions between different modules or components of the system, focusing on data flow and communication between components as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for integration testing, such as detailed component interactions, necessary preconditions, data exchange processes, dependencies, shared resources, and expected outcomes for each interaction. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with integration testing standards and ISTQB guidelines. Ensure that the interactions between components are correctly represented and that expected outcomes are properly defined. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for integration testing. Consider if the scenario clearly outlines component interactions and is structured logically. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "As a senior ISTQB-certified test analyst, generate a set of integration test cases to evaluate interactions between system components. Include detailed preconditions, interaction sequences, and data flow verification steps. Focus on validating module communication, data consistency, and error handling. Ensure the test cases align with ISTQB integration testing standards.",
                "test_case_create_prompts": {
                  "Interface Communication Test Case": "Develop a test case to validate communication between integrated components, ensuring proper data exchange and synchronization. Include scenarios for both normal and interrupted communication.",
                  "Data Consistency Test Case": "Create a test case to verify that data remains consistent when shared or transferred between modules. Specify test steps for simulating updates, deletions, and retrievals.",
                  "Shared Resource Access Test Case": "Design a test case to assess how integrated components handle access to shared resources, avoiding conflicts or deadlocks. Include test steps for concurrent access scenarios.",
                  "Error Propagation Test Case": "Create a test case to evaluate the system’s ability to handle and isolate errors in one component without affecting others. Include scenarios to test error logging and recovery mechanisms."
                },
                "customised_prompt_status": false
              }
            ],
            "process_title": "User Interface (GUI) Testing",
            "document_type": "Source Code",
            "generated_prompt": "As an ISTQB-certified test analyst, create a comprehensive GUI test scenario to evaluate the usability, responsiveness, and visual consistency of the user interface for the provided source code. Focus on ensuring that the interface meets design specifications and provides a seamless user experience for the TaskMaster project.\n\nPurpose: Evaluate the GUI of the TaskMaster project to ensure it meets design specifications and provides a seamless user experience.\n\nPreconditions:\n- Screen resolution: 1920x1080 pixels or higher\n- Device configuration: Windows 10 or macOS Catalina\n- Internet connection available\n\nTest Scope:\n- Test all UI elements, including buttons, menus, text fields, alignment, colors, font sizes, and responsiveness across different devices.\n- Check the interface's layout, navigation flow, interaction feedback, and responsiveness to user inputs.\n- Verify expected outcomes for each interaction, including alignment, visual feedback, error messages, and responsiveness criteria.\n\nTest Steps:\n1. Open the TaskMaster project and navigate to the task list view.\n2. Verify that all tasks are displayed correctly, including title, description, and status.\n3. Test button interactions, ensuring correct functionality and response time.\n4. Validate menu options, confirming correct navigation and display.\n5. Confirm text field input accuracy, including completion of form fields.\n6. Assess alignment, colors, font sizes, and responsiveness across different devices.\n7. Monitor interface layout and adjust as necessary to ensure optimal user experience.\n8. Test interaction feedback, verifying accurate error messages and visual cues.\n9. Evaluate responsiveness to user inputs, ensuring fast and accurate response times.\n10. Verify compliance with design specifications and ISTQB GUI testing standards.\n\nInstruction and consistency situation: \nAssess the scenario's alignment with the intended test category (GUI) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant UI elements, should be highlighted as 'Inconsistent' in the output with an explanation for review.\n\nScoring situation: \n Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete examples for UI elements, detailed test steps, and clear expected results. Specificity in layout checks, responsiveness, and user feedback is essential for comprehensive GUI testing.\n\n\n    JSON Output Structure:\n\n    {\n        \"TestScenarios\": [\n            {\n                \"ScenarioID\": \"User Interface (GUI) Testing_Test_Scenario_1\",\n                \"Title\": \"<Scenario Title>\",\n                \"Description\": \"<Detailed scenario description at least 3 sentences. This is the most important part of the generate test scenario!>\",\n                \"Objective\": \"<Objective or goal of the scenario>\",\n                \"Category\": \"User Interface (GUI) Testing\",\n                \"Comments\": \"<Any inconsistency or additional notes>\"\n            }\n        ]\n    }\n\n    Guidelines for JSON Structure:\n    - Ensure the ScenarioID is dynamic and matches the required format.\n\n    Example JSON output:\n    {\n        \"ScenarioID\": \"Process_A_ModelX_Test_Scenario_1\",\n        \"Title\": \"Verify Login Functionality\",\n        \"Description\": \"Test the login functionality to ensure that users can successfully log in with valid credentials and are rejected with incorrect credentials. Additionally, verify that the system displays appropriate error messages for failed login attempts to guide users in correcting their input. Furthermore, ensure that the login session is maintained correctly, allowing users to access their accounts seamlessly after a successful login.\",\n        \"Objective\": \"Validate user authentication mechanism.\",\n        \"Category\": \"Functional Tests\",\n        \"Comments\": \"\"\n    }\n\n    This document is classified as a Source Code. When generating test scenarios, ensure that the structure, format, and content align with the nature of this document type.\n\n    Document Content: \n    # taskmaster.py\n\nclass Task:\n    def __init__(self, title, description, completed=False):\n        self.title = title\n        self.description = description\n        self.completed = completed\n\n    def mark_completed(self):\n        \"\"\"Mark the task as completed.\"\"\"\n        self.completed = True\n\n    def __str__(self):\n        \"\"\"Return a string representation of the task.\"\"\"\n        status = \"Completed\" if self.completed else \"Pending\"\n        return f\"Task: {self.title}, Status: {status}\"\n\n\nclass Project:\n    def __init__(self, name):\n        self.name = name\n        self.tasks = []\n\n    def add_task(self, task):\n        \"\"\"Add a task to the project.\"\"\"\n        if isinstance(task, Task):\n            self.tasks.append(task)\n\n    def remove_task(self, task_title):\n        \"\"\"Remove a task from the project by its title.\"\"\"\n        self.tasks = [task for task in self.tasks if task.title != task_title]\n\n    def list_tasks(self):\n        \"\"\"List all tasks in the project.\"\"\"\n        return [str(task) for task in self.tasks]\n\n    def __str__(self):\n        \"\"\"Return a string representation of the project.\"\"\"\n        return f\"Project: {self.name}, Total Tasks: {len(self.tasks)}\"\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a new project\n    project = Project(\"Home Renovation\")\n\n    # Add tasks to the project\n    project.add_task(Task(\"Paint the walls\", \"Paint all the walls in the living room\"))\n    project.add_task(Task(\"Install new lights\", \"Install ceiling lights in all rooms\"))\n    project.add_task(Task(\"Buy furniture\", \"Purchase furniture for the living room\"))\n\n    # Mark a task as completed\n    project.tasks[0].mark_completed()\n\n    # List all tasks\n    print(project.list_tasks())\n\n    ",
            "model_output": {
              "TestScenarios": [
                {
                  "ScenarioID": "User Interface (GUI) Testing_TaskMaster_Test_Scenario_1",
                  "Title": "Task List Display Verification",
                  "Description": "Ensure that the task list view displays all tasks correctly, including their titles, descriptions, and completion statuses. Verify that each task is displayed in a consistent format.",
                  "Objective": "Validate the accuracy and consistency of the task display in the GUI.",
                  "Category": "User Interface (GUI) Testing",
                  "Comments": ""
                },
                {
                  "ScenarioID": "User Interface (GUI) Testing_TaskMaster_Test_Scenario_2",
                  "Title": "Task Completion Button Functionality",
                  "Description": "Test the functionality of the task completion button. Ensure that when a task is marked as completed, its status updates correctly in both the GUI and internal data structure.",
                  "Objective": "Validate the functionality and accuracy of task completion interactions.",
                  "Category": "User Interface (GUI) Testing",
                  "Comments": ""
                },
                {
                  "ScenarioID": "User Interface (GUI) Testing_TaskMaster_Test_Scenario_3",
                  "Title": "Task Removal Button Functionality",
                  "Description": "Test the functionality of the task removal button. Ensure that when a task is removed, it is no longer displayed in the GUI and is also removed from the internal data structure.",
                  "Objective": "Validate the functionality and accuracy of task removal interactions.",
                  "Category": "User Interface (GUI) Testing",
                  "Comments": ""
                },
                {
                  "ScenarioID": "User Interface (GUI) Testing_TaskMaster_Test_Scenario_4",
                  "Title": "Navigation Between Views",
                  "Description": "Test the navigation between different views in the GUI, such as from the task list view to a task detail view. Ensure that navigation is smooth and that all necessary information is displayed correctly.",
                  "Objective": "Validate the functionality and accuracy of navigation within the GUI.",
                  "Category": "User Interface (GUI) Testing",
                  "Comments": ""
                },
                {
                  "ScenarioID": "User Interface (GUI) Testing_TaskMaster_Test_Scenario_5",
                  "Title": "Visual Consistency Across Devices",
                  "Description": "Test the visual consistency of the GUI across different devices with varying screen resolutions and configurations. Ensure that all UI elements are displayed correctly and consistently.",
                  "Objective": "Validate the visual consistency of the GUI for optimal user experience.",
                  "Category": "User Interface (GUI) Testing",
                  "Comments": ""
                },
                {
                  "ScenarioID": "User Interface (GUI) Testing_TaskMaster_Test_Scenario_6",
                  "Title": "Responsiveness to User Inputs",
                  "Description": "Test the responsiveness of the GUI to user inputs, such as button clicks and text field entries. Ensure that all interactions are fast and accurate, with appropriate visual feedback provided.",
                  "Objective": "Validate the responsiveness and accuracy of user interactions within the GUI.",
                  "Category": "User Interface (GUI) Testing",
                  "Comments": ""
                }
              ]
            }
          }
          {
            "_id": {
              "$oid": "677e625bc18bc065c130c7d5"
            },
            "session_id": "20250108143241",
            "original_prompts": [
              {
                "_id": {
                  "$oid": "673d7efca741904082541a61"
                },
                "test_name": "Functional Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a detailed functional test scenario focused on evaluating the system’s core functionalities against specified requirements. This scenario should assess whether the system performs its intended operations correctly, meeting both functional specifications and expected user behaviors. Clearly define the purpose of the test, including necessary preconditions, setup configurations, and initial data required. Identify the primary functions to be tested, including input conditions, expected outputs, and any error-handling mechanisms. Outline specific test steps that simulate realistic user actions to validate each function. Describe the expected outcomes for each step, ensuring that both positive and negative cases are covered. Emphasize that the scenario aligns with ISTQB functional testing standards, aiming for accuracy, completeness, and robustness in functional validation.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as preconditions, test steps, expected outcomes, and error handling requirements.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for functional testing, including elements like input conditions, functional checks, and expected results.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, expected outcomes, and error-handling match the intended functional testing standards.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete examples for inputs, clear test step definitions, and detailed expected results. Specificity in test data and expected outcomes is essential for robust functional testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of functional testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of functional testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, input conditions, expected outputs, and error handling. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (functional) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant test steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as input conditions, expected results, and error handling. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the system's core functionalities against the specified requirements, focusing on whether it tests intended operations and expected user behaviors as outlined in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for functional testing, such as clear objectives, necessary preconditions, detailed test steps simulating realistic user actions, input conditions, expected outputs, and error handling mechanisms. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with functional testing standards and ISTQB guidelines. Ensure that the test steps are appropriate for the functionalities being tested and that expected outcomes are properly defined. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for functional testing. Consider if the scenario is well-organized, uses precise technical language, and provides clear instructions. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_create_prompts": {
                  "Core Functionalities Test Case": "Create a test case that verifies the core functionalities of the system as defined in the requirements. Include preconditions, detailed steps, input data, and expected results to ensure the system performs its intended operations correctly. Ensure the case aligns with ISTQB Functional Testing guidelines. Specify each step's expected output clearly and concisely.",
                  "Boundary Value Analysis Test Case": "Develop a test case focusing on Boundary Value Analysis (BVA) to evaluate system behavior at the edges of input ranges. Include preconditions, input data at lower and upper boundaries, and the expected system response for each scenario. Define clear test steps for validating boundary conditions, ensuring compliance with functional testing standards.",
                  "Error Recovery Functional Test Case": "Design a test case to assess the system's error recovery mechanisms during normal operations. Include scenarios with invalid inputs, interruptions, or fault conditions. Specify detailed steps to simulate these scenarios and describe the expected system behavior for each step, including recovery or error handling outputs.",
                  "User Input Validation Test Case": "Create a test case to validate the system's ability to handle various user inputs. Include valid, invalid, and edge-case inputs, with steps to simulate input actions and monitor system responses. Specify preconditions, detailed test steps, and expected outcomes to ensure the system handles inputs correctly and provides appropriate feedback for invalid inputs."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, generate a comprehensive set of functional test cases based on the provided test scenario. Ensure that the test cases collectively cover all actions, steps, and functional areas outlined in the scenario. Each test case should include detailed descriptions of preconditions, input conditions, step-by-step actions, and expected outcomes. Cover positive, negative, and boundary cases to ensure complete validation of the system’s functionalities. Include specific checks for error handling and edge cases where applicable. Organize the test cases to ensure clarity and traceability, aligning with ISTQB standards. The goal is to produce the maximum number of test cases necessary to comprehensively validate the scenario while avoiding redundancy.",
                "customised_prompt_status": false
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6551"
                },
                "test_name": "Edge Cases and Boundary Testing",
                "test_prompt": {
                  "test_name": "Edge Cases and Boundary Testing for TaskMaster Requirements Document",
                  "document_type": "Requirements Document",
                  "document_content": "# Requirements Specification Document: TaskMaster\n\n## 1. Functional Requirements\n1.1 Users should be able to create new tasks.\n1.2 Users should be able to organize tasks under projects.\n1.3 Users should be able to mark tasks as completed or pending.\n1.4 Users should be able to remove specific tasks from a project.\n1.5 Users should be able to view lists of completed and pending tasks.\n\n## 2. Performance Requirements\n2.1 The application should support projects with up to 100 tasks.\n2.2 The application should respond to user interactions within 1 second.\n\n## 3. Usability Requirements\n3.1 The user interface should be simple and user-friendly.\n3.2 Users should be able to add a new task in no more than three steps.\n\n## 4. Security Requirements\n4.1 The application should securely store user data.\n4.2 The application should protect user data from unauthorized access.\n\n## 5. Extensibility Requirements\n5.1 The application should be designed to allow for future API integrations.\n5.2 The application should support the addition of multi-language support.",
                  "test objectives": [
                    "Evaluate the system's response to edge cases and boundary values."
                  ],
                  "preconditions": [
                    "System is installed and configured correctly.",
                    "User has access to necessary permissions and credentials."
                  ],
                  "environment_configurations": [
                    "Operating System: [list operating systems].",
                    "Browser: [list supported browsers]."
                  ],
                  "test_steps": [
                    {
                      "step_id": 1,
                      "description": "Create a new task with valid input data.",
                      "boundary_values": [
                        "Just below limits",
                        "At limits",
                        "Just above limits"
                      ],
                      "edge_cases": [
                        "Invalid user data",
                        "Empty field values"
                      ]
                    },
                    {
                      "step_id": 2,
                      "description": "Organize tasks under projects with valid input data.",
                      "boundary_values": [
                        "Just below limits",
                        "At limits",
                        "Just above limits"
                      ],
                      "edge_cases": [
                        "Invalid project name",
                        "Empty field values"
                      ]
                    },
                    {
                      "step_id": 3,
                      "description": "Mark tasks as completed or pending with valid input data.",
                      "boundary_values": [
                        "Just below limits",
                        "At limits",
                        "Just above limits"
                      ],
                      "edge_cases": [
                        "Invalid task status",
                        "Empty field values"
                      ]
                    },
                    {
                      "step_id": 4,
                      "description": "Remove specific tasks from a project with valid input data.",
                      "boundary_values": [
                        "Just below limits",
                        "At limits",
                        "Just above limits"
                      ],
                      "edge_cases": [
                        "Invalid task ID",
                        "Empty field values"
                      ]
                    },
                    {
                      "step_id": 5,
                      "description": "View lists of completed and pending tasks with valid input data.",
                      "boundary_values": [
                        "Just below limits",
                        "At limits",
                        "Just above limits"
                      ],
                      "edge_cases": [
                        "Invalid task status",
                        "Empty field values"
                      ]
                    }
                  ],
                  "expected_outcomes": [
                    {
                      "step_id": 1,
                      "description": "The system creates a new task with the provided input data."
                    },
                    {
                      "step_id": 2,
                      "description": "The system organizes tasks under projects with the provided input data."
                    },
                    {
                      "step_id": 3,
                      "description": "The system marks tasks as completed or pending with the provided input data."
                    },
                    {
                      "step_id": 4,
                      "description": "The system removes specific tasks from a project with the provided input data."
                    },
                    {
                      "step_id": 5,
                      "description": "The system views lists of completed and pending tasks with the provided input data."
                    }
                  ],
                  "error_handling": [
                    {
                      "step_id": 1,
                      "description": "Handle invalid user data by displaying an error message."
                    },
                    {
                      "step_id": 2,
                      "description": "Handle empty field values by skipping the operation and indicating it in the log."
                    }
                  ]
                },
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as boundary value examples, edge cases, expected outcomes, and specific error handling requirements.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for edge cases and boundary testing, including elements like precise boundary values, error handling expectations, and critical input limits.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, boundary values, and expected outcomes match the intended category and testing standards for boundary and edge case evaluation.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that provide concrete boundary values, clear definitions of acceptable limits, and detailed error handling for edge cases. Specificity in test data and expected results is essential for robust testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of boundary and edge case testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of boundary and edge case testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, boundary values, edge cases, and expected outcomes. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (boundary and edge cases) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as boundary values, edge case examples, and error expectations. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively targets edge cases and boundary values, exploring system behavior at the extremes of input ranges as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for edge case and boundary testing, such as specific boundary values (just below, at, and just above limits), detailed preconditions, test steps, and expected outcomes for each case. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with boundary testing standards and ISTQB guidelines. Check if the test steps correspond correctly to the specified boundary values. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for boundary testing. Consider if the scenario clearly communicates the test conditions and expectations. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, create a detailed set of test cases to evaluate system behavior under edge case scenarios and at boundary input values. Include preconditions, specific test data for both lower and upper limits, and steps to assess how the system handles inputs outside the expected range. Ensure scenarios include both positive and negative test cases to validate robustness and reliability. Align test cases with ISTQB boundary testing guidelines for comprehensive validation of system behavior.",
                "test_case_create_prompts": {
                  "Boundary Value Analysis Test Case": "Design a test case focusing on testing inputs at the boundaries of acceptable ranges. Specify lower and upper limits, just below and above these limits, and expected outcomes for each condition. Include clear steps for validating the system's response to each input.",
                  "Extreme Input Test Case": "Create a test case to evaluate system performance and stability when handling extreme or unrealistic input values. Define test data that pushes the system beyond its designed operational range, specifying the expected error messages or graceful degradation responses.",
                  "Null and Empty Input Test Case": "Develop a test case to assess system behavior when null or empty inputs are provided. Include detailed steps to simulate such inputs and expected outcomes, such as validation error messages or fallback defaults.",
                  "Invalid Format Test Case": "Create a test case to evaluate the system's validation mechanisms when inputs are provided in incorrect formats. Include examples such as malformed dates, special characters in numeric fields, or overly long strings, and specify expected error handling outcomes."
                },
                "customised_prompt_status": true,
                "combined_prompt": "Acting as a senior ISTQB-certified test analyst, create a detailed set of test cases to evaluate system behavior under edge case scenarios and at boundary input values. Include preconditions, specific test data for both lower and upper limits, and steps to assess how the system handles inputs outside the expected range. Ensure scenarios include both positive and negative test cases to validate robustness and reliability. Align test cases with ISTQB boundary testing guidelines for comprehensive validation of system behavior.\n\nScenario Details:\nScenarioID: edge_and_boundary_testing_Test_Scenario_5\nTitle: View Lists of Completed and Pending Tasks with Valid Input Data\nDescription: Test the viewing of lists of completed and pending tasks functionality to ensure that users can successfully view lists of completed and pending tasks with valid input data. Verify that the system views lists of completed and pending tasks with the provided input data.\nObjective: Validate task listing mechanism.\nCategory: Edge Cases and Boundary Testing\nComments: \n\nCombined Test Case Prompts:\nTest Case Type: Invalid Format Test Case\nCreate a test case to evaluate the system's validation mechanisms when inputs are provided in incorrect formats. Include examples such as malformed dates, special characters in numeric fields, or overly long strings, and specify expected error handling outcomes.\n\n\n    JSON Output Structure:\n\n    {\n        \"TestScenarios\": [\n            {\n                \"ScenarioID\": \"<Dynamic Scenario ID>\",\n                \"TestCaseID\": \"<Dynamic Test Case ID>\",\n                \"Title\": \"<Scenario Title>\",\n                \"Description\": \"<Detailed scenario description at least 3 sentences. This is the most important part of the test scenario!>\",\n                \"Objective\": \"<Objective or goal of the scenario>\",\n                \"Category\": \"<Category of the test case>\",\n                \"Comments\": \"<Any inconsistency or additional notes>\"\n            }\n        ]\n    }\n\n    Guidelines for JSON Structure:\n    - Ensure the ScenarioID is dynamic and matches the required format.\n    - TestCaseID must be unique within the scope of its ScenarioID.\n\n    Example JSON output:\n    {\n        \"ScenarioID\": \"Scenario_1\",\n        \"TestCaseID\": \"TestCase_1\",\n        \"Title\": \"Verify Login Functionality\",\n        \"Description\": \"Test the login functionality to ensure users can log in with valid credentials and receive appropriate error messages for invalid inputs. Additionally, ensure session management operates correctly post-login.\",\n        \"Objective\": \"Validate user authentication mechanism.\",\n        \"Category\": \"Functional Tests\",\n        \"Comments\": \"Ensure edge cases for invalid inputs are covered.\"\n    }\n    \n\n",
                "scenario_id": "edge_and_boundary_testing_Test_Scenario_5",
                "test_case": {
                  "TestScenarios": [
                    {
                      "ScenarioID": "edge_and_boundary_testing_Test_Scenario_5",
                      "TestCaseID": "edge_and_boundary_testing_Test_Scenario_5_TestCase_1",
                      "Title": "View Lists of Completed and Pending Tasks with Valid Input Data",
                      "Description": "Test the viewing of lists of completed and pending tasks functionality to ensure that users can successfully view lists of completed and pending tasks with valid input data. Verify that the system views lists of completed and pending tasks with the provided input data.",
                      "Objective": "Validate task listing mechanism.",
                      "Category": "Edge Cases and Boundary Testing",
                      "Comments": ""
                    },
                    {
                      "ScenarioID": "edge_and_boundary_testing_Test_Scenario_5",
                      "TestCaseID": "edge_and_boundary_testing_Test_Scenario_5_TestCase_2",
                      "Title": "View Lists of Completed and Pending Tasks with Invalid Format Input Data",
                      "Description": "Test the viewing of lists of completed and pending tasks functionality to ensure that the system handles invalid input data formats correctly. Verify that the system displays appropriate error messages when users enter invalid data.",
                      "Objective": "Validate task listing mechanism with invalid input data.",
                      "Category": "Edge Cases and Boundary Testing",
                      "Comments": ""
                    },
                    {
                      "ScenarioID": "edge_and_boundary_testing_Test_Scenario_5",
                      "TestCaseID": "edge_and_boundary_testing_Test_Scenario_5_TestCase_3",
                      "Title": "View Lists of Completed and Pending Tasks with Boundary Input Data",
                      "Description": "Test the viewing of lists of completed and pending tasks functionality to ensure that the system handles boundary input values correctly. Verify that the system displays the correct lists of completed and pending tasks when users enter boundary values.",
                      "Objective": "Validate task listing mechanism with boundary input data.",
                      "Category": "Edge Cases and Boundary Testing",
                      "Comments": ""
                    }
                  ]
                }
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6555"
                },
                "test_name": "Security Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive security test scenario aimed at evaluating the system’s resilience against various security threats. The scenario should focus on identifying vulnerabilities, ensuring data protection, and validating access controls within the specified system. Clearly define the purpose of the test, including the preconditions, environment configurations, and user roles required to simulate realistic security conditions. Identify key areas to be tested, such as authentication, authorization, data encryption, input validation, session management, and error handling. Outline test steps that cover common security tests such as SQL injection, XSS, CSRF, brute-force attacks, and privilege escalation attempts. Describe the expected outcomes for each test, including the system’s response to attempted breaches, error messages, and any logging requirements. Emphasize that the scenario aligns with ISTQB security testing standards, focusing on thorough vulnerability detection, data integrity, and system robustness against unauthorized access.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as preconditions, security vulnerabilities to test, expected outcomes, and error handling mechanisms.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for security testing, including elements like specific security threats, test steps, and expected defensive responses.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, security vulnerabilities, and expected outcomes match the intended security testing standards.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete security threat examples, detailed test steps, and clear expected outcomes for each threat. Specificity in vulnerability descriptions and system responses is essential for robust security testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of security testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of security testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, security vulnerabilities, expected outcomes, and error handling. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (security) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant test steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as vulnerability descriptions, expected results, and error handling requirements. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the system's resilience against security threats, focusing on identifying vulnerabilities and validating access controls as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for security testing, such as specific security vulnerabilities to test (e.g., SQL injection, XSS, CSRF), necessary preconditions, user roles, test steps, expected system responses, and error handling mechanisms. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with security testing standards and ISTQB guidelines. Ensure that the security threats and test steps correspond correctly to the objectives. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for security testing. Consider if the scenario is detailed, well-structured, and uses appropriate security terminology. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive set of security test cases to evaluate the system's resilience against threats. Include scenarios for testing authentication, input validation, data encryption, and unauthorized access prevention. Each test case should include detailed preconditions, steps to simulate security vulnerabilities, and expected outcomes. Align all test cases with ISTQB security testing guidelines to ensure thorough validation of system security measures.",
                "test_case_create_prompts": {
                  "Authentication Test Case": "Create a test case to verify the robustness of the system's authentication mechanism, including password strength requirements and protection against brute-force attacks.",
                  "Input Validation Security Test Case": "Develop a test case to test for vulnerabilities such as SQL injection, XSS, or CSRF attacks by providing malicious input. Specify expected outcomes like rejection of input and secure logging.",
                  "Data Encryption Test Case": "Design a test case to assess the effectiveness of encryption for sensitive data during transmission and storage. Specify tools and methods to verify encryption implementation.",
                  "Access Control Test Case": "Create a test case to ensure access control policies are enforced correctly, preventing unauthorized users from accessing restricted features or data."
                },
                "customised_prompt_status": false
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6556"
                },
                "test_name": "User Interface (GUI) Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive GUI test scenario to evaluate the usability, responsiveness, and visual consistency of the user interface for a specified system. This scenario should focus on ensuring that the interface meets design specifications and provides a seamless user experience. Clearly outline the purpose of the test, specifying necessary preconditions, screen resolutions, and device configurations required. Identify key UI elements to be tested, such as buttons, menus, text fields, alignment, colors, font sizes, and responsiveness across different devices. Describe test steps that include checking the interface's layout, navigation flow, interaction feedback, and responsiveness to user inputs. Define expected outcomes for each interaction, including alignment, visual feedback, error messages, and responsiveness criteria. Emphasize that the scenario aligns with ISTQB GUI testing standards, focusing on visual accuracy, and user experience quality.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as preconditions, UI elements, expected outcomes, and visual consistency checks.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for GUI testing, including elements like specific UI components, visual feedback expectations, and responsiveness requirements.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, UI elements, and expected outcomes match the intended GUI testing standards.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete examples for UI elements, detailed test steps, and clear expected results. Specificity in layout checks, responsiveness, and user feedback is essential for comprehensive GUI testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of GUI testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of GUI testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, UI components, expected outcomes, and visual consistency checks. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (GUI) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant UI elements, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as UI elements, layout expectations, and visual feedback. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the usability, responsiveness, and visual consistency of the user interface, focusing on UI elements and user experience as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for GUI testing, such as specific UI elements to be tested (buttons, menus, text fields), necessary preconditions (screen resolutions, device configurations), test steps, expected outcomes (alignment, visual feedback, responsiveness), and visual consistency checks. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with GUI testing standards and ISTQB guidelines. Check if the test steps accurately reflect the UI components and expected user interactions. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for GUI testing. Consider if the scenario effectively communicates the testing of UI elements and user experience aspects. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "As a senior ISTQB-certified test analyst, create a comprehensive set of GUI test cases to validate usability, visual consistency, and responsiveness. Include preconditions, test steps, and expected outcomes for all UI elements. Ensure test cases cover navigation, layout alignment, input handling, and feedback mechanisms. Align with ISTQB GUI testing standards for a seamless and user-friendly experience.",
                "test_case_create_prompts": {
                  "Visual Consistency Test Case": "Create a test case to ensure UI elements are visually consistent with design specifications across resolutions. Specify alignment, color schemes, and font size checks.",
                  "Navigation Flow Test Case": "Develop a test case to verify the intuitiveness of the navigation flow, ensuring users can seamlessly move between pages and access features.",
                  "Input Field Validation Test Case": "Design a test case to test input field behavior, including placeholder text, error messages, and constraints on input formats.",
                  "Responsiveness Test Case": "Create a test case to evaluate how the UI adapts to different screen sizes and orientations. Specify steps to verify element repositioning and integration."
                },
                "customised_prompt_status": false
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6550"
                },
                "test_name": "Compatibility Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design comprehensive test scenarios to evaluate the compatibility of a specified system across different environments. This scenario should focus on assessing the system’s ability to function as expected on various platforms, devices, operating systems, browsers, and configurations. Clearly outline the purpose of the test, including the preconditions and environmental setups required for accurate testing across multiple platforms. Specify a range of environments, such as specific operating system versions, browser types, device specifications, screen resolutions, and hardware configurations, ensuring broad coverage. Describe the expected outcomes for each environment, identifying potential compatibility issues, or visual rendering differences. Detail test steps that simulate user behavior in each environment to validate consistent performance. Emphasize that the scenario aligns with ISTQB compatibility testing standards, prioritizing cross-environment consistency, reliability, and user experience.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective, structured approach, and including essential elements such as environment configurations, expected outcomes, and compatibility criteria.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for compatibility testing, including specific platform details, and cross-environment checks.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, environment configurations, and expected outcomes match the intended category and testing standards for compatibility evaluation.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete configurations (such as operating system versions, browser types), detailed expected results, and identified compatibility metrics.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of compatibility testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of compatibility testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, environment configurations, expected outcomes, and compatibility checks. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (compatibility) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant configurations, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as platform versions, environment configurations, and expected responses. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively addresses compatibility testing across different environments as specified in the prompt. Consider if it includes testing on various platforms, devices, operating systems, browsers, and configurations. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for compatibility testing, such as detailed environment configurations, preconditions, test steps simulating user behavior on different platforms, and expected outcomes for each environment. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with compatibility testing standards and ISTQB guidelines. Check if the test steps and expected outcomes match the specified environments and configurations. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for compatibility testing. Consider if the scenario is clearly organized, uses technical language appropriately, and provides precise details. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "As a senior ISTQB-certified test analyst, design a comprehensive set of compatibility test cases to evaluate the across different platforms, devices, operating systems, and browsers. Specify preconditions, test steps, and expected outcomes for each scenario. Ensure the cases assess cross-platform consistency, and visual rendering. Align with ISTQB compatibility testing standards to ensure the system provides a seamless user experience in diverse environments.",
                "test_case_create_prompts": {
                  "Cross-Browser Test Case": "Create a test case to ensure consistency across multiple web browsers and their versions. Specify preconditions, browser configurations, detailed steps for validation, and expected outputs for compatibility verification.",
                  "Device Compatibility Test Case": "Develop a test case to evaluate system behavior on a variety of devices, such as smartphones, tablets, and desktops. Include steps to verify responsive design and integrition across device specifications.",
                  "Operating System Compatibility Test Case": "Design a test case to assess system performance on different operating systems, including specific versions. Include steps to test installation, execution, and interoperability between OS-specific features.",
                  "Screen Resolution Test Case": "Create a test case to validate the system's responsiveness and layout consistency across various screen resolutions. Include steps for testing visual elements and alignment across resolutions."
                },
                "customised_prompt_status": false
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6552"
                },
                "test_name": "Input Data Variety Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive test scenario focused on assessing the system’s ability to handle a diverse range of input data. The scenario should evaluate how the system processes various data types, formats, edge cases, and outliers to ensure robust input handling. Clearly define the purpose of the test, the preconditions required, and the environment configurations necessary for accurate testing. Specify a range of input data examples, including valid, invalid, boundary, and null values. Describe the expected system responses, detailing any validation errors, warnings, or specific handling methods required for each type of input. Outline test steps that simulate user behavior with different input scenarios to validate the system’s resilience. Emphasize that the scenario aligns with ISTQB testing standards, ensuring a thorough assessment of data handling robustness and consistency.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective, structured approach, and including all necessary elements such as input data examples, expected outcomes, and error handling.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for input variety testing, including elements like specific data types, validation requirements, and error handling processes.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, data requirements, and expected outcomes match the intended category and testing standards for input variety evaluation.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that include concrete examples of various input data types, specific configurations for the test environment, and clear definitions of expected outcomes for each input category. Specificity in data validation requirements and error expectations is essential.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of input data variety testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of input data variety testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, data examples (valid, invalid, boundary, null), and expected outcomes. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (input data variety) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as input data examples, error expectations, and response definitions. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the system's ability to handle a diverse range of input data, including valid, invalid, boundary, and null values, as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for input data variety testing, such as a range of input data examples, necessary preconditions, detailed test steps, expected system responses, validation errors, and error handling for each type of input. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with input data variety testing standards and ISTQB guidelines. Check if the test steps accurately reflect the diverse input data and expected outcomes. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for input data variety testing. Consider if the scenario effectively communicates the testing of different data types and their expected handling. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, generate a comprehensive set of test cases to validate the system's ability to handle diverse input data types. Include scenarios for valid, invalid, null, and edge-case inputs. Specify detailed steps, expected outcomes, and validation criteria for each input type. Align all test cases with ISTQB testing guidelines to ensure the system’s robustness in input handling.",
                "test_case_create_prompts": {
                  "Valid Input Test Case": "Create a test case to validate the system’s ability to process valid input data accurately. Include examples for various data types such as text, numbers, and dates.",
                  "Invalid Input Test Case": "Develop a test case to test system behavior with invalid inputs, ensuring appropriate error messages or rejections are triggered.",
                  "Boundary Input Test Case": "Design a test case to assess the system’s handling of input values at the boundaries of acceptable ranges, including just below and above these limits.",
                  "Special Character Input Test Case": "Create a test case to evaluate how the system handles inputs with special characters, emojis, or unexpected symbols. Specify expected sanitization or rejection behaviors."
                },
                "customised_prompt_status": false
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6554"
                },
                "test_name": "Performance and Load Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive performance and load testing scenario for a specified system. The scenario should aim to assess the system’s performance and load handling under realistic conditions, simulating user behaviors to evaluate how the system operates under various levels of stress. Clearly establish the purpose of the test, the preconditions required, and the environment configurations necessary to create accurate, real-world conditions. Outline the user load, including distribution and behavior patterns that reflect peak periods, random delays, and sudden surges. Identify key performance metrics such as response time, throughput, resource utilization, and error rates, detailing acceptable thresholds based on industry standards. Specify the data requirements to represent real-world usage, including edge cases, and present a clear, sequential process for test execution from setup to conclusion. Emphasize expected outcomes, define conditions for success and failure, and include guidelines for comprehensive logging and analysis. Ensure that the scenario aligns with ISTQB performance testing standards, incorporates best practices, and prioritizes precision, consistency, and robustness.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, depth, and clarity. A high-quality scenario should align well with the specified test purpose, providing a clear objective and structured approach. It should include all necessary elements such as preconditions, environment configuration, user load details, performance metrics, and expected outcomes.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for performance and load testing, including specified elements like response time, error rates, data requirements, and system behavior under stress.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described testing steps, data requirements, and performance metrics match the intended category and testing standards for performance and load evaluation.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete numbers for user loads, specific configurations for the test environment, and clear definitions of success and failure thresholds. Specificity in metrics and detailed steps for test execution are essential.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of performance and load testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of performance and load testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, user load distribution, key performance metrics, and expected outcomes. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (performance and load) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched metrics or off-target objectives, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as user load numbers, test environment configurations, and threshold definitions. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the system's performance and load handling under various levels of stress, simulating realistic user behaviors and conditions as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for performance and load testing, such as user load details (distribution, behavior patterns), key performance metrics (response time, throughput, resource utilization, error rates), necessary preconditions, environment configurations, and expected outcomes. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with performance and load testing standards and ISTQB guidelines. Check if the test steps accurately reflect the intended load conditions and performance metrics. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for performance and load testing. Consider if the scenario effectively communicates the testing process and performance expectations. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive set of performance and load test cases to evaluate the system's ability to handle various levels of stress. Ensure that the test cases cover response time, throughput, scalability, and stability under different load conditions. Include preconditions, detailed execution steps, specific input data, and measurable expected outcomes for each scenario. Align all test cases with ISTQB performance testing guidelines to validate both typical and edge performance conditions. Emphasize precision, accuracy, and reproducibility in test documentation.",
                "test_case_create_prompts": {
                  "Core Performance Test Case": "Develop a test case to verify the system's ability to handle normal operational loads. Include typical user behavior as input, preconditions for environment setup, and measurable metrics such as response time and system throughput. Ensure test steps simulate realistic user actions and specify precise success criteria.",
                  "Stress Test Case": "Create a test case to evaluate how the system behaves under extreme stress, such as peak user loads or resource exhaustion. Include detailed steps for incrementally increasing load, monitoring system behavior, and identifying failure points. Specify thresholds for acceptable and critical performance levels.",
                  "Scalability Test Case": "Design a test case to assess the system's ability to scale with increasing workload. Include steps to simulate user growth, resource allocation adjustments, and infrastructure elasticity. Define metrics like maximum concurrent users or transactions per second.",
                  "Resource Utilization Test Case": "Create a test case focusing on system resource usage under varying load conditions. Specify steps to measure CPU, memory, disk I/O, and network bandwidth. Include scenarios for detecting bottlenecks and provide acceptable utilization thresholds."
                },
                "customised_prompt_status": false
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6553"
                },
                "test_name": "Integration Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive integration test scenario aimed at evaluating how different modules or components of a specified system interact with each other. This scenario should focus on ensuring that integrated components communicate correctly and work together. Clearly outline the purpose of the test, specifying necessary preconditions, setup configurations, and initial data required. Identify key interactions between components, including input and output data flow, dependencies, and any shared resources. Outline test steps that simulate realistic data exchange and process flows between components, capturing both normal and abnormal interaction patterns. Describe expected outcomes for each interaction, including response times, data integrity, and error handling requirements. Emphasize that the scenario aligns with ISTQB integration testing standards, focusing on accuracy, consistency, and robustness in component interaction validation.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as preconditions, data flow between components, expected outcomes, and error handling mechanisms.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for integration testing, including specific component interactions, data exchange, and expected results.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, component interactions, and expected outcomes match the intended integration testing standards.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that include concrete component interactions, detailed data flows, and clear expected outcomes for each integration point. Specificity in data handling and error scenarios is essential for thorough integration testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of integration testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of integration testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, data flow between components, expected outcomes, and error handling. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (integration) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant test steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as component interactions, expected results, and error handling mechanisms. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the interactions between different modules or components of the system, focusing on data flow and communication between components as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for integration testing, such as detailed component interactions, necessary preconditions, data exchange processes, dependencies, shared resources, and expected outcomes for each interaction. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with integration testing standards and ISTQB guidelines. Ensure that the interactions between components are correctly represented and that expected outcomes are properly defined. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for integration testing. Consider if the scenario clearly outlines component interactions and is structured logically. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "As a senior ISTQB-certified test analyst, generate a set of integration test cases to evaluate interactions between system components. Include detailed preconditions, interaction sequences, and data flow verification steps. Focus on validating module communication, data consistency, and error handling. Ensure the test cases align with ISTQB integration testing standards.",
                "test_case_create_prompts": {
                  "Interface Communication Test Case": "Develop a test case to validate communication between integrated components, ensuring proper data exchange and synchronization. Include scenarios for both normal and interrupted communication.",
                  "Data Consistency Test Case": "Create a test case to verify that data remains consistent when shared or transferred between modules. Specify test steps for simulating updates, deletions, and retrievals.",
                  "Shared Resource Access Test Case": "Design a test case to assess how integrated components handle access to shared resources, avoiding conflicts or deadlocks. Include test steps for concurrent access scenarios.",
                  "Error Propagation Test Case": "Create a test case to evaluate the system’s ability to handle and isolate errors in one component without affecting others. Include scenarios to test error logging and recovery mechanisms."
                },
                "customised_prompt_status": false
              }
            ],
            "process_title": "edge_and_boundary_testing",
            "document_type": "Requirements Document",
            "generated_prompt": "{'test_name': 'Edge Cases and Boundary Testing for TaskMaster Requirements Document', 'document_type': 'Requirements Document', 'document_content': '# Requirements Specification Document: TaskMaster\\n\\n## 1. Functional Requirements\\n1.1 Users should be able to create new tasks.\\n1.2 Users should be able to organize tasks under projects.\\n1.3 Users should be able to mark tasks as completed or pending.\\n1.4 Users should be able to remove specific tasks from a project.\\n1.5 Users should be able to view lists of completed and pending tasks.\\n\\n## 2. Performance Requirements\\n2.1 The application should support projects with up to 100 tasks.\\n2.2 The application should respond to user interactions within 1 second.\\n\\n## 3. Usability Requirements\\n3.1 The user interface should be simple and user-friendly.\\n3.2 Users should be able to add a new task in no more than three steps.\\n\\n## 4. Security Requirements\\n4.1 The application should securely store user data.\\n4.2 The application should protect user data from unauthorized access.\\n\\n## 5. Extensibility Requirements\\n5.1 The application should be designed to allow for future API integrations.\\n5.2 The application should support the addition of multi-language support.', 'test objectives': [\"Evaluate the system's response to edge cases and boundary values.\"], 'preconditions': ['System is installed and configured correctly.', 'User has access to necessary permissions and credentials.'], 'environment_configurations': ['Operating System: [list operating systems].', 'Browser: [list supported browsers].'], 'test_steps': [{'step_id': 1, 'description': 'Create a new task with valid input data.', 'boundary_values': ['Just below limits', 'At limits', 'Just above limits'], 'edge_cases': ['Invalid user data', 'Empty field values']}, {'step_id': 2, 'description': 'Organize tasks under projects with valid input data.', 'boundary_values': ['Just below limits', 'At limits', 'Just above limits'], 'edge_cases': ['Invalid project name', 'Empty field values']}, {'step_id': 3, 'description': 'Mark tasks as completed or pending with valid input data.', 'boundary_values': ['Just below limits', 'At limits', 'Just above limits'], 'edge_cases': ['Invalid task status', 'Empty field values']}, {'step_id': 4, 'description': 'Remove specific tasks from a project with valid input data.', 'boundary_values': ['Just below limits', 'At limits', 'Just above limits'], 'edge_cases': ['Invalid task ID', 'Empty field values']}, {'step_id': 5, 'description': 'View lists of completed and pending tasks with valid input data.', 'boundary_values': ['Just below limits', 'At limits', 'Just above limits'], 'edge_cases': ['Invalid task status', 'Empty field values']}], 'expected_outcomes': [{'step_id': 1, 'description': 'The system creates a new task with the provided input data.'}, {'step_id': 2, 'description': 'The system organizes tasks under projects with the provided input data.'}, {'step_id': 3, 'description': 'The system marks tasks as completed or pending with the provided input data.'}, {'step_id': 4, 'description': 'The system removes specific tasks from a project with the provided input data.'}, {'step_id': 5, 'description': 'The system views lists of completed and pending tasks with the provided input data.'}], 'error_handling': [{'step_id': 1, 'description': 'Handle invalid user data by displaying an error message.'}, {'step_id': 2, 'description': 'Handle empty field values by skipping the operation and indicating it in the log.'}]}\n\nInstruction and consistency situation: \nEnsure that each generated scenario directly corresponds to the specified context and purpose of boundary and edge case testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.\n\nInstruction and consistency situation: \nEvaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'\n\nScoring situation: \n Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that provide concrete boundary values, clear definitions of acceptable limits, and detailed error handling for edge cases. Specificity in test data and expected results is essential for robust testing.\n\n\n    JSON Output Structure:\n\n    {\n        \"TestScenarios\": [\n            {\n                \"ScenarioID\": \"edge_and_boundary_testing_Test_Scenario_1\",\n                \"Title\": \"<Scenario Title>\",\n                \"Description\": \"<Detailed scenario description at least 3 sentences. This is the most important part of the generate test scenario!>\",\n                \"Objective\": \"<Objective or goal of the scenario>\",\n                \"Category\": \"Edge Cases and Boundary Testing\",\n                \"Comments\": \"<Any inconsistency or additional notes>\"\n            }\n        ]\n    }\n\n    Guidelines for JSON Structure:\n    - Ensure the ScenarioID is dynamic and matches the required format.\n\n    Example JSON output:\n    {\n        \"ScenarioID\": \"Process_A_ModelX_Test_Scenario_1\",\n        \"Title\": \"Verify Login Functionality\",\n        \"Description\": \"Test the login functionality to ensure that users can successfully log in with valid credentials and are rejected with incorrect credentials. Additionally, verify that the system displays appropriate error messages for failed login attempts to guide users in correcting their input. Furthermore, ensure that the login session is maintained correctly, allowing users to access their accounts seamlessly after a successful login.\",\n        \"Objective\": \"Validate user authentication mechanism.\",\n        \"Category\": \"Functional Tests\",\n        \"Comments\": \"\"\n    }\n\n    This document is classified as a Requirements Document. When generating test scenarios, ensure that the structure, format, and content align with the nature of this document type.\n\n    Document Content: \n    # Requirements Specification Document: TaskMaster\n\n## 1. Functional Requirements\n1.1 Users should be able to create new tasks.\n1.2 Users should be able to organize tasks under projects.\n1.3 Users should be able to mark tasks as completed or pending.\n1.4 Users should be able to remove specific tasks from a project.\n1.5 Users should be able to view lists of completed and pending tasks.\n\n## 2. Performance Requirements\n2.1 The application should support projects with up to 100 tasks.\n2.2 The application should respond to user interactions within 1 second.\n\n## 3. Usability Requirements\n3.1 The user interface should be simple and user-friendly.\n3.2 Users should be able to add a new task in no more than three steps.\n\n## 4. Security Requirements\n4.1 The application should securely store user data.\n4.2 The application should protect user data from unauthorized access.\n\n## 5. Extensibility Requirements\n5.1 The application should be designed to allow for future API integrations.\n5.2 The application should support the addition of multi-language support.\n\n    ",
            "model_output": {
              "TestScenarios": [
                {
                  "ScenarioID": "edge_and_boundary_testing_Test_Scenario_1",
                  "Title": "Verify Task Creation Functionality",
                  "Description": "Test the task creation functionality to ensure that users can successfully create new tasks with valid input data. Verify that the system creates a new task with the provided input data and handles invalid user data by displaying an error message. Additionally, ensure that the system responds within 1 second to user interactions.",
                  "Objective": "Validate task creation mechanism.",
                  "Category": "Edge Cases and Boundary Testing",
                  "Comments": ""
                },
                {
                  "ScenarioID": "edge_and_boundary_testing_Test_Scenario_2",
                  "Title": "Organize Tasks Under Projects with Valid Input Data",
                  "Description": "Test the organization of tasks under projects functionality to ensure that users can successfully organize tasks under projects with valid input data. Verify that the system organizes tasks under projects with the provided input data and handles invalid project name by skipping the operation and indicating it in the log.",
                  "Objective": "Validate task organization mechanism.",
                  "Category": "Edge Cases and Boundary Testing",
                  "Comments": ""
                },
                {
                  "ScenarioID": "edge_and_boundary_testing_Test_Scenario_3",
                  "Title": "Mark Tasks as Completed or Pending with Valid Input Data",
                  "Description": "Test the marking of tasks as completed or pending functionality to ensure that users can successfully mark tasks as completed or pending with valid input data. Verify that the system marks tasks as completed or pending with the provided input data and handles invalid task status by displaying an error message.",
                  "Objective": "Validate task status mechanism.",
                  "Category": "Edge Cases and Boundary Testing",
                  "Comments": ""
                },
                {
                  "ScenarioID": "edge_and_boundary_testing_Test_Scenario_4",
                  "Title": "Remove Specific Tasks from a Project with Valid Input Data",
                  "Description": "Test the removal of specific tasks from a project functionality to ensure that users can successfully remove specific tasks from a project with valid input data. Verify that the system removes specific tasks from a project with the provided input data and handles invalid task ID by displaying an error message.",
                  "Objective": "Validate task removal mechanism.",
                  "Category": "Edge Cases and Boundary Testing",
                  "Comments": ""
                },
                {
                  "ScenarioID": "edge_and_boundary_testing_Test_Scenario_5",
                  "Title": "View Lists of Completed and Pending Tasks with Valid Input Data",
                  "Description": "Test the viewing of lists of completed and pending tasks functionality to ensure that users can successfully view lists of completed and pending tasks with valid input data. Verify that the system views lists of completed and pending tasks with the provided input data.",
                  "Objective": "Validate task listing mechanism.",
                  "Category": "Edge Cases and Boundary Testing",
                  "Comments": ""
                }
              ]
            }
          }
          {
            "_id": {
              "$oid": "677d348b893d231f01eb59d2"
            },
            "session_id": "20250107170458",
            "original_prompts": [
              {
                "_id": {
                  "$oid": "673d7efca741904082541a61"
                },
                "test_name": "Functional Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a detailed functional test scenario focused on evaluating the system’s core functionalities against specified requirements. This scenario should assess whether the system performs its intended operations correctly, meeting both functional specifications and expected user behaviors. Clearly define the purpose of the test, including necessary preconditions, setup configurations, and initial data required. Identify the primary functions to be tested, including input conditions, expected outputs, and any error-handling mechanisms. Outline specific test steps that simulate realistic user actions to validate each function. Describe the expected outcomes for each step, ensuring that both positive and negative cases are covered. Emphasize that the scenario aligns with ISTQB functional testing standards, aiming for accuracy, completeness, and robustness in functional validation.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as preconditions, test steps, expected outcomes, and error handling requirements.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for functional testing, including elements like input conditions, functional checks, and expected results.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, expected outcomes, and error-handling match the intended functional testing standards.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete examples for inputs, clear test step definitions, and detailed expected results. Specificity in test data and expected outcomes is essential for robust functional testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of functional testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of functional testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, input conditions, expected outputs, and error handling. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (functional) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant test steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as input conditions, expected results, and error handling. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the system's core functionalities against the specified requirements, focusing on whether it tests intended operations and expected user behaviors as outlined in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for functional testing, such as clear objectives, necessary preconditions, detailed test steps simulating realistic user actions, input conditions, expected outputs, and error handling mechanisms. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with functional testing standards and ISTQB guidelines. Ensure that the test steps are appropriate for the functionalities being tested and that expected outcomes are properly defined. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for functional testing. Consider if the scenario is well-organized, uses precise technical language, and provides clear instructions. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_create_prompts": {
                  "Core Functionalities Test Case": "Create a test case that verifies the core functionalities of the system as defined in the requirements. Include preconditions, detailed steps, input data, and expected results to ensure the system performs its intended operations correctly. Ensure the case aligns with ISTQB Functional Testing guidelines. Specify each step's expected output clearly and concisely.",
                  "Boundary Value Analysis Test Case": "Develop a test case focusing on Boundary Value Analysis (BVA) to evaluate system behavior at the edges of input ranges. Include preconditions, input data at lower and upper boundaries, and the expected system response for each scenario. Define clear test steps for validating boundary conditions, ensuring compliance with functional testing standards.",
                  "Error Recovery Functional Test Case": "Design a test case to assess the system's error recovery mechanisms during normal operations. Include scenarios with invalid inputs, interruptions, or fault conditions. Specify detailed steps to simulate these scenarios and describe the expected system behavior for each step, including recovery or error handling outputs.",
                  "User Input Validation Test Case": "Create a test case to validate the system's ability to handle various user inputs. Include valid, invalid, and edge-case inputs, with steps to simulate input actions and monitor system responses. Specify preconditions, detailed test steps, and expected outcomes to ensure the system handles inputs correctly and provides appropriate feedback for invalid inputs."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, generate a comprehensive set of functional test cases based on the provided test scenario. Ensure that the test cases collectively cover all actions, steps, and functional areas outlined in the scenario. Each test case should include detailed descriptions of preconditions, input conditions, step-by-step actions, and expected outcomes. Cover positive, negative, and boundary cases to ensure complete validation of the system’s functionalities. Include specific checks for error handling and edge cases where applicable. Organize the test cases to ensure clarity and traceability, aligning with ISTQB standards. The goal is to produce the maximum number of test cases necessary to comprehensively validate the scenario while avoiding redundancy."
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6551"
                },
                "test_name": "Edge Cases and Boundary Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design detailed test scenarios focused on evaluating the system's response to edge cases and boundary values. The scenario should explore how the system behaves at the extremes of input ranges, identifying potential vulnerabilities and ensuring reliability. Clearly define the purpose of the test, necessary preconditions, and environment configurations to create accurate testing conditions. Specify a range of test cases, including boundary values (just below, at, and just above limits) and edge cases that challenge system assumptions. Describe the expected outcomes for each case, detailing system responses, error handling, or corrective measures as needed. Outline the test steps, ensuring they capture all critical values for each parameter. Emphasize that the scenario aligns with ISTQB standards for boundary testing, aiming to identify weaknesses and ensure robustness against extreme inputs.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as boundary value examples, edge cases, expected outcomes, and specific error handling requirements.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for edge cases and boundary testing, including elements like precise boundary values, error handling expectations, and critical input limits.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, boundary values, and expected outcomes match the intended category and testing standards for boundary and edge case evaluation.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that provide concrete boundary values, clear definitions of acceptable limits, and detailed error handling for edge cases. Specificity in test data and expected results is essential for robust testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of boundary and edge case testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of boundary and edge case testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, boundary values, edge cases, and expected outcomes. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (boundary and edge cases) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as boundary values, edge case examples, and error expectations. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively targets edge cases and boundary values, exploring system behavior at the extremes of input ranges as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for edge case and boundary testing, such as specific boundary values (just below, at, and just above limits), detailed preconditions, test steps, and expected outcomes for each case. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with boundary testing standards and ISTQB guidelines. Check if the test steps correspond correctly to the specified boundary values. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for boundary testing. Consider if the scenario clearly communicates the test conditions and expectations. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, create a detailed set of test cases to evaluate system behavior under edge case scenarios and at boundary input values. Include preconditions, specific test data for both lower and upper limits, and steps to assess how the system handles inputs outside the expected range. Ensure scenarios include both positive and negative test cases to validate robustness and reliability. Align test cases with ISTQB boundary testing guidelines for comprehensive validation of system behavior.",
                "test_case_create_prompts": {
                  "Boundary Value Analysis Test Case": "Design a test case focusing on testing inputs at the boundaries of acceptable ranges. Specify lower and upper limits, just below and above these limits, and expected outcomes for each condition. Include clear steps for validating the system's response to each input.",
                  "Extreme Input Test Case": "Create a test case to evaluate system performance and stability when handling extreme or unrealistic input values. Define test data that pushes the system beyond its designed operational range, specifying the expected error messages or graceful degradation responses.",
                  "Null and Empty Input Test Case": "Develop a test case to assess system behavior when null or empty inputs are provided. Include detailed steps to simulate such inputs and expected outcomes, such as validation error messages or fallback defaults.",
                  "Invalid Format Test Case": "Create a test case to evaluate the system's validation mechanisms when inputs are provided in incorrect formats. Include examples such as malformed dates, special characters in numeric fields, or overly long strings, and specify expected error handling outcomes."
                }
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6555"
                },
                "test_name": "Security Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive security test scenario aimed at evaluating the system’s resilience against various security threats. The scenario should focus on identifying vulnerabilities, ensuring data protection, and validating access controls within the specified system. Clearly define the purpose of the test, including the preconditions, environment configurations, and user roles required to simulate realistic security conditions. Identify key areas to be tested, such as authentication, authorization, data encryption, input validation, session management, and error handling. Outline test steps that cover common security tests such as SQL injection, XSS, CSRF, brute-force attacks, and privilege escalation attempts. Describe the expected outcomes for each test, including the system’s response to attempted breaches, error messages, and any logging requirements. Emphasize that the scenario aligns with ISTQB security testing standards, focusing on thorough vulnerability detection, data integrity, and system robustness against unauthorized access.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as preconditions, security vulnerabilities to test, expected outcomes, and error handling mechanisms.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for security testing, including elements like specific security threats, test steps, and expected defensive responses.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, security vulnerabilities, and expected outcomes match the intended security testing standards.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete security threat examples, detailed test steps, and clear expected outcomes for each threat. Specificity in vulnerability descriptions and system responses is essential for robust security testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of security testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of security testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, security vulnerabilities, expected outcomes, and error handling. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (security) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant test steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as vulnerability descriptions, expected results, and error handling requirements. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the system's resilience against security threats, focusing on identifying vulnerabilities and validating access controls as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for security testing, such as specific security vulnerabilities to test (e.g., SQL injection, XSS, CSRF), necessary preconditions, user roles, test steps, expected system responses, and error handling mechanisms. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with security testing standards and ISTQB guidelines. Ensure that the security threats and test steps correspond correctly to the objectives. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for security testing. Consider if the scenario is detailed, well-structured, and uses appropriate security terminology. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive set of security test cases to evaluate the system's resilience against threats. Include scenarios for testing authentication, input validation, data encryption, and unauthorized access prevention. Each test case should include detailed preconditions, steps to simulate security vulnerabilities, and expected outcomes. Align all test cases with ISTQB security testing guidelines to ensure thorough validation of system security measures.",
                "test_case_create_prompts": {
                  "Authentication Test Case": "Create a test case to verify the robustness of the system's authentication mechanism, including password strength requirements and protection against brute-force attacks.",
                  "Input Validation Security Test Case": "Develop a test case to test for vulnerabilities such as SQL injection, XSS, or CSRF attacks by providing malicious input. Specify expected outcomes like rejection of input and secure logging.",
                  "Data Encryption Test Case": "Design a test case to assess the effectiveness of encryption for sensitive data during transmission and storage. Specify tools and methods to verify encryption implementation.",
                  "Access Control Test Case": "Create a test case to ensure access control policies are enforced correctly, preventing unauthorized users from accessing restricted features or data."
                }
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6556"
                },
                "test_name": "User Interface (GUI) Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive GUI test scenario to evaluate the usability, responsiveness, and visual consistency of the user interface for a specified system. This scenario should focus on ensuring that the interface meets design specifications and provides a seamless user experience. Clearly outline the purpose of the test, specifying necessary preconditions, screen resolutions, and device configurations required. Identify key UI elements to be tested, such as buttons, menus, text fields, alignment, colors, font sizes, and responsiveness across different devices. Describe test steps that include checking the interface's layout, navigation flow, interaction feedback, and responsiveness to user inputs. Define expected outcomes for each interaction, including alignment, visual feedback, error messages, and responsiveness criteria. Emphasize that the scenario aligns with ISTQB GUI testing standards, focusing on visual accuracy, and user experience quality.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as preconditions, UI elements, expected outcomes, and visual consistency checks.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for GUI testing, including elements like specific UI components, visual feedback expectations, and responsiveness requirements.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, UI elements, and expected outcomes match the intended GUI testing standards.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete examples for UI elements, detailed test steps, and clear expected results. Specificity in layout checks, responsiveness, and user feedback is essential for comprehensive GUI testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of GUI testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of GUI testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, UI components, expected outcomes, and visual consistency checks. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (GUI) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant UI elements, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as UI elements, layout expectations, and visual feedback. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the usability, responsiveness, and visual consistency of the user interface, focusing on UI elements and user experience as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for GUI testing, such as specific UI elements to be tested (buttons, menus, text fields), necessary preconditions (screen resolutions, device configurations), test steps, expected outcomes (alignment, visual feedback, responsiveness), and visual consistency checks. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with GUI testing standards and ISTQB guidelines. Check if the test steps accurately reflect the UI components and expected user interactions. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for GUI testing. Consider if the scenario effectively communicates the testing of UI elements and user experience aspects. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "As a senior ISTQB-certified test analyst, create a comprehensive set of GUI test cases to validate usability, visual consistency, and responsiveness. Include preconditions, test steps, and expected outcomes for all UI elements. Ensure test cases cover navigation, layout alignment, input handling, and feedback mechanisms. Align with ISTQB GUI testing standards for a seamless and user-friendly experience.",
                "test_case_create_prompts": {
                  "Visual Consistency Test Case": "Create a test case to ensure UI elements are visually consistent with design specifications across resolutions. Specify alignment, color schemes, and font size checks.",
                  "Navigation Flow Test Case": "Develop a test case to verify the intuitiveness of the navigation flow, ensuring users can seamlessly move between pages and access features.",
                  "Input Field Validation Test Case": "Design a test case to test input field behavior, including placeholder text, error messages, and constraints on input formats.",
                  "Responsiveness Test Case": "Create a test case to evaluate how the UI adapts to different screen sizes and orientations. Specify steps to verify element repositioning and integration."
                }
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6550"
                },
                "test_name": "Compatibility Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design comprehensive test scenarios to evaluate the compatibility of a specified system across different environments. This scenario should focus on assessing the system’s ability to function as expected on various platforms, devices, operating systems, browsers, and configurations. Clearly outline the purpose of the test, including the preconditions and environmental setups required for accurate testing across multiple platforms. Specify a range of environments, such as specific operating system versions, browser types, device specifications, screen resolutions, and hardware configurations, ensuring broad coverage. Describe the expected outcomes for each environment, identifying potential compatibility issues, or visual rendering differences. Detail test steps that simulate user behavior in each environment to validate consistent performance. Emphasize that the scenario aligns with ISTQB compatibility testing standards, prioritizing cross-environment consistency, reliability, and user experience.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective, structured approach, and including essential elements such as environment configurations, expected outcomes, and compatibility criteria.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for compatibility testing, including specific platform details, and cross-environment checks.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, environment configurations, and expected outcomes match the intended category and testing standards for compatibility evaluation.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete configurations (such as operating system versions, browser types), detailed expected results, and identified compatibility metrics.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of compatibility testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of compatibility testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, environment configurations, expected outcomes, and compatibility checks. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (compatibility) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant configurations, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as platform versions, environment configurations, and expected responses. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively addresses compatibility testing across different environments as specified in the prompt. Consider if it includes testing on various platforms, devices, operating systems, browsers, and configurations. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for compatibility testing, such as detailed environment configurations, preconditions, test steps simulating user behavior on different platforms, and expected outcomes for each environment. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with compatibility testing standards and ISTQB guidelines. Check if the test steps and expected outcomes match the specified environments and configurations. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for compatibility testing. Consider if the scenario is clearly organized, uses technical language appropriately, and provides precise details. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "As a senior ISTQB-certified test analyst, design a comprehensive set of compatibility test cases to evaluate the across different platforms, devices, operating systems, and browsers. Specify preconditions, test steps, and expected outcomes for each scenario. Ensure the cases assess cross-platform consistency, and visual rendering. Align with ISTQB compatibility testing standards to ensure the system provides a seamless user experience in diverse environments.",
                "test_case_create_prompts": {
                  "Cross-Browser Test Case": "Create a test case to ensure consistency across multiple web browsers and their versions. Specify preconditions, browser configurations, detailed steps for validation, and expected outputs for compatibility verification.",
                  "Device Compatibility Test Case": "Develop a test case to evaluate system behavior on a variety of devices, such as smartphones, tablets, and desktops. Include steps to verify responsive design and integrition across device specifications.",
                  "Operating System Compatibility Test Case": "Design a test case to assess system performance on different operating systems, including specific versions. Include steps to test installation, execution, and interoperability between OS-specific features.",
                  "Screen Resolution Test Case": "Create a test case to validate the system's responsiveness and layout consistency across various screen resolutions. Include steps for testing visual elements and alignment across resolutions."
                }
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6552"
                },
                "test_name": "Input Data Variety Testing",
                "test_prompt": {
                  "test_name": "Input Data Variety Testing",
                  "document_type": "Technical Design Document",
                  "content": [
                    "# Technical Design Document: TaskMaster",
                    "TaskMaster is a simple task management system that allows users to manage tasks and projects.",
                    "The application provides an interface where users can list tasks, add new tasks, and mark tasks as completed."
                  ],
                  "objectives": {
                    "test_purpose": "Assess the system's ability to handle a diverse range of input data, evaluating its processing of various data types, formats, edge cases, and outliers.",
                    "preconditions": [
                      {
                        "environment_configuration": "Development environment with TaskMaster application installed."
                      }
                    ]
                  },
                  "input_data_scenarios": {
                    "valid_input_types": [
                      {
                        "data_type": "string",
                        "description": "Sample string input for task title"
                      },
                      {
                        "data_type": "boolean",
                        "description": "Sample boolean value for task completed status"
                      }
                    ],
                    "invalid_input_types": [
                      {
                        "data_type": "null",
                        "description": "Sample null input for task description"
                      }
                    ]
                  },
                  "expected_system_responses": {
                    "validation_errors": [
                      {
                        "error_message": "Invalid input type: null",
                        "expected_response": "Error message with error code"
                      }
                    ],
                    "specific_handling_methods": [
                      {
                        "input_type": "string",
                        "description": "Special handling for string inputs to prevent SQL injection attacks"
                      }
                    ]
                  },
                  "test_steps": {
                    "step_1": {
                      "action": "Create a new project instance",
                      " precondition": [
                        {
                          "environment_configuration": "Development environment with TaskMaster application installed."
                        }
                      ],
                      "expected_response": "Project creation successful"
                    }
                  }
                },
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective, structured approach, and including all necessary elements such as input data examples, expected outcomes, and error handling.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for input variety testing, including elements like specific data types, validation requirements, and error handling processes.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, data requirements, and expected outcomes match the intended category and testing standards for input variety evaluation.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that include concrete examples of various input data types, specific configurations for the test environment, and clear definitions of expected outcomes for each input category. Specificity in data validation requirements and error expectations is essential.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of input data variety testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of input data variety testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, data examples (valid, invalid, boundary, null), and expected outcomes. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (input data variety) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as input data examples, error expectations, and response definitions. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the system's ability to handle a diverse range of input data, including valid, invalid, boundary, and null values, as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for input data variety testing, such as a range of input data examples, necessary preconditions, detailed test steps, expected system responses, validation errors, and error handling for each type of input. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with input data variety testing standards and ISTQB guidelines. Check if the test steps accurately reflect the diverse input data and expected outcomes. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for input data variety testing. Consider if the scenario effectively communicates the testing of different data types and their expected handling. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, generate a comprehensive set of test cases to validate the system's ability to handle diverse input data types. Include scenarios for valid, invalid, null, and edge-case inputs. Specify detailed steps, expected outcomes, and validation criteria for each input type. Align all test cases with ISTQB testing guidelines to ensure the system’s robustness in input handling.",
                "test_case_create_prompts": {
                  "Valid Input Test Case": "Create a test case to validate the system’s ability to process valid input data accurately. Include examples for various data types such as text, numbers, and dates.",
                  "Invalid Input Test Case": "Develop a test case to test system behavior with invalid inputs, ensuring appropriate error messages or rejections are triggered.",
                  "Boundary Input Test Case": "Design a test case to assess the system’s handling of input values at the boundaries of acceptable ranges, including just below and above these limits.",
                  "Special Character Input Test Case": "Create a test case to evaluate how the system handles inputs with special characters, emojis, or unexpected symbols. Specify expected sanitization or rejection behaviors."
                },
                "customised_prompt_status": true,
                "combined_prompt": "Acting as a senior ISTQB-certified test analyst, generate a comprehensive set of test cases to validate the system's ability to handle diverse input data types. Include scenarios for valid, invalid, null, and edge-case inputs. Specify detailed steps, expected outcomes, and validation criteria for each input type. Align all test cases with ISTQB testing guidelines to ensure the system’s robustness in input handling.\n\nScenario Details:\nScenarioID: Input_Data_Variety_Testing_Test_Scenario_5\nTitle: Validate Special Handling for String Inputs to Prevent SQL Injection Attacks\nDescription: Create a new task with string input that requires special handling to prevent SQL injection attacks. Verify that the system processes the string input correctly and no vulnerabilities are exposed.\nObjective: Confirm that TaskMaster's string inputs are processed securely, preventing SQL injection attacks\nCategory: Input Data Variety Testing\nComments: \n\nCombined Test Case Prompts:\nTest Case Type: Valid Input Test Case\nCreate a test case to validate the system’s ability to process valid input data accurately. Include examples for various data types such as text, numbers, and dates.\n\nTest Case Type: Invalid Input Test Case\nDevelop a test case to test system behavior with invalid inputs, ensuring appropriate error messages or rejections are triggered.\n\nTest Case Type: Boundary Input Test Case\nDesign a test case to assess the system’s handling of input values at the boundaries of acceptable ranges, including just below and above these limits.\n\n\n    JSON Output Structure:\n\n    {\n        \"TestScenarios\": [\n            {\n                \"ScenarioID\": \"<Dynamic Scenario ID>\",\n                \"TestCaseID\": \"<Dynamic Test Case ID>\",\n                \"Title\": \"<Scenario Title>\",\n                \"Description\": \"<Detailed scenario description at least 3 sentences. This is the most important part of the test scenario!>\",\n                \"Objective\": \"<Objective or goal of the scenario>\",\n                \"Category\": \"<Category of the test case>\",\n                \"Comments\": \"<Any inconsistency or additional notes>\"\n            }\n        ]\n    }\n\n    Guidelines for JSON Structure:\n    - Ensure the ScenarioID is dynamic and matches the required format.\n    - TestCaseID must be unique within the scope of its ScenarioID.\n\n    Example JSON output:\n    {\n        \"ScenarioID\": \"Scenario_1\",\n        \"TestCaseID\": \"TestCase_1\",\n        \"Title\": \"Verify Login Functionality\",\n        \"Description\": \"Test the login functionality to ensure users can log in with valid credentials and receive appropriate error messages for invalid inputs. Additionally, ensure session management operates correctly post-login.\",\n        \"Objective\": \"Validate user authentication mechanism.\",\n        \"Category\": \"Functional Tests\",\n        \"Comments\": \"Ensure edge cases for invalid inputs are covered.\"\n    }\n    \n\n",
                "scenario_id": "Input_Data_Variety_Testing_Test_Scenario_5",
                "test_case": {
                  "TestScenarios": [
                    {
                      "ScenarioID": "Input_Data_Variety_Testing_Test_Scenario_5",
                      "TestCaseID": "Valid_String_Input_Test_Case",
                      "Title": "Validate Handling of Valid String Input to Prevent SQL Injection Attacks",
                      "Description": "This test case verifies that the system correctly processes valid string inputs, ensuring no SQL injection vulnerabilities are exposed.",
                      "Objective": "Confirm that TaskMaster's string inputs are processed securely, preventing SQL injection attacks.",
                      "Category": "Input Data Variety Testing",
                      "Comments": "Test with a variety of valid string inputs to ensure robust handling."
                    },
                    {
                      "ScenarioID": "Input_Data_Variety_Testing_Test_Scenario_5",
                      "TestCaseID": "Invalid_String_Input_Test_Case",
                      "Title": "Validate Handling of Invalid String Input to Prevent SQL Injection Attacks",
                      "Description": "This test case verifies that the system appropriately handles invalid string inputs, triggering error messages or rejections.",
                      "Objective": "Confirm that TaskMaster detects and prevents SQL injection vulnerabilities with invalid input strings.",
                      "Category": "Input Data Variety Testing",
                      "Comments": "Test with various types of invalid inputs such as SQL commands to ensure proper handling."
                    },
                    {
                      "ScenarioID": "Input_Data_Variety_Testing_Test_Scenario_5",
                      "TestCaseID": "Boundary_String_Input_Test_Case",
                      "Title": "Validate Handling of Boundary String Inputs to Prevent SQL Injection Attacks",
                      "Description": "This test case assesses the system's handling of string inputs at the boundaries of acceptable ranges, including just below and above these limits.",
                      "Objective": "Confirm that TaskMaster handles boundary string inputs securely and prevents SQL injection vulnerabilities.",
                      "Category": "Input Data Variety Testing",
                      "Comments": "Test with strings at the boundaries to ensure no edge cases are missed."
                    }
                  ]
                }
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6554"
                },
                "test_name": "Performance and Load Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive performance and load testing scenario for a specified system. The scenario should aim to assess the system’s performance and load handling under realistic conditions, simulating user behaviors to evaluate how the system operates under various levels of stress. Clearly establish the purpose of the test, the preconditions required, and the environment configurations necessary to create accurate, real-world conditions. Outline the user load, including distribution and behavior patterns that reflect peak periods, random delays, and sudden surges. Identify key performance metrics such as response time, throughput, resource utilization, and error rates, detailing acceptable thresholds based on industry standards. Specify the data requirements to represent real-world usage, including edge cases, and present a clear, sequential process for test execution from setup to conclusion. Emphasize expected outcomes, define conditions for success and failure, and include guidelines for comprehensive logging and analysis. Ensure that the scenario aligns with ISTQB performance testing standards, incorporates best practices, and prioritizes precision, consistency, and robustness.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, depth, and clarity. A high-quality scenario should align well with the specified test purpose, providing a clear objective and structured approach. It should include all necessary elements such as preconditions, environment configuration, user load details, performance metrics, and expected outcomes.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for performance and load testing, including specified elements like response time, error rates, data requirements, and system behavior under stress.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described testing steps, data requirements, and performance metrics match the intended category and testing standards for performance and load evaluation.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete numbers for user loads, specific configurations for the test environment, and clear definitions of success and failure thresholds. Specificity in metrics and detailed steps for test execution are essential.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of performance and load testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of performance and load testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, user load distribution, key performance metrics, and expected outcomes. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (performance and load) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched metrics or off-target objectives, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as user load numbers, test environment configurations, and threshold definitions. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the system's performance and load handling under various levels of stress, simulating realistic user behaviors and conditions as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for performance and load testing, such as user load details (distribution, behavior patterns), key performance metrics (response time, throughput, resource utilization, error rates), necessary preconditions, environment configurations, and expected outcomes. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with performance and load testing standards and ISTQB guidelines. Check if the test steps accurately reflect the intended load conditions and performance metrics. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for performance and load testing. Consider if the scenario effectively communicates the testing process and performance expectations. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive set of performance and load test cases to evaluate the system's ability to handle various levels of stress. Ensure that the test cases cover response time, throughput, scalability, and stability under different load conditions. Include preconditions, detailed execution steps, specific input data, and measurable expected outcomes for each scenario. Align all test cases with ISTQB performance testing guidelines to validate both typical and edge performance conditions. Emphasize precision, accuracy, and reproducibility in test documentation.",
                "test_case_create_prompts": {
                  "Core Performance Test Case": "Develop a test case to verify the system's ability to handle normal operational loads. Include typical user behavior as input, preconditions for environment setup, and measurable metrics such as response time and system throughput. Ensure test steps simulate realistic user actions and specify precise success criteria.",
                  "Stress Test Case": "Create a test case to evaluate how the system behaves under extreme stress, such as peak user loads or resource exhaustion. Include detailed steps for incrementally increasing load, monitoring system behavior, and identifying failure points. Specify thresholds for acceptable and critical performance levels.",
                  "Scalability Test Case": "Design a test case to assess the system's ability to scale with increasing workload. Include steps to simulate user growth, resource allocation adjustments, and infrastructure elasticity. Define metrics like maximum concurrent users or transactions per second.",
                  "Resource Utilization Test Case": "Create a test case focusing on system resource usage under varying load conditions. Specify steps to measure CPU, memory, disk I/O, and network bandwidth. Include scenarios for detecting bottlenecks and provide acceptable utilization thresholds."
                }
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6553"
                },
                "test_name": "Integration Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive integration test scenario aimed at evaluating how different modules or components of a specified system interact with each other. This scenario should focus on ensuring that integrated components communicate correctly and work together. Clearly outline the purpose of the test, specifying necessary preconditions, setup configurations, and initial data required. Identify key interactions between components, including input and output data flow, dependencies, and any shared resources. Outline test steps that simulate realistic data exchange and process flows between components, capturing both normal and abnormal interaction patterns. Describe expected outcomes for each interaction, including response times, data integrity, and error handling requirements. Emphasize that the scenario aligns with ISTQB integration testing standards, focusing on accuracy, consistency, and robustness in component interaction validation.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as preconditions, data flow between components, expected outcomes, and error handling mechanisms.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for integration testing, including specific component interactions, data exchange, and expected results.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, component interactions, and expected outcomes match the intended integration testing standards.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that include concrete component interactions, detailed data flows, and clear expected outcomes for each integration point. Specificity in data handling and error scenarios is essential for thorough integration testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of integration testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of integration testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, data flow between components, expected outcomes, and error handling. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (integration) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant test steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as component interactions, expected results, and error handling mechanisms. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the interactions between different modules or components of the system, focusing on data flow and communication between components as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for integration testing, such as detailed component interactions, necessary preconditions, data exchange processes, dependencies, shared resources, and expected outcomes for each interaction. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with integration testing standards and ISTQB guidelines. Ensure that the interactions between components are correctly represented and that expected outcomes are properly defined. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for integration testing. Consider if the scenario clearly outlines component interactions and is structured logically. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "As a senior ISTQB-certified test analyst, generate a set of integration test cases to evaluate interactions between system components. Include detailed preconditions, interaction sequences, and data flow verification steps. Focus on validating module communication, data consistency, and error handling. Ensure the test cases align with ISTQB integration testing standards.",
                "test_case_create_prompts": {
                  "Interface Communication Test Case": "Develop a test case to validate communication between integrated components, ensuring proper data exchange and synchronization. Include scenarios for both normal and interrupted communication.",
                  "Data Consistency Test Case": "Create a test case to verify that data remains consistent when shared or transferred between modules. Specify test steps for simulating updates, deletions, and retrievals.",
                  "Shared Resource Access Test Case": "Design a test case to assess how integrated components handle access to shared resources, avoiding conflicts or deadlocks. Include test steps for concurrent access scenarios.",
                  "Error Propagation Test Case": "Create a test case to evaluate the system’s ability to handle and isolate errors in one component without affecting others. Include scenarios to test error logging and recovery mechanisms."
                }
              }
            ],
            "customised_prompt_status": false,
            "process_title": "Input_Data_Variety_Testing",
            "document_type": "Technical Design Document",
            "generated_prompt": "{'test_name': 'Input Data Variety Testing', 'document_type': 'Technical Design Document', 'content': ['# Technical Design Document: TaskMaster', 'TaskMaster is a simple task management system that allows users to manage tasks and projects.', 'The application provides an interface where users can list tasks, add new tasks, and mark tasks as completed.'], 'objectives': {'test_purpose': \"Assess the system's ability to handle a diverse range of input data, evaluating its processing of various data types, formats, edge cases, and outliers.\", 'preconditions': [{'environment_configuration': 'Development environment with TaskMaster application installed.'}]}, 'input_data_scenarios': {'valid_input_types': [{'data_type': 'string', 'description': 'Sample string input for task title'}, {'data_type': 'boolean', 'description': 'Sample boolean value for task completed status'}], 'invalid_input_types': [{'data_type': 'null', 'description': 'Sample null input for task description'}]}, 'expected_system_responses': {'validation_errors': [{'error_message': 'Invalid input type: null', 'expected_response': 'Error message with error code'}], 'specific_handling_methods': [{'input_type': 'string', 'description': 'Special handling for string inputs to prevent SQL injection attacks'}]}, 'test_steps': {'step_1': {'action': 'Create a new project instance', ' precondition': [{'environment_configuration': 'Development environment with TaskMaster application installed.'}], 'expected_response': 'Project creation successful'}}}\n\nInstruction and consistency situation: \nEnsure that each generated scenario directly corresponds to the specified context and purpose of input data variety testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.\n\nInstruction and consistency situation: \nReview each scenario for specificity in areas such as input data examples, error expectations, and response definitions. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.\n\nScoring situation: \n Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for input variety testing, including elements like specific data types, validation requirements, and error handling processes.\n\n\n    JSON Output Structure:\n\n    {\n        \"TestScenarios\": [\n            {\n                \"ScenarioID\": \"Input_Data_Variety_Testing_Test_Scenario_1\",\n                \"Title\": \"<Scenario Title>\",\n                \"Description\": \"<Detailed scenario description at least 3 sentences. This is the most important part of the generate test scenario!>\",\n                \"Objective\": \"<Objective or goal of the scenario>\",\n                \"Category\": \"Input Data Variety Testing\",\n                \"Comments\": \"<Any inconsistency or additional notes>\"\n            }\n        ]\n    }\n\n    Guidelines for JSON Structure:\n    - Ensure the ScenarioID is dynamic and matches the required format.\n\n    Example JSON output:\n    {\n        \"ScenarioID\": \"Process_A_ModelX_Test_Scenario_1\",\n        \"Title\": \"Verify Login Functionality\",\n        \"Description\": \"Test the login functionality to ensure that users can successfully log in with valid credentials and are rejected with incorrect credentials. Additionally, verify that the system displays appropriate error messages for failed login attempts to guide users in correcting their input. Furthermore, ensure that the login session is maintained correctly, allowing users to access their accounts seamlessly after a successful login.\",\n        \"Objective\": \"Validate user authentication mechanism.\",\n        \"Category\": \"Functional Tests\",\n        \"Comments\": \"\"\n    }\n\n    This document is classified as a Technical Design Document. When generating test scenarios, ensure that the structure, format, and content align with the nature of this document type.\n\n    Document Content: \n    # Technical Design Document: TaskMaster\n\n## 1. Introduction\nTaskMaster is a simple task management system that allows users to manage tasks and projects. The application provides an interface where users can list tasks, add new tasks, and mark tasks as completed.\n\n## 2. Architectural Design\nThe application is built around two main classes: `Task` and `Project`.\n\n### 2.1 Task Class\n- **Attributes:**\n  - `title` (str): The title of the task.\n  - `description` (str): A brief description of the task.\n  - `completed` (bool): A flag indicating whether the task is completed.\n- **Methods:**\n  - `mark_completed()`: Marks the task as completed.\n  - `__str__()`: Returns a string representation of the task status.\n\n### 2.2 Project Class\n- **Attributes:**\n  - `name` (str): The name of the project.\n  - `tasks` (list): A list of tasks associated with the project.\n- **Methods:**\n  - `add_task(task)`: Adds a new task to the project.\n  - `remove_task(task_title)`: Removes a task from the project by its title.\n  - `list_tasks()`: Lists all tasks in the project.\n  - `__str__()`: Returns a string representation of the project's status.\n\n## 3. Flow Diagram\n1. The user creates a `Project` instance.\n2. The user creates one or more `Task` instances under the project.\n3. The user can mark tasks as completed and view the list of tasks with their statuses.\n\n## 4. Modularity and Extensibility\nThe application is designed with modularity in mind, allowing for easy extension of classes and methods for future enhancements.\n\n    ",
            "model_output": {
              "TestScenarios": [
                {
                  "ScenarioID": "Input_Data_Variety_Testing_Test_Scenario_1",
                  "Title": "Verify TaskMaster's Handling of Various Input Data Types",
                  "Description": "This scenario aims to evaluate the system's ability to handle a diverse range of input data, including different types (strings and booleans) and formats. The test cases will cover valid inputs, edge cases, and outliers, ensuring the application processes these appropriately.",
                  "Objective": "Assess TaskMaster's processing capabilities for various input data types, formats, edge cases, and outliers",
                  "Category": "Input Data Variety Testing",
                  "Comments": ""
                },
                {
                  "ScenarioID": "Input_Data_Variety_Testing_Test_Scenario_2",
                  "Title": "Validate Task Creation with Valid Input Types",
                  "Description": "Create a new task instance using valid string and boolean input types. Verify that the task is added to the project and that its status can be viewed correctly.",
                  "Objective": "Ensure the creation of tasks using valid string and boolean inputs works as expected",
                  "Category": "Input Data Variety Testing",
                  "Comments": ""
                },
                {
                  "ScenarioID": "Input_Data_Variety_Testing_Test_Scenario_3",
                  "Title": "Test Task Completion Using Valid Input Types",
                  "Description": "Mark a task as completed using valid boolean input type and verify that the task's status has been updated correctly within the project.",
                  "Objective": "Confirm that tasks can be marked as completed using valid boolean inputs",
                  "Category": "Input Data Varity Testing",
                  "Comments": ""
                },
                {
                  "ScenarioID": "Input_Data_Variety_Testing_Test_Scenario_4",
                  "Title": "Assess TaskMaster's Error Handling for Invalid Input Types",
                  "Description": "Attempt to create a task with invalid input types, such as null or unsupported data types. Verify that the system correctly identifies and handles these errors, preventing potential application issues.",
                  "Objective": "Ensure TaskMaster can identify and handle errors from invalid input types",
                  "Category": "Input Data Variety Testing",
                  "Comments": ""
                },
                {
                  "ScenarioID": "Input_Data_Variety_Testing_Test_Scenario_5",
                  "Title": "Validate Special Handling for String Inputs to Prevent SQL Injection Attacks",
                  "Description": "Create a new task with string input that requires special handling to prevent SQL injection attacks. Verify that the system processes the string input correctly and no vulnerabilities are exposed.",
                  "Objective": "Confirm that TaskMaster's string inputs are processed securely, preventing SQL injection attacks",
                  "Category": "Input Data Variety Testing",
                  "Comments": ""
                }
              ]
            }
          }
          {
            "_id": {
              "$oid": "677d2b8306fd9cbc6cfd3ae3"
            },
            "session_id": "20250107162626",
            "original_prompts": [
              {
                "_id": {
                  "$oid": "673d7efca741904082541a61"
                },
                "test_name": "Functional Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a detailed functional test scenario focused on evaluating the system’s core functionalities against specified requirements. This scenario should assess whether the system performs its intended operations correctly, meeting both functional specifications and expected user behaviors. Clearly define the purpose of the test, including necessary preconditions, setup configurations, and initial data required. Identify the primary functions to be tested, including input conditions, expected outputs, and any error-handling mechanisms. Outline specific test steps that simulate realistic user actions to validate each function. Describe the expected outcomes for each step, ensuring that both positive and negative cases are covered. Emphasize that the scenario aligns with ISTQB functional testing standards, aiming for accuracy, completeness, and robustness in functional validation.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as preconditions, test steps, expected outcomes, and error handling requirements.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for functional testing, including elements like input conditions, functional checks, and expected results.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, expected outcomes, and error-handling match the intended functional testing standards.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete examples for inputs, clear test step definitions, and detailed expected results. Specificity in test data and expected outcomes is essential for robust functional testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of functional testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of functional testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, input conditions, expected outputs, and error handling. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (functional) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant test steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as input conditions, expected results, and error handling. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the system's core functionalities against the specified requirements, focusing on whether it tests intended operations and expected user behaviors as outlined in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for functional testing, such as clear objectives, necessary preconditions, detailed test steps simulating realistic user actions, input conditions, expected outputs, and error handling mechanisms. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with functional testing standards and ISTQB guidelines. Ensure that the test steps are appropriate for the functionalities being tested and that expected outcomes are properly defined. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for functional testing. Consider if the scenario is well-organized, uses precise technical language, and provides clear instructions. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_create_prompts": {
                  "Core Functionalities Test Case": "Create a test case that verifies the core functionalities of the system as defined in the requirements. Include preconditions, detailed steps, input data, and expected results to ensure the system performs its intended operations correctly. Ensure the case aligns with ISTQB Functional Testing guidelines. Specify each step's expected output clearly and concisely.",
                  "Boundary Value Analysis Test Case": "Develop a test case focusing on Boundary Value Analysis (BVA) to evaluate system behavior at the edges of input ranges. Include preconditions, input data at lower and upper boundaries, and the expected system response for each scenario. Define clear test steps for validating boundary conditions, ensuring compliance with functional testing standards.",
                  "Error Recovery Functional Test Case": "Design a test case to assess the system's error recovery mechanisms during normal operations. Include scenarios with invalid inputs, interruptions, or fault conditions. Specify detailed steps to simulate these scenarios and describe the expected system behavior for each step, including recovery or error handling outputs.",
                  "User Input Validation Test Case": "Create a test case to validate the system's ability to handle various user inputs. Include valid, invalid, and edge-case inputs, with steps to simulate input actions and monitor system responses. Specify preconditions, detailed test steps, and expected outcomes to ensure the system handles inputs correctly and provides appropriate feedback for invalid inputs."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, generate a comprehensive set of functional test cases based on the provided test scenario. Ensure that the test cases collectively cover all actions, steps, and functional areas outlined in the scenario. Each test case should include detailed descriptions of preconditions, input conditions, step-by-step actions, and expected outcomes. Cover positive, negative, and boundary cases to ensure complete validation of the system’s functionalities. Include specific checks for error handling and edge cases where applicable. Organize the test cases to ensure clarity and traceability, aligning with ISTQB standards. The goal is to produce the maximum number of test cases necessary to comprehensively validate the scenario while avoiding redundancy."
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6551"
                },
                "test_name": "Edge Cases and Boundary Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design detailed test scenarios focused on evaluating the system's response to edge cases and boundary values. The scenario should explore how the system behaves at the extremes of input ranges, identifying potential vulnerabilities and ensuring reliability. Clearly define the purpose of the test, necessary preconditions, and environment configurations to create accurate testing conditions. Specify a range of test cases, including boundary values (just below, at, and just above limits) and edge cases that challenge system assumptions. Describe the expected outcomes for each case, detailing system responses, error handling, or corrective measures as needed. Outline the test steps, ensuring they capture all critical values for each parameter. Emphasize that the scenario aligns with ISTQB standards for boundary testing, aiming to identify weaknesses and ensure robustness against extreme inputs.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as boundary value examples, edge cases, expected outcomes, and specific error handling requirements.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for edge cases and boundary testing, including elements like precise boundary values, error handling expectations, and critical input limits.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, boundary values, and expected outcomes match the intended category and testing standards for boundary and edge case evaluation.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that provide concrete boundary values, clear definitions of acceptable limits, and detailed error handling for edge cases. Specificity in test data and expected results is essential for robust testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of boundary and edge case testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of boundary and edge case testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, boundary values, edge cases, and expected outcomes. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (boundary and edge cases) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as boundary values, edge case examples, and error expectations. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively targets edge cases and boundary values, exploring system behavior at the extremes of input ranges as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for edge case and boundary testing, such as specific boundary values (just below, at, and just above limits), detailed preconditions, test steps, and expected outcomes for each case. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with boundary testing standards and ISTQB guidelines. Check if the test steps correspond correctly to the specified boundary values. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for boundary testing. Consider if the scenario clearly communicates the test conditions and expectations. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, create a detailed set of test cases to evaluate system behavior under edge case scenarios and at boundary input values. Include preconditions, specific test data for both lower and upper limits, and steps to assess how the system handles inputs outside the expected range. Ensure scenarios include both positive and negative test cases to validate robustness and reliability. Align test cases with ISTQB boundary testing guidelines for comprehensive validation of system behavior.",
                "test_case_create_prompts": {
                  "Boundary Value Analysis Test Case": "Design a test case focusing on testing inputs at the boundaries of acceptable ranges. Specify lower and upper limits, just below and above these limits, and expected outcomes for each condition. Include clear steps for validating the system's response to each input.",
                  "Extreme Input Test Case": "Create a test case to evaluate system performance and stability when handling extreme or unrealistic input values. Define test data that pushes the system beyond its designed operational range, specifying the expected error messages or graceful degradation responses.",
                  "Null and Empty Input Test Case": "Develop a test case to assess system behavior when null or empty inputs are provided. Include detailed steps to simulate such inputs and expected outcomes, such as validation error messages or fallback defaults.",
                  "Invalid Format Test Case": "Create a test case to evaluate the system's validation mechanisms when inputs are provided in incorrect formats. Include examples such as malformed dates, special characters in numeric fields, or overly long strings, and specify expected error handling outcomes."
                }
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6555"
                },
                "test_name": "Security Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive security test scenario aimed at evaluating the system’s resilience against various security threats. The scenario should focus on identifying vulnerabilities, ensuring data protection, and validating access controls within the specified system. Clearly define the purpose of the test, including the preconditions, environment configurations, and user roles required to simulate realistic security conditions. Identify key areas to be tested, such as authentication, authorization, data encryption, input validation, session management, and error handling. Outline test steps that cover common security tests such as SQL injection, XSS, CSRF, brute-force attacks, and privilege escalation attempts. Describe the expected outcomes for each test, including the system’s response to attempted breaches, error messages, and any logging requirements. Emphasize that the scenario aligns with ISTQB security testing standards, focusing on thorough vulnerability detection, data integrity, and system robustness against unauthorized access.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as preconditions, security vulnerabilities to test, expected outcomes, and error handling mechanisms.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for security testing, including elements like specific security threats, test steps, and expected defensive responses.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, security vulnerabilities, and expected outcomes match the intended security testing standards.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete security threat examples, detailed test steps, and clear expected outcomes for each threat. Specificity in vulnerability descriptions and system responses is essential for robust security testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of security testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of security testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, security vulnerabilities, expected outcomes, and error handling. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (security) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant test steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as vulnerability descriptions, expected results, and error handling requirements. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the system's resilience against security threats, focusing on identifying vulnerabilities and validating access controls as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for security testing, such as specific security vulnerabilities to test (e.g., SQL injection, XSS, CSRF), necessary preconditions, user roles, test steps, expected system responses, and error handling mechanisms. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with security testing standards and ISTQB guidelines. Ensure that the security threats and test steps correspond correctly to the objectives. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for security testing. Consider if the scenario is detailed, well-structured, and uses appropriate security terminology. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive set of security test cases to evaluate the system's resilience against threats. Include scenarios for testing authentication, input validation, data encryption, and unauthorized access prevention. Each test case should include detailed preconditions, steps to simulate security vulnerabilities, and expected outcomes. Align all test cases with ISTQB security testing guidelines to ensure thorough validation of system security measures.",
                "test_case_create_prompts": {
                  "Authentication Test Case": "Create a test case to verify the robustness of the system's authentication mechanism, including password strength requirements and protection against brute-force attacks.",
                  "Input Validation Security Test Case": "Develop a test case to test for vulnerabilities such as SQL injection, XSS, or CSRF attacks by providing malicious input. Specify expected outcomes like rejection of input and secure logging.",
                  "Data Encryption Test Case": "Design a test case to assess the effectiveness of encryption for sensitive data during transmission and storage. Specify tools and methods to verify encryption implementation.",
                  "Access Control Test Case": "Create a test case to ensure access control policies are enforced correctly, preventing unauthorized users from accessing restricted features or data."
                }
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6556"
                },
                "test_name": "User Interface (GUI) Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive GUI test scenario to evaluate the usability, responsiveness, and visual consistency of the user interface for a specified system. This scenario should focus on ensuring that the interface meets design specifications and provides a seamless user experience. Clearly outline the purpose of the test, specifying necessary preconditions, screen resolutions, and device configurations required. Identify key UI elements to be tested, such as buttons, menus, text fields, alignment, colors, font sizes, and responsiveness across different devices. Describe test steps that include checking the interface's layout, navigation flow, interaction feedback, and responsiveness to user inputs. Define expected outcomes for each interaction, including alignment, visual feedback, error messages, and responsiveness criteria. Emphasize that the scenario aligns with ISTQB GUI testing standards, focusing on visual accuracy, and user experience quality.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as preconditions, UI elements, expected outcomes, and visual consistency checks.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for GUI testing, including elements like specific UI components, visual feedback expectations, and responsiveness requirements.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, UI elements, and expected outcomes match the intended GUI testing standards.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete examples for UI elements, detailed test steps, and clear expected results. Specificity in layout checks, responsiveness, and user feedback is essential for comprehensive GUI testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of GUI testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of GUI testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, UI components, expected outcomes, and visual consistency checks. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (GUI) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant UI elements, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as UI elements, layout expectations, and visual feedback. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the usability, responsiveness, and visual consistency of the user interface, focusing on UI elements and user experience as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for GUI testing, such as specific UI elements to be tested (buttons, menus, text fields), necessary preconditions (screen resolutions, device configurations), test steps, expected outcomes (alignment, visual feedback, responsiveness), and visual consistency checks. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with GUI testing standards and ISTQB guidelines. Check if the test steps accurately reflect the UI components and expected user interactions. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for GUI testing. Consider if the scenario effectively communicates the testing of UI elements and user experience aspects. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "As a senior ISTQB-certified test analyst, create a comprehensive set of GUI test cases to validate usability, visual consistency, and responsiveness. Include preconditions, test steps, and expected outcomes for all UI elements. Ensure test cases cover navigation, layout alignment, input handling, and feedback mechanisms. Align with ISTQB GUI testing standards for a seamless and user-friendly experience.",
                "test_case_create_prompts": {
                  "Visual Consistency Test Case": "Create a test case to ensure UI elements are visually consistent with design specifications across resolutions. Specify alignment, color schemes, and font size checks.",
                  "Navigation Flow Test Case": "Develop a test case to verify the intuitiveness of the navigation flow, ensuring users can seamlessly move between pages and access features.",
                  "Input Field Validation Test Case": "Design a test case to test input field behavior, including placeholder text, error messages, and constraints on input formats.",
                  "Responsiveness Test Case": "Create a test case to evaluate how the UI adapts to different screen sizes and orientations. Specify steps to verify element repositioning and integration."
                }
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6550"
                },
                "test_name": "Compatibility Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design comprehensive test scenarios to evaluate the compatibility of a specified system across different environments. This scenario should focus on assessing the system’s ability to function as expected on various platforms, devices, operating systems, browsers, and configurations. Clearly outline the purpose of the test, including the preconditions and environmental setups required for accurate testing across multiple platforms. Specify a range of environments, such as specific operating system versions, browser types, device specifications, screen resolutions, and hardware configurations, ensuring broad coverage. Describe the expected outcomes for each environment, identifying potential compatibility issues, or visual rendering differences. Detail test steps that simulate user behavior in each environment to validate consistent performance. Emphasize that the scenario aligns with ISTQB compatibility testing standards, prioritizing cross-environment consistency, reliability, and user experience.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective, structured approach, and including essential elements such as environment configurations, expected outcomes, and compatibility criteria.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for compatibility testing, including specific platform details, and cross-environment checks.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, environment configurations, and expected outcomes match the intended category and testing standards for compatibility evaluation.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete configurations (such as operating system versions, browser types), detailed expected results, and identified compatibility metrics.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of compatibility testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of compatibility testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, environment configurations, expected outcomes, and compatibility checks. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (compatibility) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant configurations, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as platform versions, environment configurations, and expected responses. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively addresses compatibility testing across different environments as specified in the prompt. Consider if it includes testing on various platforms, devices, operating systems, browsers, and configurations. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for compatibility testing, such as detailed environment configurations, preconditions, test steps simulating user behavior on different platforms, and expected outcomes for each environment. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with compatibility testing standards and ISTQB guidelines. Check if the test steps and expected outcomes match the specified environments and configurations. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for compatibility testing. Consider if the scenario is clearly organized, uses technical language appropriately, and provides precise details. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "As a senior ISTQB-certified test analyst, design a comprehensive set of compatibility test cases to evaluate the across different platforms, devices, operating systems, and browsers. Specify preconditions, test steps, and expected outcomes for each scenario. Ensure the cases assess cross-platform consistency, and visual rendering. Align with ISTQB compatibility testing standards to ensure the system provides a seamless user experience in diverse environments.",
                "test_case_create_prompts": {
                  "Cross-Browser Test Case": "Create a test case to ensure consistency across multiple web browsers and their versions. Specify preconditions, browser configurations, detailed steps for validation, and expected outputs for compatibility verification.",
                  "Device Compatibility Test Case": "Develop a test case to evaluate system behavior on a variety of devices, such as smartphones, tablets, and desktops. Include steps to verify responsive design and integrition across device specifications.",
                  "Operating System Compatibility Test Case": "Design a test case to assess system performance on different operating systems, including specific versions. Include steps to test installation, execution, and interoperability between OS-specific features.",
                  "Screen Resolution Test Case": "Create a test case to validate the system's responsiveness and layout consistency across various screen resolutions. Include steps for testing visual elements and alignment across resolutions."
                }
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6552"
                },
                "test_name": "Input Data Variety Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive test scenario focused on assessing the system’s ability to handle a diverse range of input data. The scenario should evaluate how the system processes various data types, formats, edge cases, and outliers to ensure robust input handling. Clearly define the purpose of the test, the preconditions required, and the environment configurations necessary for accurate testing. Specify a range of input data examples, including valid, invalid, boundary, and null values. Describe the expected system responses, detailing any validation errors, warnings, or specific handling methods required for each type of input. Outline test steps that simulate user behavior with different input scenarios to validate the system’s resilience. Emphasize that the scenario aligns with ISTQB testing standards, ensuring a thorough assessment of data handling robustness and consistency.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective, structured approach, and including all necessary elements such as input data examples, expected outcomes, and error handling.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for input variety testing, including elements like specific data types, validation requirements, and error handling processes.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, data requirements, and expected outcomes match the intended category and testing standards for input variety evaluation.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that include concrete examples of various input data types, specific configurations for the test environment, and clear definitions of expected outcomes for each input category. Specificity in data validation requirements and error expectations is essential.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of input data variety testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of input data variety testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, data examples (valid, invalid, boundary, null), and expected outcomes. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (input data variety) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as input data examples, error expectations, and response definitions. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the system's ability to handle a diverse range of input data, including valid, invalid, boundary, and null values, as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for input data variety testing, such as a range of input data examples, necessary preconditions, detailed test steps, expected system responses, validation errors, and error handling for each type of input. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with input data variety testing standards and ISTQB guidelines. Check if the test steps accurately reflect the diverse input data and expected outcomes. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for input data variety testing. Consider if the scenario effectively communicates the testing of different data types and their expected handling. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, generate a comprehensive set of test cases to validate the system's ability to handle diverse input data types. Include scenarios for valid, invalid, null, and edge-case inputs. Specify detailed steps, expected outcomes, and validation criteria for each input type. Align all test cases with ISTQB testing guidelines to ensure the system’s robustness in input handling.",
                "test_case_create_prompts": {
                  "Valid Input Test Case": "Create a test case to validate the system’s ability to process valid input data accurately. Include examples for various data types such as text, numbers, and dates.",
                  "Invalid Input Test Case": "Develop a test case to test system behavior with invalid inputs, ensuring appropriate error messages or rejections are triggered.",
                  "Boundary Input Test Case": "Design a test case to assess the system’s handling of input values at the boundaries of acceptable ranges, including just below and above these limits.",
                  "Special Character Input Test Case": "Create a test case to evaluate how the system handles inputs with special characters, emojis, or unexpected symbols. Specify expected sanitization or rejection behaviors."
                }
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6554"
                },
                "test_name": "Performance and Load Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive performance and load testing scenario for a specified system. The scenario should aim to assess the system’s performance and load handling under realistic conditions, simulating user behaviors to evaluate how the system operates under various levels of stress. Clearly establish the purpose of the test, the preconditions required, and the environment configurations necessary to create accurate, real-world conditions. Outline the user load, including distribution and behavior patterns that reflect peak periods, random delays, and sudden surges. Identify key performance metrics such as response time, throughput, resource utilization, and error rates, detailing acceptable thresholds based on industry standards. Specify the data requirements to represent real-world usage, including edge cases, and present a clear, sequential process for test execution from setup to conclusion. Emphasize expected outcomes, define conditions for success and failure, and include guidelines for comprehensive logging and analysis. Ensure that the scenario aligns with ISTQB performance testing standards, incorporates best practices, and prioritizes precision, consistency, and robustness.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, depth, and clarity. A high-quality scenario should align well with the specified test purpose, providing a clear objective and structured approach. It should include all necessary elements such as preconditions, environment configuration, user load details, performance metrics, and expected outcomes.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for performance and load testing, including specified elements like response time, error rates, data requirements, and system behavior under stress.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described testing steps, data requirements, and performance metrics match the intended category and testing standards for performance and load evaluation.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete numbers for user loads, specific configurations for the test environment, and clear definitions of success and failure thresholds. Specificity in metrics and detailed steps for test execution are essential.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of performance and load testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of performance and load testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, user load distribution, key performance metrics, and expected outcomes. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (performance and load) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched metrics or off-target objectives, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as user load numbers, test environment configurations, and threshold definitions. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the system's performance and load handling under various levels of stress, simulating realistic user behaviors and conditions as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for performance and load testing, such as user load details (distribution, behavior patterns), key performance metrics (response time, throughput, resource utilization, error rates), necessary preconditions, environment configurations, and expected outcomes. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with performance and load testing standards and ISTQB guidelines. Check if the test steps accurately reflect the intended load conditions and performance metrics. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for performance and load testing. Consider if the scenario effectively communicates the testing process and performance expectations. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive set of performance and load test cases to evaluate the system's ability to handle various levels of stress. Ensure that the test cases cover response time, throughput, scalability, and stability under different load conditions. Include preconditions, detailed execution steps, specific input data, and measurable expected outcomes for each scenario. Align all test cases with ISTQB performance testing guidelines to validate both typical and edge performance conditions. Emphasize precision, accuracy, and reproducibility in test documentation.",
                "test_case_create_prompts": {
                  "Core Performance Test Case": "Develop a test case to verify the system's ability to handle normal operational loads. Include typical user behavior as input, preconditions for environment setup, and measurable metrics such as response time and system throughput. Ensure test steps simulate realistic user actions and specify precise success criteria.",
                  "Stress Test Case": "Create a test case to evaluate how the system behaves under extreme stress, such as peak user loads or resource exhaustion. Include detailed steps for incrementally increasing load, monitoring system behavior, and identifying failure points. Specify thresholds for acceptable and critical performance levels.",
                  "Scalability Test Case": "Design a test case to assess the system's ability to scale with increasing workload. Include steps to simulate user growth, resource allocation adjustments, and infrastructure elasticity. Define metrics like maximum concurrent users or transactions per second.",
                  "Resource Utilization Test Case": "Create a test case focusing on system resource usage under varying load conditions. Specify steps to measure CPU, memory, disk I/O, and network bandwidth. Include scenarios for detecting bottlenecks and provide acceptable utilization thresholds."
                }
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6553"
                },
                "test_name": "Integration Testing",
                "test_prompt": "As a senior ISTQB-certified test analyst, design an integration test scenario for the taskmaster.py document, focusing on evaluating how the Task and Project classes interact with each other. This scenario should ensure that tasks are added correctly to projects, marked as completed, and listed in a project's task list. Clearly outline the purpose of the test, specifying necessary preconditions, setup configurations, and initial data required for testing Task and Project class interactions. Identify key interactions between Task and Project classes, including input and output data flow, dependencies, and shared resources. Outline test steps that simulate realistic data exchange and process flows between these classes, capturing both normal and abnormal interaction patterns. Describe expected outcomes for each interaction, including response times, data integrity, and error handling requirements. Emphasize that the scenario aligns with ISTQB integration testing standards, focusing on accuracy, consistency, and robustness in component interaction validation.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as preconditions, data flow between components, expected outcomes, and error handling mechanisms.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for integration testing, including specific component interactions, data exchange, and expected results.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, component interactions, and expected outcomes match the intended integration testing standards.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that include concrete component interactions, detailed data flows, and clear expected outcomes for each integration point. Specificity in data handling and error scenarios is essential for thorough integration testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of integration testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of integration testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, data flow between components, expected outcomes, and error handling. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (integration) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant test steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as component interactions, expected results, and error handling mechanisms. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the interactions between different modules or components of the system, focusing on data flow and communication between components as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for integration testing, such as detailed component interactions, necessary preconditions, data exchange processes, dependencies, shared resources, and expected outcomes for each interaction. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with integration testing standards and ISTQB guidelines. Ensure that the interactions between components are correctly represented and that expected outcomes are properly defined. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for integration testing. Consider if the scenario clearly outlines component interactions and is structured logically. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "As a senior ISTQB-certified test analyst, generate a set of integration test cases to evaluate interactions between system components. Include detailed preconditions, interaction sequences, and data flow verification steps. Focus on validating module communication, data consistency, and error handling. Ensure the test cases align with ISTQB integration testing standards.",
                "test_case_create_prompts": {
                  "Interface Communication Test Case": "Develop a test case to validate communication between integrated components, ensuring proper data exchange and synchronization. Include scenarios for both normal and interrupted communication.",
                  "Data Consistency Test Case": "Create a test case to verify that data remains consistent when shared or transferred between modules. Specify test steps for simulating updates, deletions, and retrievals.",
                  "Shared Resource Access Test Case": "Design a test case to assess how integrated components handle access to shared resources, avoiding conflicts or deadlocks. Include test steps for concurrent access scenarios.",
                  "Error Propagation Test Case": "Create a test case to evaluate the system’s ability to handle and isolate errors in one component without affecting others. Include scenarios to test error logging and recovery mechanisms."
                },
                "customised_prompt_status": true,
                "combined_prompt": "As a senior ISTQB-certified test analyst, generate a set of integration test cases to evaluate interactions between system components. Include detailed preconditions, interaction sequences, and data flow verification steps. Focus on validating module communication, data consistency, and error handling. Ensure the test cases align with ISTQB integration testing standards.\n\nScenario Details:\nScenarioID: taskmaster_integration_test_scenario_1\nTitle: Verify Task Management Functionality\nDescription: Test the interaction between the Task and Project classes to ensure that tasks can be added to projects, marked as completed, and listed in the project's task list.\nObjective: Validate task management functionality.\nCategory: Integration Testing\nComments: \n\nCombined Test Case Prompts:\nTest Case Type: Interface Communication Test Case\nDevelop a test case to validate communication between integrated components, ensuring proper data exchange and synchronization. Include scenarios for both normal and interrupted communication.\n\nTest Case Type: Error Propagation Test Case\nCreate a test case to evaluate the system’s ability to handle and isolate errors in one component without affecting others. Include scenarios to test error logging and recovery mechanisms.\n\n\n    JSON Output Structure:\n\n    {\n        \"TestScenarios\": [\n            {\n                \"ScenarioID\": \"<Dynamic Scenario ID>\",\n                \"TestCaseID\": \"<Dynamic Test Case ID>\",\n                \"Title\": \"<Scenario Title>\",\n                \"Description\": \"<Detailed scenario description at least 3 sentences. This is the most important part of the test scenario!>\",\n                \"Objective\": \"<Objective or goal of the scenario>\",\n                \"Category\": \"<Category of the test case>\",\n                \"Comments\": \"<Any inconsistency or additional notes>\"\n            }\n        ]\n    }\n\n    Guidelines for JSON Structure:\n    - Ensure the ScenarioID is dynamic and matches the required format.\n    - TestCaseID must be unique within the scope of its ScenarioID.\n\n    Example JSON output:\n    {\n        \"ScenarioID\": \"Scenario_1\",\n        \"TestCaseID\": \"TestCase_1\",\n        \"Title\": \"Verify Login Functionality\",\n        \"Description\": \"Test the login functionality to ensure users can log in with valid credentials and receive appropriate error messages for invalid inputs. Additionally, ensure session management operates correctly post-login.\",\n        \"Objective\": \"Validate user authentication mechanism.\",\n        \"Category\": \"Functional Tests\",\n        \"Comments\": \"Ensure edge cases for invalid inputs are covered.\"\n    }\n    \n\n",
                "scenario_id": "taskmaster_integration_test_scenario_1",
                "test_case": {
                  "TestScenarios": [
                    {
                      "ScenarioID": "taskmaster_integration_test_scenario_1",
                      "TestCaseID": "TC001 Verify Task Management Functionality",
                      "Title": "Verify Task Management Functionality",
                      "Description": "This test scenario evaluates the interaction between the Task and Project classes to ensure that tasks can be added to projects, marked as completed, and listed in the project's task list. The test will verify data consistency and error handling mechanisms.",
                      "Objective": "Validate task management functionality",
                      "Category": "Integration Testing",
                      "Comments": ""
                    },
                    {
                      "ScenarioID": "taskmaster_integration_test_scenario_1",
                      "TestCaseID": "TC002 Normal Communication Test",
                      "Title": "Normal Communication Test",
                      "Description": "This test case evaluates the communication between integrated components during normal operation. The test will verify proper data exchange and synchronization.",
                      "Objective": "Validate interface communication",
                      "Category": "Integration Testing",
                      "Comments": ""
                    },
                    {
                      "ScenarioID": "taskmaster_integration_test_scenario_1",
                      "TestCaseID": "TC003 Normal Communication Interrupted Test",
                      "Title": "Normal Communication Interrupted Test",
                      "Description": "This test case evaluates the communication between integrated components during normal operation, but with an interruption. The test will verify proper data exchange and synchronization after the interruption.",
                      "Objective": "Validate interface communication under interrupted conditions",
                      "Category": "Integration Testing",
                      "Comments": ""
                    },
                    {
                      "ScenarioID": "taskmaster_integration_test_scenario_1",
                      "TestCaseID": "TC004 Error Propagation Test",
                      "Title": "Error Propagation Test",
                      "Description": "This test case evaluates the system’s ability to handle and isolate errors in one component without affecting others. The test will verify error logging and recovery mechanisms.",
                      "Objective": "Validate error propagation and recovery",
                      "Category": "Integration Testing",
                      "Comments": ""
                    },
                    {
                      "ScenarioID": "taskmaster_integration_test_scenario_1",
                      "TestCaseID": "TC005 Error Propagation Interrupted Test",
                      "Title": "Error Propagation Interrupted Test",
                      "Description": "This test case evaluates the system’s ability to handle and isolate errors in one component without affecting others, but with an interruption. The test will verify error logging and recovery mechanisms after the interruption.",
                      "Objective": "Validate error propagation under interrupted conditions",
                      "Category": "Integration Testing",
                      "Comments": ""
                    }
                  ]
                }
              }
            ],
            "customised_prompt_status": false,
            "process_title": "Integration Testing",
            "generated_prompt": "As a senior ISTQB-certified test analyst, design an integration test scenario for the taskmaster.py document, focusing on evaluating how the Task and Project classes interact with each other. This scenario should ensure that tasks are added correctly to projects, marked as completed, and listed in a project's task list. Clearly outline the purpose of the test, specifying necessary preconditions, setup configurations, and initial data required for testing Task and Project class interactions. Identify key interactions between Task and Project classes, including input and output data flow, dependencies, and shared resources. Outline test steps that simulate realistic data exchange and process flows between these classes, capturing both normal and abnormal interaction patterns. Describe expected outcomes for each interaction, including response times, data integrity, and error handling requirements. Emphasize that the scenario aligns with ISTQB integration testing standards, focusing on accuracy, consistency, and robustness in component interaction validation.\n\nInstruction and consistency situation: \nEnsure that each generated scenario directly corresponds to the specified context and purpose of integration testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.\n\nInstruction and consistency situation: \nEvaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'\n\nScoring situation: \n Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for integration testing, including specific component interactions, data exchange, and expected results.\n\n\n    JSON Output Structure:\n\n    {\n        \"TestScenarios\": [\n            {\n                \"ScenarioID\": \"fonksiyonel_test_senaryo_test_durumlari_ornek_Test_Scenario_1\",\n                \"Title\": \"<Scenario Title>\",\n                \"Description\": \"<Detailed scenario description at least 3 sentences. This is the most important part of the generate test scenario!>\",\n                \"Objective\": \"<Objective or goal of the scenario>\",\n                \"Category\": \"Integration Testing\",\n                \"Comments\": \"<Any inconsistency or additional notes>\"\n            }\n        ]\n    }\n\n    Guidelines for JSON Structure:\n    - Ensure the ScenarioID is dynamic and matches the required format.\n\n    Example JSON output:\n    {\n        \"ScenarioID\": \"Process_A_ModelX_Test_Scenario_1\",\n        \"Title\": \"Verify Login Functionality\",\n        \"Description\": \"Test the login functionality to ensure that users can successfully log in with valid credentials and are rejected with incorrect credentials. Additionally, verify that the system displays appropriate error messages for failed login attempts to guide users in correcting their input. Furthermore, ensure that the login session is maintained correctly, allowing users to access their accounts seamlessly after a successful login.\",\n        \"Objective\": \"Validate user authentication mechanism.\",\n        \"Category\": \"Functional Tests\",\n        \"Comments\": \"\"\n    }\n\n    This document is classified as a Source Code. When generating test scenarios, ensure that the structure, format, and content align with the nature of this document type.\n\n    Document Content: \n    # taskmaster.py\n\nclass Task:\n    def __init__(self, title, description, completed=False):\n        self.title = title\n        self.description = description\n        self.completed = completed\n\n    def mark_completed(self):\n        \"\"\"Mark the task as completed.\"\"\"\n        self.completed = True\n\n    def __str__(self):\n        \"\"\"Return a string representation of the task.\"\"\"\n        status = \"Completed\" if self.completed else \"Pending\"\n        return f\"Task: {self.title}, Status: {status}\"\n\n\nclass Project:\n    def __init__(self, name):\n        self.name = name\n        self.tasks = []\n\n    def add_task(self, task):\n        \"\"\"Add a task to the project.\"\"\"\n        if isinstance(task, Task):\n            self.tasks.append(task)\n\n    def remove_task(self, task_title):\n        \"\"\"Remove a task from the project by its title.\"\"\"\n        self.tasks = [task for task in self.tasks if task.title != task_title]\n\n    def list_tasks(self):\n        \"\"\"List all tasks in the project.\"\"\"\n        return [str(task) for task in self.tasks]\n\n    def __str__(self):\n        \"\"\"Return a string representation of the project.\"\"\"\n        return f\"Project: {self.name}, Total Tasks: {len(self.tasks)}\"\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a new project\n    project = Project(\"Home Renovation\")\n\n    # Add tasks to the project\n    project.add_task(Task(\"Paint the walls\", \"Paint all the walls in the living room\"))\n    project.add_task(Task(\"Install new lights\", \"Install ceiling lights in all rooms\"))\n    project.add_task(Task(\"Buy furniture\", \"Purchase furniture for the living room\"))\n\n    # Mark a task as completed\n    project.tasks[0].mark_completed()\n\n    # List all tasks\n    print(project.list_tasks())\n\n    ",
            "model_output": {
              "TestScenarios": [
                {
                  "ScenarioID": "taskmaster_integration_test_scenario_1",
                  "Title": "Verify Task Management Functionality",
                  "Description": "Test the interaction between the Task and Project classes to ensure that tasks can be added to projects, marked as completed, and listed in the project's task list.",
                  "Objective": "Validate task management functionality.",
                  "Category": "Integration Testing",
                  "Comments": ""
                }
              ]
            }
          }
        }
        {
            "_id": {
              "$oid": "677d31d926081be52f73ac0c"
            },
            "session_id": "20250107165327",
            "original_prompts": [
              {
                "_id": {
                  "$oid": "673d7efca741904082541a61"
                },
                "test_name": "Functional Testing",
                "test_prompt": "Functional Testing of Requirements Document\n\nAs an ISTQB-certified test analyst, design a functional test scenario to evaluate the system's core functionalities against the requirements outlined in the provided document. This scenario should assess whether the system performs its intended operations correctly, meeting both functional specifications and expected user behaviors.\n\nPurpose: To validate the system's functionality against the specified requirements, ensuring accuracy, completeness, and robustness in functional validation.\n\nPreconditions:\n* Access to the required application or system\n* Valid login credentials for authorized users\n* Sample data for testing purposes (e.g., tasks, projects, users)\n\nSetup Configurations:\n* System configuration settings (e.g., project categories, task status filters)\n* User preferences (e.g., notification settings, task priority levels)\n\nInitial Data Requirements:\n* A set of sample tasks with varying statuses and due dates\n* Sample projects and user data for testing purposes\n\nPrimary Functions to be Tested:\n1. Task creation and editing\n2. Project organization and filtering\n3. Task status management (completed, pending, in-progress)\n4. Due date reminders and notifications\n5. Task prioritization and search functionality\n6. Note and attachment capabilities for individual tasks\n\nInput Conditions: User interactions with the system (e.g., logging in, creating tasks, updating project categories)\nExpected Outputs: Validated system behavior and user interactions\nError-Handling Mechanisms: System response to invalid inputs or errors\n\nTest Steps:\n1. Log in to the application as an authorized user\n2. Create a new task with sample data (e.g., title, description, due date)\n3. Edit existing tasks to verify updates and changes\n4. Organize tasks into sample projects and view filtered results\n5. Update task statuses to demonstrate completion, pending, and in-progress scenarios\n6. Set due dates and receive reminders for tasks\n7. Prioritize tasks using the system's priority levels\n8. Search for tasks based on keywords or project names\n9. Add notes or attachments to individual tasks\n10. Verify that errors are handled correctly by the system (e.g., invalid input, data inconsistencies)\n\nExpected Outcomes:\n* System functionality is validated against the specified requirements\n* Expected user behaviors and interactions are observed and verified\n* Error-handling mechanisms function as intended\n* Test scenario aligns with ISTQB functional testing standards",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as preconditions, test steps, expected outcomes, and error handling requirements.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for functional testing, including elements like input conditions, functional checks, and expected results.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, expected outcomes, and error-handling match the intended functional testing standards.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete examples for inputs, clear test step definitions, and detailed expected results. Specificity in test data and expected outcomes is essential for robust functional testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of functional testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of functional testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, input conditions, expected outputs, and error handling. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (functional) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant test steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as input conditions, expected results, and error handling. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the system's core functionalities against the specified requirements, focusing on whether it tests intended operations and expected user behaviors as outlined in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for functional testing, such as clear objectives, necessary preconditions, detailed test steps simulating realistic user actions, input conditions, expected outputs, and error handling mechanisms. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with functional testing standards and ISTQB guidelines. Ensure that the test steps are appropriate for the functionalities being tested and that expected outcomes are properly defined. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for functional testing. Consider if the scenario is well-organized, uses precise technical language, and provides clear instructions. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_create_prompts": {
                  "Core Functionalities Test Case": "Create a test case that verifies the core functionalities of the system as defined in the requirements. Include preconditions, detailed steps, input data, and expected results to ensure the system performs its intended operations correctly. Ensure the case aligns with ISTQB Functional Testing guidelines. Specify each step's expected output clearly and concisely.",
                  "Boundary Value Analysis Test Case": "Develop a test case focusing on Boundary Value Analysis (BVA) to evaluate system behavior at the edges of input ranges. Include preconditions, input data at lower and upper boundaries, and the expected system response for each scenario. Define clear test steps for validating boundary conditions, ensuring compliance with functional testing standards.",
                  "Error Recovery Functional Test Case": "Design a test case to assess the system's error recovery mechanisms during normal operations. Include scenarios with invalid inputs, interruptions, or fault conditions. Specify detailed steps to simulate these scenarios and describe the expected system behavior for each step, including recovery or error handling outputs.",
                  "User Input Validation Test Case": "Create a test case to validate the system's ability to handle various user inputs. Include valid, invalid, and edge-case inputs, with steps to simulate input actions and monitor system responses. Specify preconditions, detailed test steps, and expected outcomes to ensure the system handles inputs correctly and provides appropriate feedback for invalid inputs."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, generate a comprehensive set of functional test cases based on the provided test scenario. Ensure that the test cases collectively cover all actions, steps, and functional areas outlined in the scenario. Each test case should include detailed descriptions of preconditions, input conditions, step-by-step actions, and expected outcomes. Cover positive, negative, and boundary cases to ensure complete validation of the system’s functionalities. Include specific checks for error handling and edge cases where applicable. Organize the test cases to ensure clarity and traceability, aligning with ISTQB standards. The goal is to produce the maximum number of test cases necessary to comprehensively validate the scenario while avoiding redundancy.",
                "customised_prompt_status": true,
                "combined_prompt": "Acting as a senior ISTQB-certified test analyst, generate a comprehensive set of functional test cases based on the provided test scenario. Ensure that the test cases collectively cover all actions, steps, and functional areas outlined in the scenario. Each test case should include detailed descriptions of preconditions, input conditions, step-by-step actions, and expected outcomes. Cover positive, negative, and boundary cases to ensure complete validation of the system’s functionalities. Include specific checks for error handling and edge cases where applicable. Organize the test cases to ensure clarity and traceability, aligning with ISTQB standards. The goal is to produce the maximum number of test cases necessary to comprehensively validate the scenario while avoiding redundancy.\n\nScenario Details:\nScenarioID: fonksiyonel_test_senaryo_durumlari_function_testing_ornek_Test_Scenario_1\nTitle: Verify Task Creation and Editing Functionality\nDescription: This test scenario evaluates the system's ability to create, edit, and delete tasks. The goal is to ensure that users can successfully add new tasks with relevant details and update existing tasks with accurate information.\nObjective: Validate task creation and editing functionality.\nCategory: Functional Testing\nComments: \n\nCombined Test Case Prompts:\nTest Case Type: Core Functionalities Test Case\nCreate a test case that verifies the core functionalities of the system as defined in the requirements. Include preconditions, detailed steps, input data, and expected results to ensure the system performs its intended operations correctly. Ensure the case aligns with ISTQB Functional Testing guidelines. Specify each step's expected output clearly and concisely.\n\nTest Case Type: User Input Validation Test Case\nCreate a test case to validate the system's ability to handle various user inputs. Include valid, invalid, and edge-case inputs, with steps to simulate input actions and monitor system responses. Specify preconditions, detailed test steps, and expected outcomes to ensure the system handles inputs correctly and provides appropriate feedback for invalid inputs.\n\n\n    JSON Output Structure:\n\n    {\n        \"TestScenarios\": [\n            {\n                \"ScenarioID\": \"<Dynamic Scenario ID>\",\n                \"TestCaseID\": \"<Dynamic Test Case ID>\",\n                \"Title\": \"<Scenario Title>\",\n                \"Description\": \"<Detailed scenario description at least 3 sentences. This is the most important part of the test scenario!>\",\n                \"Objective\": \"<Objective or goal of the scenario>\",\n                \"Category\": \"<Category of the test case>\",\n                \"Comments\": \"<Any inconsistency or additional notes>\"\n            }\n        ]\n    }\n\n    Guidelines for JSON Structure:\n    - Ensure the ScenarioID is dynamic and matches the required format.\n    - TestCaseID must be unique within the scope of its ScenarioID.\n\n    Example JSON output:\n    {\n        \"ScenarioID\": \"Scenario_1\",\n        \"TestCaseID\": \"TestCase_1\",\n        \"Title\": \"Verify Login Functionality\",\n        \"Description\": \"Test the login functionality to ensure users can log in with valid credentials and receive appropriate error messages for invalid inputs. Additionally, ensure session management operates correctly post-login.\",\n        \"Objective\": \"Validate user authentication mechanism.\",\n        \"Category\": \"Functional Tests\",\n        \"Comments\": \"Ensure edge cases for invalid inputs are covered.\"\n    }\n    \n\n",
                "scenario_id": "fonksiyonel_test_senaryo_durumlari_function_testing_ornek_Test_Scenario_1",
                "test_case": {
                  "TestScenarios": [
                    {
                      "ScenarioID": "fonksiyonel_test_senaryo_durumlari_function_testing_ornek_Test_Scenario_1",
                      "TestCaseID": "TC_Fonksiyonel_Tasarim_001",
                      "Title": "Verify Task Creation Functionality",
                      "Description": "This test scenario evaluates the system's ability to create tasks. The goal is to ensure that users can successfully add new tasks with relevant details.",
                      "Objective": "Validate task creation functionality.",
                      "Category": "Functional Testing",
                      "Comments": ""
                    },
                    {
                      "ScenarioID": "fonksiyonel_test_senaryo_durumlari_function_testing_ornek_Test_Scenario_1",
                      "TestCaseID": "TC_Fonksiyonel_Tasarim_002",
                      "Title": "Verify Task Editing Functionality",
                      "Description": "This test scenario evaluates the system's ability to edit tasks. The goal is to ensure that users can update existing tasks with accurate information.",
                      "Objective": "Validate task editing functionality.",
                      "Category": "Functional Testing",
                      "Comments": ""
                    },
                    {
                      "ScenarioID": "fonksiyonel_test_senaryo_durumlari_function_testing_ornek_Test_Scenario_1",
                      "TestCaseID": "TC_Fonksiyonel_Tasarim_003",
                      "Title": "Verify Task Deletion Functionality",
                      "Description": "This test scenario evaluates the system's ability to delete tasks. The goal is to ensure that users can successfully delete existing tasks.",
                      "Objective": "Validate task deletion functionality.",
                      "Category": "Functional Testing",
                      "Comments": ""
                    },
                    {
                      "ScenarioID": "fonksiyonel_test_senaryo_durumlari_function_testing_ornek_Test_Scenario_1",
                      "TestCaseID": "TC_Fonksiyonel_Tasarim_004",
                      "Title": "Verify Task Creation Functionality with Invalid Inputs",
                      "Description": "This test scenario evaluates the system's ability to handle invalid inputs when creating tasks. The goal is to ensure that users receive appropriate error messages for invalid inputs.",
                      "Objective": "Validate task creation functionality with invalid inputs.",
                      "Category": "Functional Testing",
                      "Comments": ""
                    },
                    {
                      "ScenarioID": "fonksiyonel_test_senaryo_durumlari_function_testing_ornek_Test_Scenario_1",
                      "TestCaseID": "TC_Fonksiyonel_Tasarim_005",
                      "Title": "Verify Task Editing Functionality with Invalid Inputs",
                      "Description": "This test scenario evaluates the system's ability to handle invalid inputs when editing tasks. The goal is to ensure that users receive appropriate error messages for invalid inputs.",
                      "Objective": "Validate task editing functionality with invalid inputs.",
                      "Category": "Functional Testing",
                      "Comments": ""
                    },
                    {
                      "ScenarioID": "fonksiyonel_test_senaryo_durumlari_function_testing_ornek_Test_Scenario_1",
                      "TestCaseID": "TC_Fonksiyonel_Tasarim_006",
                      "Title": "Verify Task Deletion Functionality with Invalid Inputs",
                      "Description": "This test scenario evaluates the system's ability to handle invalid inputs when deleting tasks. The goal is to ensure that users receive appropriate error messages for invalid inputs.",
                      "Objective": "Validate task deletion functionality with invalid inputs.",
                      "Category": "Functional Testing",
                      "Comments": ""
                    },
                    {
                      "ScenarioID": "fonksiyonel_test_senaryo_durumlari_function_testing_ornek_Test_Scenario_1",
                      "TestCaseID": "TC_Fonksiyonel_Tasarim_007",
                      "Title": "Verify Task Creation Functionality with Edge Cases",
                      "Description": "This test scenario evaluates the system's ability to handle edge cases when creating tasks. The goal is to ensure that users can successfully add new tasks with relevant details and handle edge cases.",
                      "Objective": "Validate task creation functionality with edge cases.",
                      "Category": "Functional Testing",
                      "Comments": ""
                    },
                    {
                      "ScenarioID": "fonksiyonel_test_senaryo_durumlari_function_testing_ornek_Test_Scenario_1",
                      "TestCaseID": "TC_Fonksiyonel_Tasarim_008",
                      "Title": "Verify Task Editing Functionality with Edge Cases",
                      "Description": "This test scenario evaluates the system's ability to handle edge cases when editing tasks. The goal is to ensure that users can update existing tasks with accurate information and handle edge cases.",
                      "Objective": "Validate task editing functionality with edge cases.",
                      "Category": "Functional Testing",
                      "Comments": ""
                    },
                    {
                      "ScenarioID": "fonksiyonel_test_senaryo_durumlari_function_testing_ornek_Test_Scenario_1",
                      "TestCaseID": "TC_Fonksiyonel_Tasarim_009",
                      "Title": "Verify Task Deletion Functionality with Edge Cases",
                      "Description": "This test scenario evaluates the system's ability to handle edge cases when deleting tasks. The goal is to ensure that users can successfully delete existing tasks and handle edge cases.",
                      "Objective": "Validate task deletion functionality with edge cases.",
                      "Category": "Functional Testing",
                      "Comments": ""
                    },
                    {
                      "ScenarioID": "fonksiyonel_test_senaryo_durumlari_function_testing_ornek_Test_Scenario_1",
                      "TestCaseID": "TC_Fonksiyonel_Tasarim_010",
                      "Title": "Verify User Input Validation for Task Creation",
                      "Description": "This test scenario evaluates the system's ability to handle user inputs when creating tasks. The goal is to ensure that users receive appropriate feedback for valid and invalid inputs.",
                      "Objective": "Validate user input validation for task creation.",
                      "Category": "User Input Validation",
                      "Comments": ""
                    },
                    {
                      "ScenarioID": "fonksiyonel_test_senaryo_durumlari_function_testing_ornek_Test_Scenario_1",
                      "TestCaseID": "TC_Fonksiyonel_Tasarim_011",
                      "Title": "Verify User Input Validation for Task Editing",
                      "Description": "This test scenario evaluates the system's ability to handle user inputs when editing tasks. The goal is to ensure that users receive appropriate feedback for valid and invalid inputs.",
                      "Objective": "Validate user input validation for task editing.",
                      "Category": "User Input Validation",
                      "Comments": ""
                    },
                    {
                      "ScenarioID": "fonksiyonel_test_senaryo_durumlari_function_testing_ornek_Test_Scenario_1",
                      "TestCaseID": "TC_Fonksiyonel_Tasarim_012",
                      "Title": "Verify User Input Validation for Task Deletion",
                      "Description": "This test scenario evaluates the system's ability to handle user inputs when deleting tasks. The goal is to ensure that users receive appropriate feedback for valid and invalid inputs.",
                      "Objective": "Validate user input validation for task deletion.",
                      "Category": "User Input Validation",
                      "Comments": ""
                    },
                    {
                      "ScenarioID": "fonksiyonel_test_senaryo_durumlari_function_testing_ornek_Test_Scenario_1",
                      "TestCaseID": "TC_Fonksiyonel_Tasarim_013",
                      "Title": "Verify User Input Validation for Task Creation with Invalid Inputs",
                      "Description": "This test scenario evaluates the system's ability to handle user inputs when creating tasks. The goal is to ensure that users receive appropriate error messages for invalid inputs.",
                      "Objective": "Validate user input validation for task creation with invalid inputs.",
                      "Category": "User Input Validation",
                      "Comments": ""
                    },
                    {
                      "ScenarioID": "fonksiyonel_test_senaryo_durumlari_function_testing_ornek_Test_Scenario_1",
                      "TestCaseID": "TC_Fonksiyonel_Tasarim_014",
                      "Title": "Verify User Input Validation for Task Editing with Invalid Inputs",
                      "Description": "This test scenario evaluates the system's ability to handle user inputs when editing tasks. The goal is to ensure that users receive appropriate error messages for invalid inputs.",
                      "Objective": "Validate user input validation for task editing with invalid inputs.",
                      "Category": "User Input Validation",
                      "Comments": ""
                    },
                    {
                      "ScenarioID": "fonksiyonel_test_senaryo_durumlari_function_testing_ornek_Test_Scenario_1",
                      "TestCaseID": "TC_Fonksiyonel_Tasarim_015",
                      "Title": "Verify User Input Validation for Task Deletion with Invalid Inputs",
                      "Description": "This test scenario evaluates the system's ability to handle user inputs when deleting tasks. The goal is to ensure that users receive appropriate error messages for invalid inputs.",
                      "Objective": "Validate user input validation for task deletion with invalid inputs.",
                      "Category": "User Input Validation",
                      "Comments": ""
                    },
                    {
                      "ScenarioID": "fonksiyonel_test_senaryo_durumlari_function_testing_ornek_Test_Scenario_1",
                      "TestCaseID": "TC_Fonksiyonel_Tasarim_016",
                      "Title": "Verify User Input Validation for Task Creation with Edge Cases",
                      "Description": "This test scenario evaluates the system's ability to handle user inputs when creating tasks. The goal is to ensure that users receive appropriate feedback for valid and invalid inputs and handle edge cases.",
                      "Objective": "Validate user input validation for task creation with edge cases.",
                      "Category": "User Input Validation",
                      "Comments": ""
                    },
                    {
                      "ScenarioID": "fonksiyonel_test_senaryo_durumlari_function_testing_ornek_Test_Scenario_1",
                      "TestCaseID": "TC_Fonksiyonel_Tasarim_017",
                      "Title": "Verify User Input Validation for Task Editing with Edge Cases",
                      "Description": "This test scenario evaluates the system's ability to handle user inputs when editing tasks. The goal is to ensure that users receive appropriate feedback for valid and invalid inputs and handle edge cases.",
                      "Objective": "Validate user input validation for task editing with edge cases.",
                      "Category": "User Input Validation",
                      "Comments": ""
                    },
                    {
                      "ScenarioID": "fonksiyonel_test_senaryo_durumlari_function_testing_ornek_Test_Scenario_1",
                      "TestCaseID": "TC_Fonksiyonel_Tasarim_018",
                      "Title": "Verify User Input Validation for Task Deletion with Edge Cases",
                      "Description": "This test scenario evaluates the system's ability to handle user inputs when deleting tasks. The goal is to ensure that users receive appropriate feedback for valid and invalid inputs and handle edge cases.",
                      "Objective": "Validate user input validation for task deletion with edge cases.",
                      "Category": "User Input Validation",
                      "Comments": ""
                    }
                  ]
                }
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6551"
                },
                "test_name": "Edge Cases and Boundary Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design detailed test scenarios focused on evaluating the system's response to edge cases and boundary values. The scenario should explore how the system behaves at the extremes of input ranges, identifying potential vulnerabilities and ensuring reliability. Clearly define the purpose of the test, necessary preconditions, and environment configurations to create accurate testing conditions. Specify a range of test cases, including boundary values (just below, at, and just above limits) and edge cases that challenge system assumptions. Describe the expected outcomes for each case, detailing system responses, error handling, or corrective measures as needed. Outline the test steps, ensuring they capture all critical values for each parameter. Emphasize that the scenario aligns with ISTQB standards for boundary testing, aiming to identify weaknesses and ensure robustness against extreme inputs.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as boundary value examples, edge cases, expected outcomes, and specific error handling requirements.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for edge cases and boundary testing, including elements like precise boundary values, error handling expectations, and critical input limits.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, boundary values, and expected outcomes match the intended category and testing standards for boundary and edge case evaluation.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that provide concrete boundary values, clear definitions of acceptable limits, and detailed error handling for edge cases. Specificity in test data and expected results is essential for robust testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of boundary and edge case testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of boundary and edge case testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, boundary values, edge cases, and expected outcomes. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (boundary and edge cases) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as boundary values, edge case examples, and error expectations. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively targets edge cases and boundary values, exploring system behavior at the extremes of input ranges as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for edge case and boundary testing, such as specific boundary values (just below, at, and just above limits), detailed preconditions, test steps, and expected outcomes for each case. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with boundary testing standards and ISTQB guidelines. Check if the test steps correspond correctly to the specified boundary values. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for boundary testing. Consider if the scenario clearly communicates the test conditions and expectations. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, create a detailed set of test cases to evaluate system behavior under edge case scenarios and at boundary input values. Include preconditions, specific test data for both lower and upper limits, and steps to assess how the system handles inputs outside the expected range. Ensure scenarios include both positive and negative test cases to validate robustness and reliability. Align test cases with ISTQB boundary testing guidelines for comprehensive validation of system behavior.",
                "test_case_create_prompts": {
                  "Boundary Value Analysis Test Case": "Design a test case focusing on testing inputs at the boundaries of acceptable ranges. Specify lower and upper limits, just below and above these limits, and expected outcomes for each condition. Include clear steps for validating the system's response to each input.",
                  "Extreme Input Test Case": "Create a test case to evaluate system performance and stability when handling extreme or unrealistic input values. Define test data that pushes the system beyond its designed operational range, specifying the expected error messages or graceful degradation responses.",
                  "Null and Empty Input Test Case": "Develop a test case to assess system behavior when null or empty inputs are provided. Include detailed steps to simulate such inputs and expected outcomes, such as validation error messages or fallback defaults.",
                  "Invalid Format Test Case": "Create a test case to evaluate the system's validation mechanisms when inputs are provided in incorrect formats. Include examples such as malformed dates, special characters in numeric fields, or overly long strings, and specify expected error handling outcomes."
                }
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6555"
                },
                "test_name": "Security Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive security test scenario aimed at evaluating the system’s resilience against various security threats. The scenario should focus on identifying vulnerabilities, ensuring data protection, and validating access controls within the specified system. Clearly define the purpose of the test, including the preconditions, environment configurations, and user roles required to simulate realistic security conditions. Identify key areas to be tested, such as authentication, authorization, data encryption, input validation, session management, and error handling. Outline test steps that cover common security tests such as SQL injection, XSS, CSRF, brute-force attacks, and privilege escalation attempts. Describe the expected outcomes for each test, including the system’s response to attempted breaches, error messages, and any logging requirements. Emphasize that the scenario aligns with ISTQB security testing standards, focusing on thorough vulnerability detection, data integrity, and system robustness against unauthorized access.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as preconditions, security vulnerabilities to test, expected outcomes, and error handling mechanisms.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for security testing, including elements like specific security threats, test steps, and expected defensive responses.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, security vulnerabilities, and expected outcomes match the intended security testing standards.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete security threat examples, detailed test steps, and clear expected outcomes for each threat. Specificity in vulnerability descriptions and system responses is essential for robust security testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of security testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of security testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, security vulnerabilities, expected outcomes, and error handling. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (security) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant test steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as vulnerability descriptions, expected results, and error handling requirements. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the system's resilience against security threats, focusing on identifying vulnerabilities and validating access controls as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for security testing, such as specific security vulnerabilities to test (e.g., SQL injection, XSS, CSRF), necessary preconditions, user roles, test steps, expected system responses, and error handling mechanisms. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with security testing standards and ISTQB guidelines. Ensure that the security threats and test steps correspond correctly to the objectives. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for security testing. Consider if the scenario is detailed, well-structured, and uses appropriate security terminology. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive set of security test cases to evaluate the system's resilience against threats. Include scenarios for testing authentication, input validation, data encryption, and unauthorized access prevention. Each test case should include detailed preconditions, steps to simulate security vulnerabilities, and expected outcomes. Align all test cases with ISTQB security testing guidelines to ensure thorough validation of system security measures.",
                "test_case_create_prompts": {
                  "Authentication Test Case": "Create a test case to verify the robustness of the system's authentication mechanism, including password strength requirements and protection against brute-force attacks.",
                  "Input Validation Security Test Case": "Develop a test case to test for vulnerabilities such as SQL injection, XSS, or CSRF attacks by providing malicious input. Specify expected outcomes like rejection of input and secure logging.",
                  "Data Encryption Test Case": "Design a test case to assess the effectiveness of encryption for sensitive data during transmission and storage. Specify tools and methods to verify encryption implementation.",
                  "Access Control Test Case": "Create a test case to ensure access control policies are enforced correctly, preventing unauthorized users from accessing restricted features or data."
                }
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6556"
                },
                "test_name": "User Interface (GUI) Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive GUI test scenario to evaluate the usability, responsiveness, and visual consistency of the user interface for a specified system. This scenario should focus on ensuring that the interface meets design specifications and provides a seamless user experience. Clearly outline the purpose of the test, specifying necessary preconditions, screen resolutions, and device configurations required. Identify key UI elements to be tested, such as buttons, menus, text fields, alignment, colors, font sizes, and responsiveness across different devices. Describe test steps that include checking the interface's layout, navigation flow, interaction feedback, and responsiveness to user inputs. Define expected outcomes for each interaction, including alignment, visual feedback, error messages, and responsiveness criteria. Emphasize that the scenario aligns with ISTQB GUI testing standards, focusing on visual accuracy, and user experience quality.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as preconditions, UI elements, expected outcomes, and visual consistency checks.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for GUI testing, including elements like specific UI components, visual feedback expectations, and responsiveness requirements.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, UI elements, and expected outcomes match the intended GUI testing standards.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete examples for UI elements, detailed test steps, and clear expected results. Specificity in layout checks, responsiveness, and user feedback is essential for comprehensive GUI testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of GUI testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of GUI testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, UI components, expected outcomes, and visual consistency checks. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (GUI) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant UI elements, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as UI elements, layout expectations, and visual feedback. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the usability, responsiveness, and visual consistency of the user interface, focusing on UI elements and user experience as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for GUI testing, such as specific UI elements to be tested (buttons, menus, text fields), necessary preconditions (screen resolutions, device configurations), test steps, expected outcomes (alignment, visual feedback, responsiveness), and visual consistency checks. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with GUI testing standards and ISTQB guidelines. Check if the test steps accurately reflect the UI components and expected user interactions. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for GUI testing. Consider if the scenario effectively communicates the testing of UI elements and user experience aspects. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "As a senior ISTQB-certified test analyst, create a comprehensive set of GUI test cases to validate usability, visual consistency, and responsiveness. Include preconditions, test steps, and expected outcomes for all UI elements. Ensure test cases cover navigation, layout alignment, input handling, and feedback mechanisms. Align with ISTQB GUI testing standards for a seamless and user-friendly experience.",
                "test_case_create_prompts": {
                  "Visual Consistency Test Case": "Create a test case to ensure UI elements are visually consistent with design specifications across resolutions. Specify alignment, color schemes, and font size checks.",
                  "Navigation Flow Test Case": "Develop a test case to verify the intuitiveness of the navigation flow, ensuring users can seamlessly move between pages and access features.",
                  "Input Field Validation Test Case": "Design a test case to test input field behavior, including placeholder text, error messages, and constraints on input formats.",
                  "Responsiveness Test Case": "Create a test case to evaluate how the UI adapts to different screen sizes and orientations. Specify steps to verify element repositioning and integration."
                }
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6550"
                },
                "test_name": "Compatibility Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design comprehensive test scenarios to evaluate the compatibility of a specified system across different environments. This scenario should focus on assessing the system’s ability to function as expected on various platforms, devices, operating systems, browsers, and configurations. Clearly outline the purpose of the test, including the preconditions and environmental setups required for accurate testing across multiple platforms. Specify a range of environments, such as specific operating system versions, browser types, device specifications, screen resolutions, and hardware configurations, ensuring broad coverage. Describe the expected outcomes for each environment, identifying potential compatibility issues, or visual rendering differences. Detail test steps that simulate user behavior in each environment to validate consistent performance. Emphasize that the scenario aligns with ISTQB compatibility testing standards, prioritizing cross-environment consistency, reliability, and user experience.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective, structured approach, and including essential elements such as environment configurations, expected outcomes, and compatibility criteria.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for compatibility testing, including specific platform details, and cross-environment checks.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, environment configurations, and expected outcomes match the intended category and testing standards for compatibility evaluation.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete configurations (such as operating system versions, browser types), detailed expected results, and identified compatibility metrics.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of compatibility testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of compatibility testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, environment configurations, expected outcomes, and compatibility checks. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (compatibility) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant configurations, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as platform versions, environment configurations, and expected responses. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively addresses compatibility testing across different environments as specified in the prompt. Consider if it includes testing on various platforms, devices, operating systems, browsers, and configurations. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for compatibility testing, such as detailed environment configurations, preconditions, test steps simulating user behavior on different platforms, and expected outcomes for each environment. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with compatibility testing standards and ISTQB guidelines. Check if the test steps and expected outcomes match the specified environments and configurations. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for compatibility testing. Consider if the scenario is clearly organized, uses technical language appropriately, and provides precise details. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "As a senior ISTQB-certified test analyst, design a comprehensive set of compatibility test cases to evaluate the across different platforms, devices, operating systems, and browsers. Specify preconditions, test steps, and expected outcomes for each scenario. Ensure the cases assess cross-platform consistency, and visual rendering. Align with ISTQB compatibility testing standards to ensure the system provides a seamless user experience in diverse environments.",
                "test_case_create_prompts": {
                  "Cross-Browser Test Case": "Create a test case to ensure consistency across multiple web browsers and their versions. Specify preconditions, browser configurations, detailed steps for validation, and expected outputs for compatibility verification.",
                  "Device Compatibility Test Case": "Develop a test case to evaluate system behavior on a variety of devices, such as smartphones, tablets, and desktops. Include steps to verify responsive design and integrition across device specifications.",
                  "Operating System Compatibility Test Case": "Design a test case to assess system performance on different operating systems, including specific versions. Include steps to test installation, execution, and interoperability between OS-specific features.",
                  "Screen Resolution Test Case": "Create a test case to validate the system's responsiveness and layout consistency across various screen resolutions. Include steps for testing visual elements and alignment across resolutions."
                }
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6552"
                },
                "test_name": "Input Data Variety Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive test scenario focused on assessing the system’s ability to handle a diverse range of input data. The scenario should evaluate how the system processes various data types, formats, edge cases, and outliers to ensure robust input handling. Clearly define the purpose of the test, the preconditions required, and the environment configurations necessary for accurate testing. Specify a range of input data examples, including valid, invalid, boundary, and null values. Describe the expected system responses, detailing any validation errors, warnings, or specific handling methods required for each type of input. Outline test steps that simulate user behavior with different input scenarios to validate the system’s resilience. Emphasize that the scenario aligns with ISTQB testing standards, ensuring a thorough assessment of data handling robustness and consistency.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective, structured approach, and including all necessary elements such as input data examples, expected outcomes, and error handling.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for input variety testing, including elements like specific data types, validation requirements, and error handling processes.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, data requirements, and expected outcomes match the intended category and testing standards for input variety evaluation.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that include concrete examples of various input data types, specific configurations for the test environment, and clear definitions of expected outcomes for each input category. Specificity in data validation requirements and error expectations is essential.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of input data variety testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of input data variety testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, data examples (valid, invalid, boundary, null), and expected outcomes. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (input data variety) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as input data examples, error expectations, and response definitions. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the system's ability to handle a diverse range of input data, including valid, invalid, boundary, and null values, as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for input data variety testing, such as a range of input data examples, necessary preconditions, detailed test steps, expected system responses, validation errors, and error handling for each type of input. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with input data variety testing standards and ISTQB guidelines. Check if the test steps accurately reflect the diverse input data and expected outcomes. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for input data variety testing. Consider if the scenario effectively communicates the testing of different data types and their expected handling. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, generate a comprehensive set of test cases to validate the system's ability to handle diverse input data types. Include scenarios for valid, invalid, null, and edge-case inputs. Specify detailed steps, expected outcomes, and validation criteria for each input type. Align all test cases with ISTQB testing guidelines to ensure the system’s robustness in input handling.",
                "test_case_create_prompts": {
                  "Valid Input Test Case": "Create a test case to validate the system’s ability to process valid input data accurately. Include examples for various data types such as text, numbers, and dates.",
                  "Invalid Input Test Case": "Develop a test case to test system behavior with invalid inputs, ensuring appropriate error messages or rejections are triggered.",
                  "Boundary Input Test Case": "Design a test case to assess the system’s handling of input values at the boundaries of acceptable ranges, including just below and above these limits.",
                  "Special Character Input Test Case": "Create a test case to evaluate how the system handles inputs with special characters, emojis, or unexpected symbols. Specify expected sanitization or rejection behaviors."
                }
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6554"
                },
                "test_name": "Performance and Load Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive performance and load testing scenario for a specified system. The scenario should aim to assess the system’s performance and load handling under realistic conditions, simulating user behaviors to evaluate how the system operates under various levels of stress. Clearly establish the purpose of the test, the preconditions required, and the environment configurations necessary to create accurate, real-world conditions. Outline the user load, including distribution and behavior patterns that reflect peak periods, random delays, and sudden surges. Identify key performance metrics such as response time, throughput, resource utilization, and error rates, detailing acceptable thresholds based on industry standards. Specify the data requirements to represent real-world usage, including edge cases, and present a clear, sequential process for test execution from setup to conclusion. Emphasize expected outcomes, define conditions for success and failure, and include guidelines for comprehensive logging and analysis. Ensure that the scenario aligns with ISTQB performance testing standards, incorporates best practices, and prioritizes precision, consistency, and robustness.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, depth, and clarity. A high-quality scenario should align well with the specified test purpose, providing a clear objective and structured approach. It should include all necessary elements such as preconditions, environment configuration, user load details, performance metrics, and expected outcomes.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for performance and load testing, including specified elements like response time, error rates, data requirements, and system behavior under stress.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described testing steps, data requirements, and performance metrics match the intended category and testing standards for performance and load evaluation.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that offer concrete numbers for user loads, specific configurations for the test environment, and clear definitions of success and failure thresholds. Specificity in metrics and detailed steps for test execution are essential.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of performance and load testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of performance and load testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, user load distribution, key performance metrics, and expected outcomes. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (performance and load) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched metrics or off-target objectives, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as user load numbers, test environment configurations, and threshold definitions. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the system's performance and load handling under various levels of stress, simulating realistic user behaviors and conditions as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for performance and load testing, such as user load details (distribution, behavior patterns), key performance metrics (response time, throughput, resource utilization, error rates), necessary preconditions, environment configurations, and expected outcomes. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with performance and load testing standards and ISTQB guidelines. Check if the test steps accurately reflect the intended load conditions and performance metrics. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for performance and load testing. Consider if the scenario effectively communicates the testing process and performance expectations. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive set of performance and load test cases to evaluate the system's ability to handle various levels of stress. Ensure that the test cases cover response time, throughput, scalability, and stability under different load conditions. Include preconditions, detailed execution steps, specific input data, and measurable expected outcomes for each scenario. Align all test cases with ISTQB performance testing guidelines to validate both typical and edge performance conditions. Emphasize precision, accuracy, and reproducibility in test documentation.",
                "test_case_create_prompts": {
                  "Core Performance Test Case": "Develop a test case to verify the system's ability to handle normal operational loads. Include typical user behavior as input, preconditions for environment setup, and measurable metrics such as response time and system throughput. Ensure test steps simulate realistic user actions and specify precise success criteria.",
                  "Stress Test Case": "Create a test case to evaluate how the system behaves under extreme stress, such as peak user loads or resource exhaustion. Include detailed steps for incrementally increasing load, monitoring system behavior, and identifying failure points. Specify thresholds for acceptable and critical performance levels.",
                  "Scalability Test Case": "Design a test case to assess the system's ability to scale with increasing workload. Include steps to simulate user growth, resource allocation adjustments, and infrastructure elasticity. Define metrics like maximum concurrent users or transactions per second.",
                  "Resource Utilization Test Case": "Create a test case focusing on system resource usage under varying load conditions. Specify steps to measure CPU, memory, disk I/O, and network bandwidth. Include scenarios for detecting bottlenecks and provide acceptable utilization thresholds."
                }
              },
              {
                "_id": {
                  "$oid": "6732fb8abb7182490b5d6553"
                },
                "test_name": "Integration Testing",
                "test_prompt": "Acting as a senior ISTQB-certified test analyst, design a comprehensive integration test scenario aimed at evaluating how different modules or components of a specified system interact with each other. This scenario should focus on ensuring that integrated components communicate correctly and work together. Clearly outline the purpose of the test, specifying necessary preconditions, setup configurations, and initial data required. Identify key interactions between components, including input and output data flow, dependencies, and any shared resources. Outline test steps that simulate realistic data exchange and process flows between components, capturing both normal and abnormal interaction patterns. Describe expected outcomes for each interaction, including response times, data integrity, and error handling requirements. Emphasize that the scenario aligns with ISTQB integration testing standards, focusing on accuracy, consistency, and robustness in component interaction validation.",
                "test_scoring_elements_and_prompts": {
                  "Test Scenario Quality Score": "Assign a score (1-5) based on the scenario’s relevance, clarity, and depth. A high-quality scenario should align with the test purpose, providing a clear objective and structured approach. It should include essential elements such as preconditions, data flow between components, expected outcomes, and error handling mechanisms.",
                  "Output Alignment Score": "Evaluate how well the generated output aligns with the expected scenario type (1-5). This score should reflect the output’s adherence to a structured, ISTQB-like format for integration testing, including specific component interactions, data exchange, and expected results.",
                  "Consistency Score": "Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, component interactions, and expected outcomes match the intended integration testing standards.",
                  "Detail and Specificity Score": "Provide a score (1-5) that reflects the level of detail and specificity included in the scenario. Higher scores should be awarded to scenarios that include concrete component interactions, detailed data flows, and clear expected outcomes for each integration point. Specificity in data handling and error scenarios is essential for thorough integration testing.",
                  "Professional Standard Score": "Assess the professionalism and adherence to industry best practices (1-5). A high score indicates that the scenario is crafted in a clear, structured, and concise manner that would meet industry expectations, reflecting a thorough understanding of integration testing as per ISTQB guidelines."
                },
                "test_instruction_elements_and_prompts": {
                  "Scenario Context Verification": "Ensure that each generated scenario directly corresponds to the specified context and purpose of integration testing as outlined in the document. If a reference number is associated with a line or section, include it in the scenario as 'DocRef.' If no reference is provided, label it as 'NoRef' for tracking.",
                  "Relevance and Completeness Check": "Confirm that each scenario includes essential elements such as objective, preconditions, data flow between components, expected outcomes, and error handling. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.",
                  "Consistency and Alignment Verification": "Assess the scenario's alignment with the intended test category (integration) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant test steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.",
                  "Detail Verification": "Review each scenario for specificity in areas such as component interactions, expected results, and error handling mechanisms. Entries without concrete values should be flagged with 'NeedsDetail' to signal areas requiring further refinement.",
                  "Professional and Structured Formatting": "Evaluate the scenario’s structure and clarity, ensuring it adheres to a professional standard of concise, technical language. Label scenarios that meet professional expectations with 'Formatted,' and flag those that require further refinement in structure or clarity as 'ReviseFormat.'"
                },
                "llm_output_judges_and_prompts": {
                  "Relevance": "Assess whether the generated test scenario effectively evaluates the interactions between different modules or components of the system, focusing on data flow and communication between components as specified in the prompt. Provide a score (1-5) and a brief justification.",
                  "Completeness": "Evaluate if the scenario includes all essential components for integration testing, such as detailed component interactions, necessary preconditions, data exchange processes, dependencies, shared resources, and expected outcomes for each interaction. Provide a score (1-5) and specify any missing elements.",
                  "Consistency": "Determine if the scenario maintains consistency across its objectives, test steps, and expected results, aligning with integration testing standards and ISTQB guidelines. Ensure that the interactions between components are correctly represented and that expected outcomes are properly defined. Provide a True/False verdict and explain any inconsistencies.",
                  "Professionalism": "Assess the scenario's adherence to professional standards in terms of structure, language, and clarity, specifically for integration testing. Consider if the scenario clearly outlines component interactions and is structured logically. Provide a score (1-5) and suggest improvements if necessary."
                },
                "test_case_main_prompt": "As a senior ISTQB-certified test analyst, generate a set of integration test cases to evaluate interactions between system components. Include detailed preconditions, interaction sequences, and data flow verification steps. Focus on validating module communication, data consistency, and error handling. Ensure the test cases align with ISTQB integration testing standards.",
                "test_case_create_prompts": {
                  "Interface Communication Test Case": "Develop a test case to validate communication between integrated components, ensuring proper data exchange and synchronization. Include scenarios for both normal and interrupted communication.",
                  "Data Consistency Test Case": "Create a test case to verify that data remains consistent when shared or transferred between modules. Specify test steps for simulating updates, deletions, and retrievals.",
                  "Shared Resource Access Test Case": "Design a test case to assess how integrated components handle access to shared resources, avoiding conflicts or deadlocks. Include test steps for concurrent access scenarios.",
                  "Error Propagation Test Case": "Create a test case to evaluate the system’s ability to handle and isolate errors in one component without affecting others. Include scenarios to test error logging and recovery mechanisms."
                }
              }
            ],
            "customised_prompt_status": false,
            "document_type": "Requirements Document",
            "generated_prompt": "Functional Testing of Requirements Document\n\nAs an ISTQB-certified test analyst, design a functional test scenario to evaluate the system's core functionalities against the requirements outlined in the provided document. This scenario should assess whether the system performs its intended operations correctly, meeting both functional specifications and expected user behaviors.\n\nPurpose: To validate the system's functionality against the specified requirements, ensuring accuracy, completeness, and robustness in functional validation.\n\nPreconditions:\n* Access to the required application or system\n* Valid login credentials for authorized users\n* Sample data for testing purposes (e.g., tasks, projects, users)\n\nSetup Configurations:\n* System configuration settings (e.g., project categories, task status filters)\n* User preferences (e.g., notification settings, task priority levels)\n\nInitial Data Requirements:\n* A set of sample tasks with varying statuses and due dates\n* Sample projects and user data for testing purposes\n\nPrimary Functions to be Tested:\n1. Task creation and editing\n2. Project organization and filtering\n3. Task status management (completed, pending, in-progress)\n4. Due date reminders and notifications\n5. Task prioritization and search functionality\n6. Note and attachment capabilities for individual tasks\n\nInput Conditions: User interactions with the system (e.g., logging in, creating tasks, updating project categories)\nExpected Outputs: Validated system behavior and user interactions\nError-Handling Mechanisms: System response to invalid inputs or errors\n\nTest Steps:\n1. Log in to the application as an authorized user\n2. Create a new task with sample data (e.g., title, description, due date)\n3. Edit existing tasks to verify updates and changes\n4. Organize tasks into sample projects and view filtered results\n5. Update task statuses to demonstrate completion, pending, and in-progress scenarios\n6. Set due dates and receive reminders for tasks\n7. Prioritize tasks using the system's priority levels\n8. Search for tasks based on keywords or project names\n9. Add notes or attachments to individual tasks\n10. Verify that errors are handled correctly by the system (e.g., invalid input, data inconsistencies)\n\nExpected Outcomes:\n* System functionality is validated against the specified requirements\n* Expected user behaviors and interactions are observed and verified\n* Error-handling mechanisms function as intended\n* Test scenario aligns with ISTQB functional testing standards\n\nInstruction and consistency situation: \nConfirm that each scenario includes essential elements such as objective, preconditions, input conditions, expected outputs, and error handling. Scenarios lacking any critical component should be flagged as 'Incomplete,' with an indication of the missing elements in the output.\n\nInstruction and consistency situation: \nAssess the scenario's alignment with the intended test category (functional) and ensure it reflects the overall testing goals specified in the document. Any inconsistencies, such as mismatched objectives or irrelevant test steps, should be highlighted as 'Inconsistent' in the output with an explanation for review.\n\nScoring situation: \n Rate the consistency (True/False) between the generated scenario, its objectives, and its category. A consistent scenario should maintain alignment across all elements, ensuring that the described test steps, expected outcomes, and error-handling match the intended functional testing standards.\n\n\n    JSON Output Structure:\n\n    {\n        \"TestScenarios\": [\n            {\n                \"ScenarioID\": \"fonksiyonel_test_senaryo_durumlari_function_testing_ornek_Test_Scenario_1\",\n                \"Title\": \"<Scenario Title>\",\n                \"Description\": \"<Detailed scenario description at least 3 sentences. This is the most important part of the generate test scenario!>\",\n                \"Objective\": \"<Objective or goal of the scenario>\",\n                \"Category\": \"Functional Testing\",\n                \"Comments\": \"<Any inconsistency or additional notes>\"\n            }\n        ]\n    }\n\n    Guidelines for JSON Structure:\n    - Ensure the ScenarioID is dynamic and matches the required format.\n\n    Example JSON output:\n    {\n        \"ScenarioID\": \"Process_A_ModelX_Test_Scenario_1\",\n        \"Title\": \"Verify Login Functionality\",\n        \"Description\": \"Test the login functionality to ensure that users can successfully log in with valid credentials and are rejected with incorrect credentials. Additionally, verify that the system displays appropriate error messages for failed login attempts to guide users in correcting their input. Furthermore, ensure that the login session is maintained correctly, allowing users to access their accounts seamlessly after a successful login.\",\n        \"Objective\": \"Validate user authentication mechanism.\",\n        \"Category\": \"Functional Tests\",\n        \"Comments\": \"\"\n    }\n\n    This document is classified as a Requirements Document. When generating test scenarios, ensure that the structure, format, and content align with the nature of this document type.\n\n    Document Content: \n    1. Users should be able to create, edit, and delete tasks.\r\n2. Users should organize tasks under specific projects or categories.\r\n3. Users can mark tasks as completed, pending, or in progress.\r\n4. Users can set due dates and receive reminders for tasks.\r\n5. Users can filter and view tasks based on status (completed, pending, in-progress).\r\n6. The application should allow users to prioritize tasks.\r\n7. Users should be able to search for tasks based on keywords or project names.\r\n8. Users can add notes or attachments to individual tasks.\r\n\n    ",
            "model_output": {
              "TestScenarios": [
                {
                  "ScenarioID": "fonksiyonel_test_senaryo_durumlari_function_testing_ornek_Test_Scenario_1",
                  "Title": "Verify Task Creation and Editing Functionality",
                  "Description": "This test scenario evaluates the system's ability to create, edit, and delete tasks. The goal is to ensure that users can successfully add new tasks with relevant details and update existing tasks with accurate information.",
                  "Objective": "Validate task creation and editing functionality.",
                  "Category": "Functional Testing",
                  "Comments": ""
                }
              ]
            }
          }